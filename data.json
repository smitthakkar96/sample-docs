[{"topic_url": "/latest/admin_guide/allocating_node_resources.html", "title": "Atomic Registry Latest | Cluster Administration | Allocating Node Resources", "content": "\nTo provide more reliable scheduling and minimize node resource overcommitment,\neach node can reserve a portion of its resources for use by all underlying\nnode\ncomponents (e.g., kubelet, kube-proxy, Docker) and the remaining system\ncomponents (e.g., sshd, NetworkManager) on the host. Once specified, the\nscheduler has more information about the resources (e.g., memory, CPU) a node\nhas allocated for pods.\n\nResources reserved for node components are based on two node settings:\n\nYou can set these in the kubeletArguments section of the\nnode\nconfiguration file (the /etc/origin/node/node-config.yaml file by default)\nusing a set of <resource_type>=<resource_quantity> pairs (e.g.,\ncpu=200m,memory=30G). Add the section if it does not already exist:\n\nCurrently, the cpu and memory resource types are supported. For cpu,\nthe resource quantity is specified in units of cores (e.g., 200m, 100Ki, 50M).\nFor memory, it is specified in units of bytes (e.g., 200Ki, 100M, 50Gi).\n\nSee Compute Resources for more\ndetails.\n\nIf a flag is not set, it defaults to 0. If none of the flags are set, the\nallocated resource is set to the node\u2019s capacity as it was before the\nintroduction of allocatable resources.\n\nAn allocated amount of a resource is computed based on the following formula:\n\nThe withholding of Hard-Eviction-Thresholds from allocatable is a change in behavior to improve\nsystem reliability now that allocatable is enforced for end-user pods at the node level.\nThe experimental-allocatable-ignore-eviction setting is available to preserve legacy behavior,\nbut it will be deprecated in a future release.\n\nIf [Allocatable] is negative, it is set to 0.\n\nTo see a node\u2019s current capacity and allocatable resources, you can run:\n\nStarting with Atomic Registry\neach node reports system resources utilized by the container runtime and kubelet.\nTo better aid your ability to configure --system-reserved and --kube-reserved,\nyou can introspect corresponding node\u2019s resource usage using the node summary API,\nwhich is accessible at <master>/api/v1/nodes/<node>/proxy/stats/summary.\n\nFor instance, to access the resources from cluster.node22 node, you can run:\n\nSee REST API Overview for more details about certificate details.\n\nThe node is able to limit the total amount of resources that pods\nmay consume based on the configured allocatable value.  This feature significantly\nimproves the reliability of the node by preventing pods from starving\nsystem services (for example: container runtime, node agent, etc.) for resources.\nIt is strongly encouraged that administrators reserve\nresources based on the desired node utilization target\nin order to improve node reliability.\n\nThe node enforces resource constraints using a new cgroup hierarchy\nthat enforces quality of service.  All pods are launched in a\ndedicated cgroup hierarchy separate from system daemons.\n\nTo configure this ability, the following kubelet arguments are provided.\n\nOptionally, the node can be made to enforce kube-reserved and system-reserved by\nspecifying those tokens in the enforce-node-allocatable flag.  If specified, the\ncorresponding --kube-reserved-cgroup or --system-reserved-cgroup needs to be provided.\nIn future releases, the node and container runtime will be packaged in a common cgroup\nseparate from system.slice.  Until that time, we do not recommend users\nchange the default value of enforce-node-allocatable flag.\n\nAdministrators should treat system daemons similar to Guaranteed pods.  System daemons\ncan burst within their bounding control groups and this behavior needs to be managed\nas part of cluster deployments.  Enforcing system-reserved limits\ncan lead to critical system services being CPU starved or OOM killed on the node. The\nrecommendation is to enforce system-reserved only if operators have profiled their nodes\nexhaustively to determine precise estimates and are confident in their ability to\nrecover if any process in that group is OOM killed.\n\nAs a result, we strongly recommended that users only enforce node allocatable for\npods by default, and set aside appropriate reservations for system daemons to maintain\noverall node reliability.\n\nIf a node is under memory pressure, it can impact the entire node and all pods running on\nit.  If a system daemon is using more than its reserved amount of memory, an OOM\nevent may occur that can impact the entire node and all pods running on it.  To avoid\n(or reduce the probability of) system OOMs the node\nprovides Out Of Resource Handling.\n\nBy reserving some memory via the --eviction-hard flag, the node attempts to evict\npods whenever memory availability on the node drops below the absolute value or percentage.\nIf system daemons did not exist on a node, pods are limited to the memory\ncapacity - eviction-hard. For this reason, resources set aside as a buffer for eviction\nbefore reaching out of memory conditions are not available for pods.\n\nHere is an example to illustrate the impact of node allocatable for memory:\n\nFor this node, the effective node allocatable value is 28.9Gi. If the node\nand system components use up all their reservation, the memory available for pods is 28.9Gi,\nand kubelet will evict pods when it exceeds this usage.\n\nIf we enforce node allocatable (28.9Gi) via top level cgroups, then pods can never exceed 28.9Gi.\nEvictions would not be performed unless system daemons are consuming more than 3.1Gi of memory.\n\nIf system daemons do not use up all their reservation, with the above example,\npods would face memcg OOM kills from their bounding cgroup before node evictions kick in.\nTo better enforce QoS under this situation, the node applies the hard eviction thresholds to\nthe top-level cgroup for all pods to be Node Allocatable + Eviction Hard Thresholds.\n\nIf system daemons do not use up all their reservation, the node will evict pods whenever\nthey consume more than 28.9Gi of memory. If eviction does not occur in time, a pod\nwill be OOM killed if pods consume 29Gi of memory.\n\nThe scheduler now uses the value of node.Status.Allocatable instead of\nnode.Status.Capacity to decide if a node will become a candidate for pod\nscheduling.\n\nBy default, the node will report its machine capacity as fully schedulable by\nthe cluster.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/backup_restore.html", "title": "Atomic Registry Latest | Cluster Administration | Backup and Restore", "content": "\nIn Atomic Registry, you can back up (saving state to separate storage) and\nrestore (recreating state from separate storage) at the cluster level. There\nis also some preliminary support for per-project backup.\nThe full state of a cluster installation includes:\n\nThis topic does not cover how to back up and restore\npersistent\nstorage, as those topics are left to the underlying storage provider. However,\nan example of how to perform a generic backup of\napplication data is provided.\n\nThis topic only provides a generic way of backing up applications and the\nAtomic Registry cluster. It can not take into account custom requirements.\nTherefore, you should create a full backup and restore procedure. To prevent\ndata loss, necessary precautions should be taken.\n\nNote the location of the etcd data directory (or $ETCD_DATA_DIR in the\nfollowing sections), which depends on how etcd is deployed.\n\nAlthough this step is not strictly necessary, doing so ensures that the etcd\ndata is fully synchronized.\n\nIf etcd is running on more than one host,\nthe various instances regularly synchronize their data,\nso creating a backup for one of them is sufficient.\n\nFor a container-based installation, you must use docker exec to run etcdctl\ninside the container.\n\nTo restore the cluster:\n\nThis should be done in the\nsame way that\nAtomic Registry was previously installed.\n\nTo do so, edit the /usr/lib/systemd/system/etcd.service file, and add\n--force-new-cluster:\n\nThen, restart the etcd service:\n\nWhen using an external etcd host, you must first restore the etcd backup\nby creating a new, single node etcd cluster. If using external etcd with\nmultiple members, you must then also add any additional etcd members to the\ncluster one by one.\n\nHowever, the details of the restoration process differ between\nembedded and\nexternal etcd. See the following\nsection and follow the relevant steps\nbefore\nBringing OpenShift\nServices Back Online.\n\nRestore your etcd backup and configuration:\n\nThe $ETCD_DIR location differs between external and embedded etcd.\n\nVerify etcd has started successfully by checking the output from the above\ncommand, which should look similar to the following near the end:\n\nChoose a system to be the initial etcd member, and restore its etcd backup and\nconfiguration:\n\nThe $ETCD_DIR location differs between external and embedded etcd.\n\nTo do so, edit the /usr/lib/systemd/system/etcd.service file, and add\n--force-new-cluster:\n\nThen restart the etcd service:\n\nTo add additional etcd members to the cluster, you must first adjust the default\nlocalhost peer in the peerURLs value for the first member:\n\nAlternatively, you can use curl:\n\nEach member must be fully added and brought online one at a time. When adding\neach additional member to the cluster, the peerURLs list must be correct for\nthat point in time, so it will grow by one for each member added. The etcdctl\nmember add command will output the values that need to be set in the\netcd.conf file as you add each member, as described in the following\ninstructions.\n\nIn cases where etcd members have failed and you still have a quorum of etcd\ncluster members running, you can use the surviving members to\nadd additional etcd members without downtime.\n\nSuggested Cluster Size\n\nHaving a cluster with an odd number of etcd hosts can account for fault\ntolerance. Having an odd number of etcd hosts does not change the number needed\nfor a quorum, but increases the tolerance for failure. For example, a cluster\nsize of three members, quorum is two leaving a failure tolerance of\none. This ensures the cluster will continue to operate if two of the members are\nhealthy.\n\nHaving an in-production cluster of three etcd hosts is recommended.\n\nThe following presumes you have a backup of the /etc/etcd configuration for\nthe etcd hosts.\n\nEnsure version etcd-2.3.7-4.el7.x86_64 or greater is installed, and that the\nsame version is installed on each host.\n\nEnsure version etcd-2.3.7-4.el7.x86_64 or greater is installed, and that the\nsame version is installed on the new host.\n\nStop the etcd service on the failed etcd member:\n\nCopy the three environment variables in the etcdctl member add output. They will be used later.\n\nReplace the IP address with the \"NEW_ETCD\" value for:\n\nFor replacing failed members, you will need to remove the failed hosts from the\netcd configuration.\n\nOn a single master cluster installation:\n\nOn a multi-master cluster installation, on each master:\n\nThe procedure to add an etcd member is complete.\n\nOn each Atomic Registry master, restore your master and node configuration from\nbackup and enable and restart all relevant services.\n\nOn the master in a single master cluster:\n\nOn each master in a multi-master cluster:\n\nOn each Atomic Registry node, restore your node-config.yaml file from backup\nand enable and restart the atomic-openshift-node service:\n\nYour Atomic Registry cluster should now be back online.\n\nA future release of Atomic Registry will feature specific support for\nper-project back up and restore.\n\nFor now, to back up API objects at the project level, use oc export for each\nobject to be saved. For example, to save the deployment configuration frontend\nin YAML format:\n\nTo back up all of the project (with the exception of cluster objects like\nnamespaces and projects):\n\nSometimes custom policy\nrole\nbindings are used in a project. For example, a project administrator can give\nanother user a certain role in the project and grant that user project access.\n\nThese role bindings can be exported:\n\nIf custom service accounts are created in a project, these need to be exported:\n\nCustom secrets like source control management secrets (SSH Public Keys,\nUsername/Password) should be exported if they are used:\n\nIf the an application within a project uses a persistent volume through a\npersistent volume claim (PVC), these should be backed up:\n\nTo restore a project, recreate the project and recreate all all of the objects\nthat were exported during the backup:\n\nSome resources can fail to be created (for example, pods and default service\naccounts).\n\nIn many cases, application data can be backed up using the oc rsync command,\nassuming rsync is installed within the container image. The Red Hat rhel7\nbase image does contain rsync. Therefore, all images that are based on rhel7\ncontain it as well. See Troubleshooting and Debugging CLI Operations - rsync.\n\nThis is a generic backup of application data and does not take into account\napplication-specific backup procedures, for example special export/import\nprocedures for database systems.\n\nOther means of backup may exist depending on the type of the persistent volume\n(for example, Cinder, NFS, Gluster, or others).\n\nThe paths to back up are also application specific. You can determine\nwhat path to back up by looking at the mountPath for volumes in the\ndeploymentconfig.\n\nThis type of application data backup can only be performed while an application\npod is currently running.\n\nThe process for restoring application data is similar to the\napplication backup procedure using the oc rsync\ntool. The same restrictions apply and the process of restoring application data\nrequires a persistent volume.\n\nDepending on the application, you may be required to restart the application.\n\nAlternatively, you can scale down the deployment to 0, and then up again:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/idling_applications.html", "title": "Atomic Registry Latest | Cluster Administration | Idling Applications", "content": "\nAs an Atomic Registry administrator, you can idle applications to reduce\nresource consumption. This is useful when deployed on a public cloud where cost\nis related to resource consumption.\n\nIf any scalable resources are not in use, Atomic Registry discovers, then idles\nthem, by scaling them to 0 replicas. When network traffic is directed to the\nresources, they are unidled by scaling up the replicas, then operation\ncontinues.\n\nApplications are made of services, as well as other scalable resources, such as\ndeployment configurations. The action of idling an application involves idling\nall associated resources.\n\nIdling an application involves finding the scalable resources (deployment\nconfigurations, replication controllers, and others) associated with a service.\nIdling an application finds the service and marks it as idled, scaling down the\nresources to zero replicas.\n\nYou can use the oc idle command to\nidle\na single service, or use the --resource-names-file option to\nidle\nmultiple services.\n\nIdle a single service with the following command:\n\nIdle multiple services by creating a list of the desired services, then using the --resource-names-file option with the oc idle command.\n\nThis is helpful if an application spans across a set of services, or when idling\nmultiples services in conjunction with a script in order to idle applications in\nbulk.\n\nApplication services become active again when they receive network traffic and\nwill be scaled back up their previous state. This includes both traffic to the\nservices and traffic passing through routes.\n\nAutomatic unidling by a router is currently only supported by the default HAProxy router.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/image_policy.html", "title": "Atomic Registry Latest | Cluster Administration | Image Policy", "content": "\nYou can control which images are allowed to run on your cluster using the ImagePolicy\nadmission plug-in (currently considered beta). It allows you to control:\n\nTo enable this feature, configure the plug-in in master-config.yaml:\n\nFor example, use the information above, then test like this:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/index.html", "title": "Atomic Registry Latest | Cluster Administration | Overview", "content": "\n\u00a0\nThese Cluster Administration topics cover the day-to-day tasks for managing\nyour Atomic Registry cluster and other advanced configuration topics.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/limits.html", "title": "Atomic Registry Latest | Cluster Administration | Setting Limit Ranges", "content": "\nA limit range, defined by a LimitRange object, enumerates\ncompute resource\nconstraints in a project at the pod,\ncontainer, image, image stream, and persistent volume claim level, and specifies the amount of resources\nthat a pod, container, image, image stream, or persistent volume claim can consume.\n\nAll resource create and modification requests are evaluated against each\nLimitRange object in the project. If the resource violates any of the\nenumerated constraints, then the resource is rejected. If the resource does not\nset an explicit value, and if the constraint supports a default value, then the\ndefault value is applied to the resource.\n\nBoth core and Atomic Registry resources can be specified in just one limit range\nobject. They are separated here into two examples for clarity.\n\nSupported Resources:\n\nSupported Constraints:\n\nPer container, the following must hold true if specified:\n\nMin\n\nMin[resource] less than or equal to container.resources.requests[resource]\n(required) less than or equal to container/resources.limits[resource]\n(optional)\n\nIf the configuration defines a min CPU, then the request value must be greater\nthan the CPU value. A limit value does not need to be specified.\n\nMax\n\ncontainer.resources.limits[resource] (required) less than or equal to\nMax[resource]\n\nIf the configuration defines a max CPU, then you do not need to define a\nrequest value, but a limit value does need to be set that satisfies the maximum\nCPU constraint.\n\nMaxLimitRequestRatio\n\nMaxLimitRequestRatio[resource] less than or equal to (\ncontainer.resources.limits[resource] /\ncontainer.resources.requests[resource])\n\nIf a configuration defines a maxLimitRequestRatio value, then any new\ncontainers must have both a request and limit value. Additionally,\nAtomic Registry calculates a limit to request ratio by dividing the limit by the\nrequest.\n\nFor example, if a container has cpu: 500 in the limit value, and\ncpu: 100 in the request value, then its limit to request ratio for cpu is\n5. This ratio must be less than or equal to the maxLimitRequestRatio.\n\nSupported Defaults:\n\nSupported Resources:\n\nSupported Constraints:\n\nAcross all containers in a pod, the following must hold true:\n\nMin\n\nMin[resource] less than or equal to container.resources.requests[resource]\n(required) less than or equal to container.resources.limits[resource]\n(optional)\n\nMax\n\ncontainer.resources.limits[resource] (required) less than or equal to\nMax[resource]\n\nMaxLimitRequestRatio\n\nMaxLimitRequestRatio[resource] less than or equal to (\ncontainer.resources.limits[resource] /\ncontainer.resources.requests[resource])\n\nSupported Resources:\n\nResource type name:\n\nPer image, the following must hold true if specified:\n\nMax\n\nimage.dockerimagemetadata.size less than or equal to Max[resource]\n\nThe image size is not always available in the manifest of an uploaded image.\nThis is especially the case for images built with Docker 1.10 or higher and\npushed to a v2 registry. If such an image is pulled with an older Docker daemon,\nthe image manifest will be converted by the registry to schema v1 lacking all\nthe size information. No storage limit set on images will prevent it from being\nuploaded.\n\nThe issue is being\naddressed.\n\nSupported Resources:\n\nResource type name:\n\nPer image stream, the following must hold true if specified:\n\nMax[openshift.io/image-tags]\n\nlength( uniqueimagetags( imagestream.spec.tags ) ) less than or equal to Max[openshift.io/image-tags]\n\nuniqueimagetags returns unique references to images of given spec tags.\n\nMax[openshift.io/images]\n\nlength( uniqueimages( imagestream.status.tags ) ) less than or equal to Max[openshift.io/images]\n\nuniqueimages returns unique image names found in status tags. The name equals\nimage\u2019s digest.\n\nResource openshift.io/image-tags represents unique\nimage\nreferences. Possible references are an ImageStreamTag, an\nImageStreamImage and a DockerImage. They may be created using commands\noc tag and oc import-image or by using\ntag tracking. No distinction\nis made between internal and external references. However, each unique reference\ntagged in the image stream\u2019s specification is counted just once. It does not\nrestrict pushes to an internal container registry in any way, but is useful for tag\nrestriction.\n\nResource openshift.io/images represents unique image names recorded in image\nstream status. It allows for restriction of a number of images that can be\npushed to the internal registry. Internal and external references are not\ndistinguished.\n\nSupported Resources:\n\nSupported Constraints:\n\nAcross all persistent volume claims in a project, the following must hold true:\n\nMin\n\nMin[resource] \u21d0 claim.spec.resources.requests[resource] (required)\n\nMax\n\nclaim.spec.resources.requests[resource] (required) \u21d0 Max[resource]\n\nTo apply a limit range to a project, create a limit range\nobject definition on your file system to your desired specifications, then run:\n\nYou can view any limit ranges defined in a project by navigating in the web\nconsole to the project\u2019s Settings tab.\n\nYou can also use the CLI to view limit range details:\n\nRemove any active limit range to no longer enforce the limits of a project:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/manage_authorization_policy.html", "title": "Atomic Registry Latest | Cluster Administration | Managing Authorization Policies", "content": "\nYou can use the CLI to view\nauthorization\npolicies and the administrator CLI to manage the\nroles and bindings\nwithin a policy.\n\nThe web console also provides some basic management of authorization roles.\nThrough the web console users and groups may be assigned registry-admin,\nregistry-editor or registry-viewer.\n\nRoles grant\nvarious levels of access in the system-wide\ncluster\npolicy as well as project-scoped\nlocal\npolicies.\nUsers\nand groups can be associated with, or bound to, multiple roles at the same\ntime.  You can view details about the roles and their bindings using the oc\ndescribe command.\n\nUsers with the cluster-admin\ndefault role\nin the cluster policy can view cluster policy and all local policies. Users with\nthe admin\ndefault role\nin a given local policy can view that project-scoped policy.\n\nReview a full list of verbs in the\nEvaluating\nAuthorization section.\n\nTo view the cluster roles and their associated rule sets in the cluster policy:\n\nTo view the current set of cluster bindings, which shows the users and groups that are bound to various roles:\n\nWhile the list of local roles and their associated rule sets are not viewable\nwithin a local policy, all of the\ndefault roles\nare still applicable and can be added to users or groups, other than the\ncluster-admin default role. The local bindings, however, are viewable.\n\nTo view the current set of local bindings, which shows the users and groups that\nare bound to various roles:\n\nBy default, the current project is used when viewing local policy.\nAlternatively, a project can be specified with the -n flag. This is useful for\nviewing the local policy of another project, if the user already has the admin\ndefault role\nin it.\n\nBy default in a local policy, only the binding for the admin role is\nimmediately listed. However, if other\ndefault roles\nare added to users and groups within a local policy, they become listed as well.\n\nAdding, or binding, a\nrole to\nusers\nor groups gives the user or group the relevant access granted by the role. You\ncan add and remove roles to and from users and groups using oadm policy\ncommands.\n\nWhen managing a user or group\u2019s associated roles for a local policy using the\nfollowing operations, a project may be specified with the -n flag. If it is\nnot specified, then the current project is used.\n\nYou can also manage role bindings for the cluster policy using the following\noperations. The -n flag is not used for these operations because the\ncluster policy uses non-namespaced resources.\n\nFor example, you can add the admin role to the alice user in joe-project\nby running:\n\nYou can then view the local bindings and verify the addition in the output:\n\nBy default, project developers do not have the permission to create\ndaemonsets. As a cluster\nadministrator, you can grant them the abilities.\n\nTo create a local role for a project, you can either copy and modify an existing\nrole or build a new role from scratch. It is recommended that you build it from\nscratch so that you understand each of the permissions assigned.\n\nTo copy the cluster role view to use as a local role, run:\n\nTo create a new role from scratch, save this snippet into the file\nrole_exampleview.yaml:\n\nThen, to use the current project, run:\n\nOptionally, annotate it with a description.\n\nTo use the new role, run:\n\nA clusterrolebinding is a role binding that exists at the cluster level. A\nrolebinding exists at the project level. This can be confusing. The\nclusterrolebinding view must be assigned to a user within a project for that\nuser to view the project. Local roles are only created if a cluster role does\nnot provide the set of permissions needed for a particular situation, which is\nunlikely.\n\nSome cluster role names are initially confusing. The clusterrole\nclusteradmin can be assigned to a user within a project, making it appear that\nthis user has the privileges of a cluster administrator. This is not the case.\nThe clusteradmin cluster role bound to a certain project is more like a super\nadministrator for that project, granting the permissions of the cluster role\nadmin, plus a few additional permissions like the ability to edit rate limits.\nThis can appear especially confusing via the web console UI, which does not list\ncluster policy (where cluster administrators exist). However, it does list local\npolicy (where a locally bound clusteradmin may exist).\n\nWithin a project, project administrators should be able to see rolebindings,\nnot clusterrolebindings.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/manage_users.html", "title": "Atomic Registry Latest | Cluster Administration | Managing Users", "content": "\nThis topic describes the management of\nuser accounts,\nincluding how new user accounts are created in Atomic Registry and how they can\nbe deleted.\n\nAfter new users log in to Atomic Registry, an account is created for that user\nper the identity\nprovider configured on the master. The cluster administrator can\nmanage the access level of\neach user.\n\nAtomic Registry user configuration is stored in several locations within\nAtomic Registry. Regardless of the identity provider, Atomic Registry internally\nstores details like role-based access control (RBAC) information and group\nmembership. To completely remove user information, this data must be removed in\naddition to the user account.\n\nIn Atomic Registry, two object types contain user data outside the\nidentification provider: user and identity.\n\nTo get the current list of users:\n\nTo get the current list of identities:\n\nNote the matching UID between the two object types. If you attempt to change the\nauthentication provider after starting to use Atomic Registry, the user names\nthat overlap will not work because of the entries in the identity list, which\nwill still point to the old authentication method.\n\nTo add a label to a user or group:\n\nFor example, if the user name is theuser and the label is level=gold:\n\nTo remove the label:\n\nTo show labels for a user or group:\n\nTo delete a user:\n\nThe identity of the user is related to the identification provider you use. Get\nthe provider name from the user record in oc get user.\n\nIn this example, the identity provider name is htpasswd_auth. The command is:\n\nIf you skip this step, the user will not be able to log in again.\n\nAfter you complete these steps, a new account will be created in Atomic Registry\nwhen the user logs in again.\n\nIf your intention is to prevent the user from being able to log in again (for\nexample, if an employee has left the company and you want to permanently delete\nthe account), you can also remove the user from your authentication back end\n(like htpasswd, kerberos, or others) for the configured identity\nprovider.\n\nFor example, if you are using htpasswd, delete the entry in the htpasswd\nfile that is configured for Atomic Registry with the user name and password.\n\nFor external identification management like Lightweight Directory Access\nProtocol (LDAP) or Red Hat Identity Management (IdM), use the user management\ntools to remove the user entry.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/managing_networking.html", "title": "Atomic Registry Latest | Cluster Administration | Managing Networking", "content": "\nThis topic describes the management of the overall\ncluster\nnetwork, including project isolation and outbound traffic control.\n\nPod-level networking features, such as per-pod bandwidth limits, are discussed\nin Managing\nPods.\n\nWhen your cluster is configured to use\nthe ovs-multitenant SDN\nplug-in, you can manage the separate pod overlay networks for projects using\nthe administrator CLI.\n\nTo join projects to an existing project network:\n\nIn the above example, all the pods and services in <project2> and <project3>\ncan now access any pods and services in <project1> and vice versa. Services\ncan be accessed either by IP or fully-qualified DNS name\n(<service>.<pod_namespace>.svc.cluster.local). For example, to access a\nservice named db in a project myproject, use db.myproject.svc.cluster.local.\n\nAlternatively, instead of specifying specific project names, you can use the\n--selector=<project_selector> option.\n\nTo isolate the project network in the cluster and vice versa, run:\n\nIn the above example, all of the pods and services in <project1> and\n<project2> can not access any pods and services from other non-global\nprojects in the cluster and vice versa.\n\nAlternatively, instead of specifying specific project names, you can use the\n--selector=<project_selector> option.\n\nTo allow projects to access all pods and services in the cluster and vice versa:\n\nIn the above example, all the pods and services in <project1> and <project2>\ncan now access any pods and services in the cluster and vice versa.\n\nAlternatively, instead of specifying specific project names, you can use the\n--selector=<project_selector> option.\n\nIn Atomic Registry, host name collision prevention for routes and ingress\nobjects is enabled by default. This means that the host name in a route or\ningress object can only be set on creation and not edited afterwards. Disabling\nhost name collision prevention lets you edit a host name for ingress objects after creation.\nHowever, because Atomic Registry uses the object creation timestamp to determine\nthe oldest route or ingress object for a given host name, the route or ingress\nobject can hijack a host name with a newer route. This can happen if an older\nroute changes its host name, or if an ingress object is introduced.\n\nThis is relevant to Atomic Registry installations that depend upon Kubernetes\nbehavior, including allowing the host names in ingress objects be edited.\n\nAs a cluster administrator you can allocate a number of static IP addresses to a\nspecific node at the host level. If an application developer needs a dedicated\nIP address for their application service, they can request one during the\nprocess they use to ask for firewall access. They can then deploy an egress\nrouter from the developer\u2019s project, using a nodeSelector in the deployment\nconfiguration to ensure that the pod lands on the host with the pre-allocated\nstatic IP address.\n\nThe egress pod\u2019s deployment declares one of the source IPs, the destination IP\nof the protected service, and a gateway IP to reach the destination. After the\npod is deployed, you can\ncreate\na service to access the egress router pod, then add that source IP to the\ncorporate firewall. The developer then has access information to the egress\nrouter service that was created in their project, for example,\nservice.project.cluster.domainname.com.\n\nWhen the developer needs to access the external, firewalled service, they can\ncall out to the egress router pod\u2019s service\n(service.project.cluster.domainname.com) in their application (for example,\nthe JDBC connection information) rather than the actual protected service URL.\n\nAs an Atomic Registry cluster administrator, you can control egress traffic in three ways:\n\nAs an Atomic Registry cluster administrator, you can use egress firewall policy\nto limit the external addresses that some or all pods can access from within the\ncluster, so that:\n\nOr,\n\nOr,\n\nYou can configure projects to have different egress policies. For example,\nallowing <project A> access to a specified IP range, but denying the same\naccess to <project B>. Or restrict application developers from updating from\n(Python) pip mirrors, and forcing updates to only come from desired sources.\n\nYou must have the\novs-multitenant plug-in enabled in order to limit pod access via egress policy.\n\nProject administrators can neither create EgressNetworkPolicy objects, nor\nedit the ones you create in their project. There are also several other\nrestrictions on where EgressNetworkPolicy can be created:\n\nViolating any of these restrictions results in broken egress policy for the\nproject, and may cause all external network traffic to be dropped.\n\nUse the oc command or the REST API to configure egress policy. You can use\noc [create|replace|delete] to manipulate EgressNetworkPolicy objects. The\napi/swagger-spec/oapi-v1.json file has API-level details on how the objects\nactually work.\n\nTo configure egress policy:\n\nWhen the example above is added to a project, it allows traffic to IP range\n1.2.3.0/24 and domain name www.foo.com, but denies access to all other\nexternal IP addresses. Traffic to other pods is not affected because the policy\nonly applies to external traffic.\n\nThe rules in an EgressNetworkPolicy are checked in order, and the first one\nthat matches takes effect. If the three rules in the above example were\nreversed, then traffic would not be allowed to 1.2.3.0/24 and www.foo.com\nbecause the 0.0.0.0/0 rule would be checked first, and it would match and deny\nall traffic.\n\nDomain name updates are polled based on the TTL (time to live) value of the\ndomain of the local non-authoritative server, or 30 minutes if the TTL is unable\nto be fetched. The pod should also resolve the domain from the same local\nnon-authoritative server when necessary, otherwise the IP addresses for the\ndomain perceived by the egress network policy controller and the pod will be\ndifferent, and the egress network policy may not be enforced as expected. In the\nabove example, suppose www.foo.com resolved to 10.11.12.13 and has a DNS TTL\nof one minute, but was later changed to 20.21.22.23. Atomic Registry will then\ntake up to one minute to adapt to these changes.\n\nThe egress firewall always allows pods access to the external interface of the\nnode the pod is on for DNS resolution. If your DNS resolution is not handled by\nsomething on the local node, then you will need to add egress firewall rules\nallowing access to the DNS server\u2019s IP addresses if you are using domain names\nin your pods. The default installer\nsets up a local dnsmasq, so if you are using that setup you will not need to add extra rules.\n\nExposing services by creating\nroutes will ignore\nEgressNetworkPolicy. Egress network policy service endpoint filtering is done\nat the node kubeproxy. When the router is involved, kubeproxy is bypassed\nand egress network policy enforcement is not applied. Administrators can prevent\nthis bypass by limiting access to create routes.\n\nThe Atomic Registry egress router runs a service that redirects traffic to a\nspecified remote server, using a private source IP address that is not used for\nanything else. The service allows pods to talk to servers that are set up\nto only allow access from whitelisted IP addresses.\n\nThe egress router is not intended for every outgoing connection. Creating large\nnumbers of egress routers can push the limits of your network hardware. For\nexample, creating an egress router for every project or application could exceed\nthe number of local MAC addresses that the network interface can handle before\nfalling back to filtering MAC addresses in software.\n\nDeployment Considerations\n\nThe Egress router adds a second IP address and MAC address to the node\u2019s primary\nnetwork interface. If you are not running Atomic Registry on bare metal, you may\nneed to configure your hypervisor or cloud provider to allow the additional\naddress.\n\nEgress Router Modes\n\nThe egress router can run in two different modes:\nredirect mode and\nHTTP proxy mode.\nRedirect mode works for all services except for HTTP and HTTPS. For HTTP and\nHTTPS services, use HTTP proxy mode.\n\nIn redirect mode, the egress router sets up iptables rules to redirect traffic\nfrom its own IP address to one or more destination IP addresses. Client pods\nthat want to make use of the reserved source IP address must be modified to\nconnect to the egress router rather than connecting directly to the destination\nIP.\n\nTo check to see if the pod has been created:\n\nYour pods can now connect to this service. Their connections are redirected to\nthe corresponding ports on the external server, using the reserved egress IP\naddress.\n\nThe egress router setup is performed by an \"init container\" created from the\nimage, and that container is run privileged so that it can configure the Macvlan\ninterface and set up iptables rules. After it finishes setting up\nthe iptables rules, it exits and the\ncontainer will run (doing nothing) until the pod is killed.\n\nThe environment variables tell the egress-router image what addresses to use; it\nwill configure the Macvlan interface to use EGRESS_SOURCE as its IP address,\nwith EGRESS_GATEWAY as its gateway.\n\nNAT rules are set up so that connections to any TCP or UDP port on the\npod\u2019s cluster IP address are redirected to the same port on\nEGRESS_DESTINATION.\n\nIf only some of the nodes in your cluster are capable of claiming the specified\nsource IP address and using the specified gateway, you can specify a\nnodeName or nodeSelector indicating which nodes are acceptable.\n\nIn the previous example, connections to the egress pod (or its corresponding\nservice) on any port are redirected to a single destination IP. You can also\nconfigure different destination IPs depending on the port:\n\nEach line of EGRESS_DESTINATION can be one of three types:\n\nFor a large or frequently-changing set of destination mappings, you\ncan use a ConfigMap to externally maintain the list, and have the egress router\npod read it from there. This comes with the advantage of project administrators\nbeing able to edit the ConfigMap, whereas they may not be able to edit the Pod\ndefinition directly, because it contains a privileged container.\n\nNote that you can put blank lines and comments into this file\n\nHere egress-routes is the name of the ConfigMap object being\ncreated and my-egress-destination.txt is the name of the file the\ndata is being read from.\n\nThe egress router does not automatically update when the ConfigMap changes.\nRestart the pod to get updates.\n\nIn HTTP proxy mode, the egress router runs as an HTTP proxy on port 8080.\nThis only works for clients talking to HTTP or HTTPS-based services, but usually\nrequires fewer changes to the client pods to get them to work. Programs can be\ntold to use an HTTP proxy by setting an environment variable.\n\nYou can specify any of the following for the EGRESS_HTTP_PROXY_DESTINATION\nvalue. You can also use *, meaning \"allow connections to all remote\ndestinations\". Each line in the configuration specifies one group of connections\nto allow or deny:\n\nUsing the http_proxy and https_proxy environment variables is not necessary\nfor all setups. If the above does not create a working setup, then consult the\ndocumentation for the tool or software you are running in the pod.\n\nYou can also specify the EGRESS_HTTP_PROXY_DESTINATION using a\nConfigMap, similarly to\nthe redirecting egress router example above.\n\nUsing a replication controller, you can ensure that there is always one copy of the egress router pod in order to prevent downtime.\n\nSome cluster administrators may want to perform actions on outgoing\ntraffic that do not fit within the model of EgressNetworkPolicy or the\negress router. In some cases, this can be done by creating iptables\nrules directly.\n\nFor example, you could create rules that log traffic to particular\ndestinations, or to prevent more than a certain number of outgoing\nconnections per second.\n\nAtomic Registry does not provide a way to add custom iptables rules\nautomatically, but it does provide a place where such rules can be\nadded manually by the administrator. Each node, on startup, will\ncreate an empty chain called OPENSHIFT-ADMIN-OUTPUT-RULES in the\nfilter table (assuming that the chain does not already exist). Any\nrules added to that chain by an administrator will be applied to all\ntraffic going from a pod to a destination outside the cluster (and not\nto any other traffic).\n\nThere are a few things to watch out for when using this functionality:\n\nAt this time, multicast is best used for low bandwidth coordination or service\ndiscovery and not a high-bandwidth solution.\n\nMulticast traffic between Atomic Registry pods is disabled by default. You can\nenable Multicast on a per-project basis by setting an annotation on the\nproject\u2019s corresponding netnamespace object:\n\nDisable multicast by removing the annotation:\n\nIf you have\njoined\nnetworks together, you will need to enable Multicast in each projects'\nnetnamespace in order for it to take effect in any of the projects. To enable\nMulticast in the default project, you must also enable it in all other\nprojects that have been\nmade\nglobal.\n\nMulticast global projects are not \"global\", but instead communicate with only\nother global projects via Multicast, not with all projects in the cluster, as is\nthe case with unicast.\n\nEnabling the Kubernetes NetworkPolicy is a Technology Preview feature only.\n\nKubernetes NetworkPolicy is not currently fully supported by Atomic Registry,\nand the ovs-subnet and ovs-multitenant plug-ins ignore NetworkPolicy\nobjects. However, a Technology Preview of NetworkPolicy support is available by\nusing the ovs-networkpolicy plug-in.\n\nIn a cluster\nconfigured\nto use the ovs-networkpolicy plug-in, network isolation is controlled\nentirely by NetworkPolicy objects and the associated Namespace annotation. In particular, by default, all projects are able to access pods\nin all other projects. The project to be isolated must first be configured to\nopt in to isolation by setting the proper annotation on its Namespace object,\nand then creating NetworkPolicy objects indicating the incoming connections to\nbe allowed.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/managing_pods.html", "title": "Atomic Registry Latest | Cluster Administration | Managing Pods", "content": "\nThis topic describes the management of\npods, including\nlimiting their run-once duration, and how much bandwidth they can use.\n\nYou can apply quality-of-service traffic shaping to a pod and effectively limit\nits available bandwidth. Egress traffic (from the pod) is handled by policing,\nwhich simply drops packets in excess of the configured rate. Ingress traffic (to\nthe pod) is handled by shaping queued packets to effectively handle data. The\nlimits you place on a pod do not affect the bandwidth of other pods.\n\nTo limit the bandwidth on a pod:\n\nA pod disruption budget is part of the\nKubernetes API, which can be\nmanaged with oc commands like other\nobject types. They\nallow the specification of safety constraints on pods during operations, such as\ndraining a node for maintenance.\n\nPodDisruptionBudget is an API object that specifies the minimum number or\npercentage of replicas that must be up at a time. Setting these in projects can\nbe helpful during node maintenance (such as scaling a cluster down or a cluster\nupgrade) and is only honored on voluntary evictions (not on node failures).\n\nA PodDisruptionBudget object\u2019s configuration consists of the following key\nparts:\n\nThe following is an example of a PodDisruptionBudget resource:\n\nIf you created a YAML file with the above object definition, you could add it to project with the following:\n\nYou can check for pod disruption budgets across all projects with the following:\n\nThe PodDisruptionBudget is considered healthy when there are at least\nminAvailable pods running in the system. Every pod above that limit can be\nevicted.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/managing_projects.html", "title": "Atomic Registry Latest | Cluster Administration | Managing Projects", "content": "\nIn Atomic Registry, projects are used to group and isolate related objects. As an administrator, you can give developers access to certain projects, allow them to create their own, and give them administrative rights within individual projects.\n\nYou can allow developers to create their own projects. There is an endpoint\nthat will provision a project according to a\ntemplate. The web console and oc new-project\ncommand use this endpoint when a developer creates a new project.\n\nThe API server automatically provisions projects based on the template that is\nidentified by the projectRequestTemplate parameter of the master-config.yaml\nfile. If the parameter is not defined, the API server creates a default template\nthat creates a project with the requested name, and assigns the requesting user\nto the \"admin\" role for that project.\n\nTo create your own custom project template:\n\nWhen a project request is submitted, the API substitutes the following parameters into the template:\n\nAccess to the API is granted to developers with the\nself-provisioner\nrole and the self-provisioners cluster role binding. This role is available\nto all authenticated developers by default.\n\nRemoving the self-provisioners\ncluster role\nfrom authenticated user groups will deny permissions for self-provisioning any new projects.\n\nWhen disabling self-provisioning, set the projectRequestMessage parameter in the\nmaster-config.yaml file to instruct developers on how to request a new\nproject. This parameter is a string that will be presented to the developer in\nthe web console and command line when they attempt to self-provision a project.\nFor example:\n\nor:\n\nThe number of self-provisioned projects requested by a given user can be limited\nwith the ProjectRequestLimit\nadmission\ncontrol plug-in.\n\nIf your project request template was created in Atomic Registry 3.1 or earlier\nusing the process described in\nModifying the Template for New\nProjects, then the generated template does not include the annotation\nopenshift.io/requester: ${PROJECT_REQUESTING_USER}, which is used for the\nProjectRequestLimitConfig. You must add the annotation.\n\nIn order to specify limits for users, a configuration must be specified for the\nplug-in within the master configuration file\n(/etc/origin/master/master-config.yaml). The plug-in configuration takes a\nlist of user label selectors and the associated maximum project requests.\n\nSelectors are evaluated in order. The first one matching the current user will\nbe used to determine the maximum number of projects. If a selector is not\nspecified, a limit applies to all users. If a maximum number of projects is not\nspecified, then an unlimited number of projects are allowed for a specific\nselector.\n\nThe following configuration sets a global limit of 2 projects per user while allowing 10\nprojects for users with a label of level=advanced and unlimited projects for\nusers with a label of level=admin.\n\nManaging\nUser and Group Labels provides further guidance on how to add, remove, or show\nlabels for users and groups.\n\nOnce your changes are made, restart Atomic Registry for the changes to take\neffect.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/multiproject_quota.html", "title": "Atomic Registry Latest | Cluster Administration | Setting Multi-Project Quotas", "content": "\nA multi-project quota, defined by a ClusterResourceQuota object, allows\nquotas to be shared across\nmultiple projects. Resources used in each selected project will be aggregated\nand that aggregate will be used to limit resources across all the selected\nprojects.\n\nProjects can be selected based on either annotation selection, label selection, or both.\nFor example:\n\ncreates:\n\nThis multi-project quota document controls all projects requested by\n<user-name> using the default project request endpoint. You are limited to 10\npods and 20 secrets.\n\nA project administrator is not allowed to create or modify the multi-project\nquota that limits his or her project, but the administrator is allowed to view the\nmulti-project quota documents that are applied to his or her project. The\nproject administrator can do this via the AppliedClusterResourceQuota\nresource.\n\nproduces:\n\nDue to the locking consideration when claiming quota allocations, the number of\nactive projects selected by a multi-project quota is an important consideration.\nSelecting more than 100 projects under a single multi-project quota may have\ndetrimental effects on API server responsiveness in those projects.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/quota.html", "title": "Atomic Registry Latest | Cluster Administration | Setting Quotas", "content": "\nA resource quota, defined by a ResourceQuota object, provides constraints\nthat limit aggregate resource consumption per project. It can limit the quantity\nof objects that can be created in a project by type, as well as the total amount\nof compute resources and storage that may be consumed by resources in that project.\n\nThe following describes the set of compute resources and object types that may be\nmanaged by a quota.\n\nA pod is in a terminal state if status.phase in (Failed, Succeeded) is true.\n\ncpu\n\nThe sum of CPU requests across all pods in a non-terminal state cannot exceed\nthis value. cpu and requests.cpu are the same value and can be used\ninterchangeably.\n\nmemory\n\nThe sum of memory requests across all pods in a non-terminal state cannot\nexceed this value. memory and requests.memory are the same value and can\nbe used interchangeably.\n\nrequests.cpu\n\nThe sum of CPU requests across all pods in a non-terminal state cannot exceed\nthis value. cpu and requests.cpu are the same value and can be used\ninterchangeably.\n\nrequests.memory\n\nThe sum of memory requests across all pods in a non-terminal state cannot\nexceed this value. memory and requests.memory are the same value and can\nbe used interchangeably.\n\nlimits.cpu\n\nThe sum of CPU limits across all pods in a non-terminal state cannot exceed\nthis value.\n\nlimits.memory\n\nThe sum of memory limits across all pods in a non-terminal state cannot exceed\nthis value.\n\nrequests.storage\n\nThe sum of storage requests across all persistent volume claims in any state cannot\nexceed this value.\n\npersistentvolumeclaims\n\nThe total number of persistent volume claims that can exist in the project.\n\n<storage-class-name>.storageclass.storage.k8s.io/requests.storage\n\nThe sum of storage requests across all persistent volume claims in any state that have a matching storage class, cannot exceed this value.\n\n<storage-class-name>.storageclass.storage.k8s.io/persistentvolumeclaims\n\nThe total number of persistent volume claims with a matching storage class that can exist in the project.\n\npods\n\nThe total number of pods in a non-terminal state that can exist in the project.\n\nreplicationcontrollers\n\nThe total number of replication controllers that can exist in the project.\n\nresourcequotas\n\nThe total number of resource quotas that can exist in the project.\n\nservices\n\nThe total number of services that can exist in the project.\n\nsecrets\n\nThe total number of secrets that can exist in the project.\n\nconfigmaps\n\nThe total number of ConfigMap objects that can exist in the project.\n\npersistentvolumeclaims\n\nThe total number of persistent volume claims that can exist in the project.\n\nopenshift.io/imagestreams\n\nThe total number of image streams that can exist in the project.\n\nEach quota can have an associated set of scopes. A quota will only\nmeasure usage for a resource if it matches the intersection of enumerated\nscopes.\n\nAdding a scope to a quota restricts the set of resources to which that quota can\napply. Specifying a resource outside of the allowed set results in a validation\nerror.\n\nTerminating\n\nMatch pods where spec.activeDeadlineSeconds >= 0.\n\nNotTerminating\n\nMatch pods where spec.activeDeadlineSeconds is nil.\n\nBestEffort\n\nMatch pods that have best effort quality of service for either cpu or\nmemory.\n\nNotBestEffort\n\nMatch pods that do not have best effort quality of service for cpu and\nmemory.\n\nA BestEffort scope restricts a quota to limiting the following resources:\n\nA Terminating, NotTerminating, or NotBestEffort scope restricts a quota\nto tracking the following resources:\n\nAfter a resource quota for a project is first created, the project restricts the\nability to create any new resources that may violate a quota constraint until it\nhas calculated updated usage statistics.\n\nAfter a quota is created and usage statistics are updated, the project accepts\nthe creation of new content. When you create or modify resources, your quota\nusage is incremented immediately upon the request to create or modify the\nresource.\n\nWhen you delete a resource, your quota use is decremented during the next full\nrecalculation of quota statistics for the project.\nA configurable amount of time determines\nhow long it takes to reduce quota usage statistics to their current observed\nsystem value.\n\nIf project modifications exceed a quota usage limit, the server denies the\naction, and an appropriate error message is returned to the user explaining the\nquota constraint violated, and what their currently observed usage stats are in\nthe system.\n\nWhen allocating\ncompute\nresources, each container may specify a request and a limit value each for\nCPU and memory. Quotas can restrict any of these values.\n\nIf the quota has a value specified for requests.cpu or requests.memory,\nthen it requires that every incoming container make an explicit request for\nthose resources. If the quota has a value specified for limits.cpu or\nlimits.memory, then it requires that every incoming container specify an\nexplicit limit for those resources.\n\nTo create a quota, first define the quota to your specifications in a file, for\nexample as seen in\nSample Resource\nQuota Definitions. Then, create using that file to apply it to a project:\n\nFor example:\n\nYou can view usage statistics related to any hard limits defined in a project\u2019s\nquota by navigating in the web console to the project\u2019s Settings tab.\n\nYou can also use the CLI to view quota details:\n\nWhen a set of resources are deleted, the synchronization time frame of resources\nis determined by the resource-quota-sync-period setting in the\n/etc/origin/master/master-config.yaml file.\n\nBefore quota usage is restored, a user may encounter problems when attempting to\nreuse the resources. You can change the resource-quota-sync-period setting\nto have the set of resources regenerate at the desired amount of time (in\nseconds) and for the resources to be available again:\n\nAfter making any changes, restart the master service to apply them.\n\nAdjusting the regeneration time can be helpful for creating resources and\ndetermining resource usage when automation is used.\n\nThe resource-quota-sync-period setting is designed to balance system\nperformance. Reducing the sync period can result in a heavy load on the master.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/scoped_tokens.html", "title": "Atomic Registry Latest | Cluster Administration | Scoped Tokens", "content": "\nA user may want to give another entity the power to act as they have, but only\nin a limited way. For example, a project administrator may want to delegate the\npower to create pods. One way to do this is to create a scoped token.\n\nA scoped token is a token that identifies as a given user, but is limited to\ncertain actions by its scope. Right now, only a cluster-admin can create\nscoped tokens.\n\nScopes are evaluated by converting the set of scopes for a token into a set of\nPolicyRules. Then, the request is matched against those rules. The request\nattributes must match at least one of the scope rules to be passed to the\n\"normal\" authorizer for further authorization checks.\n\nUser scopes are focused on getting information about a given user. They are\nintent-based, so the rules are automatically created for you:\n\nThe role scope allows you to have the same level of access as a given role\nfiltered by namespace.\n\nCaveat: This prevents escalating access. Even if the role allows access to\nresources like secrets, rolebindings, and roles, this scope will deny access\nto those resources. This helps prevent unexpected escalations. Many people do\nnot think of a role like edit as being an escalating role, but with access to\na secret it is.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/seccomp.html", "title": "Atomic Registry Latest | Cluster Administration | Restricting Application Capabilities Using Seccomp", "content": "\nSeccomp (secure computing mode) is used to restrict the set of system calls\napplications can make, allowing cluster administrators greater control over the\nsecurity of workloads running in Atomic Registry.\n\nSeccomp support is achieved via two annotations in the pod configuration:\n\nContainers are run with unconfined seccomp settings by default.\n\nFor detailed design information, refer to the\nseccomp\ndesign document.\n\nSeccomp is a feature of the Linux kernel. To ensure seccomp is enabled on your\nsystem, run:\n\nA seccomp profile is a json file providing syscalls and the appropriate action\nto take when a syscall is invoked.\n\nThe\ndefault\nprofile is sufficient in many cases, but the cluster administrator must define\nthe security constraints of an individual system.\n\nTo create your own custom profile, create a file on every node in the\nseccomp-profile-root directory.\n\nIf you are using the default docker/default profile, you do not need to\ncreate one.\n\nThe allowable formats of the seccompProfiles field include:\n\nFor example, if you are using the default docker/default profile, configure the restricted SCC with:\n\nTo ensure pods in your cluster run with a custom profile in the restricted SCC:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/service_accounts.html", "title": "Atomic Registry Latest | Cluster Administration | Configuring Service Accounts", "content": "\nService accounts provide a flexible way to control API access without sharing a\nregular user\u2019s credentials.\n\nEvery service account has an associated user name that can be granted roles,\njust like a regular user. The user name is derived from its project and name:\n\nFor example, to add the view role to the robot service account in the\ntop-secret project:\n\nEvery service account is also a member of two groups:\n\nFor example, to allow all service accounts in all projects to view resources in\nthe top-secret project:\n\nTo allow all service accounts in the managers project to edit resources in the\ntop-secret project:\n\nService accounts authenticate to the API using tokens signed by a private RSA\nkey. The authentication layer verifies the signature using a matching public RSA\nkey.\n\nTo enable service account token generation, update the serviceAccountConfig\nstanza in the /etc/origin/master/master-config.yml file on the master to\nspecify a privateKeyFile (for signing), and a matching public key file in\nthe publicKeyFiles list:\n\nService accounts are required in each project to run builds, deployments, and\nother pods. The managedNames setting in the\n/etc/origin/master/master-config.yml file on the master controls which\nservice accounts are automatically created in every project:\n\nAll service accounts in a project are given the system:image-puller role,\nwhich allows pulling images from any image stream in the project using the\ninternal container registry.\n\nSeveral infrastructure controllers run using service account credentials. The\nfollowing service accounts are created in the Atomic Registry infrastructure\nproject (openshift-infra) at server start, and given the following roles\ncluster-wide:\n\nTo configure the project where those service accounts are created, set the\nopenshiftInfrastructureNamespace field in in the\n/etc/origin/master/master-config.yml file on the master:\n\nSet the limitSecretReferences field in the\n/etc/origin/master/master-config.yml file on the master to true to require\npod secret references to be whitelisted by their service accounts. Set its value\nto false to allow pods to reference any secret in the project.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/tcp_ingress_external_ports.html", "title": "Atomic Registry Latest | Cluster Administration | Assigning Unique External IPs for Ingress Traffic", "content": "\nOne approach to getting\nexternal\ntraffic into the cluster is by using ExternalIP or IngressIP addresses.\n\nThis feature is only supported in non-cloud deployments. For cloud (GCE, AWS, and OpenStack) deployments,\nload\nBalancer services can be used to automatically deploy a cloud load balancer to target the service\u2019s endpoints.\n\nAtomic Registry supports two pools of IP addresses:\n\nBoth have to be configured to a device on an Atomic Registry host to be used,\nwhether with network interface controller (NIC) or virtual ethernet, as well as\nexternal routing. Ipfailover is recommended for this, because it selects the\nhost and configures the NIC.\n\nIngressIP and ExternalIP both allow external traffic access to the cluster, and,\nif routed correctly, external traffic can reach that service\u2019s endpoints via any\nTCP/UDP port the service exposes. This can be simpler than having to manage the\nport space of a limited number of shared IP addresses when manually assigning\nexternal IPs to services. Also, these addresses can be used as virtual IPs\n(VIPs) when configuring\nhigh\navailability.\n\nAtomic Registry supports both the automatic and manual assignment of IP\naddresses, and each address is guaranteed to be assigned to a maximum of one\nservice. This ensures that each service can expose its chosen ports regardless\nof the ports exposed by other services.\n\nTo use an ExternalIP, you can:\n\nYou must ensure that the IP address pool you assign terminates at one or more nodes in your cluster. You can use the existing\noadm ipfailover to ensure that the external IPs are highly available.\n\nFor manually-configured external IPs, potential port clashes are handled on a first-come, first-served basis. If you request a port, it is only available if it has not yet been assigned for that IP address. For example:\n\nTwo services have been manually configured with the same external\nIP address of 172.7.7.7.\n\nMongoDB service A requests port 27017, and then\nMongoDB service B requests the same port; the first request gets the port.\n\nHowever, port clashes are not an issue for external IPs assigned by the ingress controller, because the controller assigns each service a unique address.\n\nIngress IPs can only be assigned if the cluster is not running in the cloud. In cloud environments, LoadBalancer-type services configure cloud-specific load balancers.\n\nIn non-cloud clusters, IngressIPNetworkCIDR is set by default to\n172.29.0.0/16. If your cluster environment is not already using this private\nrange, you can use the default. However, if you want to use a different range,\nthen you must set\ningressIPNetworkCIDR\nin the /etc/origin/master/master-config.yaml file before you assign an\ningress IP. Then, restart the master service.\n\nExternal IPs assigned to services of type LoadBalancer will always be in the\nrange of IngressIPNetworkCIDR. If IngressIPNetworkCIDR is changed such that\nthe assigned external IPs are no longer in range, the affected services will be\nassigned new external IPs compatible with the new range.\n\nTo assign an ingress IP:\n\nWhen your LoadBalancer-type service has an external IP assigned, the output\ndisplays the IP:\n\nAdd a static route directing traffic for the ingress CIDR to a node in the\ncluster. For example:\n\nIn the example above, 172.29.0.0/16 is the ingressIPNetworkCIDR, and 10.66.140.17 is the node IP.\n\nIn addition to the cluster\u2019s internal IP addresses, the application developer\ncan configure IP addresses that are external to the cluster. As the\nAtomic Registry administrator, you are responsible for ensuring that traffic\narrives at a node with this IP.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/additional_concepts/authentication.html", "title": "Atomic Registry Latest | Architecture | Additional Concepts | Authentication", "content": "\nThe authentication layer identifies the user associated with requests to the\nAtomic Registry API. The authorization layer then uses information about the\nrequesting user to determine if the request should be allowed.\n\nA user in Atomic Registry is an entity that can make requests to the\nAtomic Registry API. Typically, this represents the account of a developer or\nadministrator that is interacting with Atomic Registry.\n\nA user can be assigned to one or more groups, each of which represent a\ncertain set of users. Groups are useful when\nto grant permissions to multiple users at once, for example allowing\naccess to objects within a\nproject, versus granting\nthem to users individually.\n\nIn addition to explicitly defined groups, there are also\nsystem groups, or virtual groups, that are automatically provisioned by\nOpenShift.\n\nIn the default set of virtual groups, note the following in\nparticular:\n\nRequests to the Atomic Registry API are authenticated using the following\nmethods:\n\nAny request with an invalid access token or an invalid certificate is rejected\nby the authentication layer with a 401 error.\n\nIf no access token or certificate is presented, the authentication layer assigns\nthe system:anonymous virtual user and the system:unauthenticated virtual\ngroup to the request. This allows the authorization layer to determine which\nrequests, if any, an anonymous user is allowed to make.\n\nSee the REST API Overview for more information\nand examples.\n\nA request to the Atomic Registry API may include an Impersonate-User header,\nwhich indicates that the requester wants to have the request handled as though\nit came from the specified user. This can be done on the command line by passing\nthe --as=username flag.\n\nBefore User A is allowed to impersonate User B, User A is first authenticated.\nThen, an authorization check occurs to ensure that User A is allowed to\nimpersonate the user named User B. If User A is requesting to impersonate a\nservice account (system:serviceaccount:namespace:name), Atomic Registry checks\nto ensure that User A can impersonate the serviceaccount named name in\nnamespace. If the check fails, the request fails with a 403 (Forbidden) error\ncode.\n\nBy default, project administrators and editors are allowed to impersonate\nservice accounts in their namespace. The sudoers role allows a user to\nimpersonate system:admin, which in turn has cluster administrator permissions.\nThis grants some protection against typos (but not security) for someone\nadministering the cluster. For example, oc delete nodes --all would be\nforbidden, but oc delete nodes --all --as=system:admin would be allowed. You\ncan add a user to that group using oadm policy add-cluster-role-to-user sudoer\n<username>.\n\nThe Atomic Registry master includes a built-in OAuth server. Users obtain OAuth\naccess tokens to authenticate themselves to the API.\n\nWhen a person requests a new OAuth token, the OAuth server uses the configured\nto determine the identity of the person making the request.\n\nIt then determines what user that identity maps to, creates an access token for\nthat user, and returns the token for use.\n\nEvery request for an OAuth token must specify the OAuth client that will\nreceive and use the token. The following OAuth clients are automatically created\nwhen starting the Atomic Registry API:\n\nTo register additional clients:\n\nA\nservice\naccount can be used as a constrained form of OAuth client. Service accounts can\nonly request a subset of\nscopes that\nallow access to some basic user information and role-based power inside of the\nservice account\u2019s own namespace:\n\nWhen using a service account as an OAuth client:\n\nAnnotation keys must have the prefix\nserviceaccounts.openshift.io/oauth-redirecturi. or\nserviceaccounts.openshift.io/oauth-redirectreference. such as:\n\nIn its simplest form, the annotation can be used to directly specify valid\nredirect URIs. For example:\n\nThe first and second postfixes in the above example are used to separate the\ntwo valid redirect URIs.\n\nIn more complex configurations, static redirect URIs may not be enough. For\nexample, perhaps you want all ingresses for a route to be considered valid. This\nis where dynamic redirect URIs via the\nserviceaccounts.openshift.io/oauth-redirectreference. prefix come into play.\n\nFor example:\n\nSince the value for this annotation contains serialized JSON data, it is easier\nto see in an expanded format:\n\nNow you can see that an OAuthRedirectReference allows us to reference the\nroute named jenkins. Thus, all ingresses for that route will now be considered\nvalid.  The full specification for an OAuthRedirectReference is:\n\nBoth annotation prefixes can be combined to override the data provided by the\nreference object. For example:\n\nThe first postfix is used to tie the annotations together. Assuming that the\njenkins route had an ingress of https://example.com, now\nhttps://example.com/custompath is considered valid, but\nhttps://example.com is not.  The format for partially supplying override\ndata is as follows:\n\nScheme\n\n\"https://\"\n\nHostname\n\n\"//website.com\"\n\nPort\n\n\"//:8000\"\n\nPath\n\n\"examplepath\"\n\nSpecifying a host name override will replace the host name data from the\nreferenced object, which is not likely to be desired behavior.\n\nAny combination of the above syntax can be combined using the following format:\n\n<scheme:>//<hostname><:port>/<path>\n\nThe same object can be referenced more than once for more flexibility:\n\nAssuming that the route named jenkins has an ingress of\nhttps://example.com, then both https://example.com:8000 and\nhttps://example.com/custompath are considered valid.\n\nStatic and dynamic annotations can be used at the same time to achieve the\ndesired behavior:\n\nAll requests for OAuth tokens involve a request to <master>/oauth/authorize.\nMost authentication integrations place an authenticating proxy in front of this\nendpoint, or configure Atomic Registry to validate credentials against a backing\nRequests to <master>/oauth/authorize can come from user-agents that cannot\ndisplay interactive login pages, such as the CLI. Therefore, Atomic Registry\nsupports authenticating using a WWW-Authenticate challenge in addition to\ninteractive login flows.\n\nIf an authenticating proxy is placed in front of the\n<master>/oauth/authorize endpoint, it should send unauthenticated,\nnon-browser user-agents WWW-Authenticate challenges, rather than displaying an\ninteractive login page or redirecting to an interactive login flow.\n\nTo prevent cross-site request forgery (CSRF) attacks against browser clients, Basic authentication challenges\nshould only be sent if a X-CSRF-Token header is present on the request. Clients that expect\nto receive Basic WWW-Authenticate challenges should set this header to a non-empty value.\n\nIf the authenticating proxy cannot support WWW-Authenticate challenges, or if\nAtomic Registry is configured to use an identity provider that does not support\nWWW-Authenticate challenges, users can visit <master>/oauth/token/request\nusing a browser to obtain an access token manually.\n\nApplications running in Atomic Registry may need to discover information about\nthe built-in OAuth server. For example, they may need to discover what the\naddress of the <master> server is without manual configuration.  To aid in\nthis, Atomic Registry implements the IETF\nOAuth 2.0\nAuthorization Server Metadata draft specification.\n\nThus, any application running inside the cluster can issue a GET request to\nhttps://openshift.default.svc/.well-known/oauth-authorization-server to fetch\nthe following information:\n\nThe OAuth server supports standard\nauthorization code grant\nand the implicit grant\nOAuth authorization flows.\n\nWhen requesting an OAuth token using the implicit grant flow\n(response_type=token) with a client_id configured to request WWW-Authenticate\nchallenges (like openshift-challenging-client), these are the possible server\nresponses from /oauth/authorize, and how they should be handled:\n\n302\n\nLocation header containing an access_token parameter in the URL fragment (RFC 4.2.2)\n\nUse the access_token value as the OAuth token\n\n302\n\nLocation header containing an error query parameter (RFC 4.1.2.1)\n\nFail, optionally surfacing the error (and optional error_description) query values to the user\n\n302\n\nOther Location header\n\nFollow the redirect, and process the result using these rules\n\n401\n\nWWW-Authenticate header present\n\nRespond to challenge if type is recognized (e.g. Basic, Negotiate, etc), resubmit request, and process the result using these rules\n\n401\n\nWWW-Authenticate header missing\n\nNo challenge authentication is possible. Fail and show response body (which might contain links or details on alternate methods to obtain an OAuth token)\n\nOther\n\nOther\n\nFail, optionally surfacing response body to the user\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/additional_concepts/authorization.html", "title": "Atomic Registry Latest | Architecture | Additional Concepts | Authorization", "content": "\nAuthorization policies determine whether a user is allowed to perform a given\naction within a project. This allows platform administrators to\nuse the cluster policy to control who has\nvarious access levels to the Atomic Registry platform itself and all projects. It also\nallows developers to use local policy to\ncontrol who has access to their\nprojects. Note that\nauthorization is a separate step from authentication,\nwhich is more about determining the identity of who is taking the action.\n\nAuthorization is managed using:\n\nCluster administrators can visualize rules, roles, and bindings\nFor example, consider the following excerpt from viewing a policy, showing rule\nsets for the admin and basic-user default roles:\n\nThe following excerpt from viewing policy bindings shows the above roles bound\nto various users and groups:\n\nThe relationships between the the policy roles, policy bindings, users, and\ndevelopers are illustrated below.\n\nSeveral factors are combined to make the decision when Atomic Registry evaluates\nauthorization:\n\nThe action being performed. In most cases, this consists of:\n\nAtomic Registry evaluates authorizations using the following steps:\n\nThere are two levels of authorization policy:\n\nThis two-level hierarchy allows re-usability over multiple projects through the\ncluster policy while allowing customization inside of individual projects\nthrough local policies.\n\nDuring evaluation, both the cluster bindings and the local bindings are used.\nFor example:\n\nRoles are collections of policy rules, which are sets of\npermitted verbs that can be performed on a set of resources. Atomic Registry\nincludes a set of default roles that can be added to users and groups in the\ncluster policy or in a\nlocal policy.\n\nCluster administrators can visualize these roles, including a matrix of the\nverbs and resources each are associated using the CLI to\nAdditional system: roles are listed as well, which\nare used for various Atomic Registry system and component operations.\n\nBy default in a local policy, only the binding for the admin role is\nimmediately listed when using the CLI to\nHowever, if other default roles are added to users and groups within a local\npolicy, they become listed in the CLI output, as well.\n\nThe cluster- role assigned by the project administrator is limited in a\nproject. It is not the same cluster- role granted by the cluster-admin or\nsystem:admin.\n\nCluster roles are roles defined at the cluster level, but can be bound either at\nthe cluster level or at the project level.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/additional_concepts/other_api_objects.html", "title": "Atomic Registry Latest | Architecture | Additional Concepts | Other API Objects", "content": "\nAn OAuthClient represents an OAuth client, as described in\nRFC 6749, section 2.\n\nThe following OAuthClient objects are automatically created:\n\nAn OAuthClientAuthorization represents an approval by a User for a\nparticular OAuthClient to be given an OAuthAccessToken with particular\nscopes.\n\nCreation of OAuthClientAuthorization objects is done during an\nauthorization request to the OAuth server.\n\nAn OAuthAuthorizeToken represents an OAuth authorization code, as\ndescribed in RFC 6749, section\n1.3.1.\n\nAn OAuthAuthorizeToken is created by a request to the /oauth/authorize endpoint,\nas described in RFC 6749,\nsection 4.1.1.\n\nAn OAuthAuthorizeToken can then be used to obtain an OAuthAccessToken\nwith a request to the /oauth/token endpoint, as described in\nRFC 6749, section 4.1.3.\n\nAn OAuthAccessToken represents an OAuth access token, as described in\nRFC 6749, section 1.4.\n\nAn OAuthAccessToken is created by a request to the /oauth/token endpoint,\nas described in RFC 6749,\nsection 4.1.3.\n\nAccess tokens are used as bearer tokens to authenticate to the API.\n\nWhen a user logs into Atomic Registry, they do so using a configured\nThis determines the user\u2019s identity, and provides that information to\nAtomic Registry.\n\nAtomic Registry then looks for a UserIdentityMapping for that Identity:\n\nA User represents an actor in the system. Users are granted permissions by\n\nUser objects are created automatically on first login, or can be created via the\nAPI.\n\nA UserIdentityMapping maps an Identity to a User.\n\nCreating, updating, or deleting a UserIdentityMapping modifies the\ncorresponding fields in the Identity and  User objects.\n\nAn Identity can only map to a single User, so logging in as a particular\nidentity unambiguously determines the User.\n\nA User can have multiple identities mapped to it. This allows multiple login\nmethods to identify the same User.\n\nA Group represents a list of users in the system. Groups are granted permissions by\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/core_concepts/builds_and_image_streams.html", "title": "Atomic Registry Latest | Architecture | Core Concepts | Image Streams", "content": "\nAn image stream comprises any number of\nDocker-formatted\ncontainer images identified by tags. It presents a single virtual view of\nrelated images, similar to an image repository, and may contain images from any\nof the following:\n\nAn image stream image is a virtual resource that allows you to retrieve an\nimage from a particular image stream where it is tagged. It is often\nabbreviated as isimage. It consists of two parts delimited by an at sign:\n<image stream name>@<image name>. To refer to the image in the\nexample above, the isimage looks like:\n\nUsers, without permission to read or list images on the cluster level, can still\nretrieve the images tagged in a project they have access to using this resource.\n\nAn image stream tag is a named pointer to an image in an image stream. It\nis often abbreviated as istag. It can reference any local or externally\nmanaged image. It contains a history of images represented as a stack of all\nimages the tag ever pointed to. Whenever a new or existing image is tagged\nunder particular istag, it is just placed at the first position in the\nhistory stack. Image previously occupying the top position will be available at\nthe second position etc. This allows for easy rollbacks to make tags point to\nhistorical images again.\n\nThe istag is composed of two parts separated by a colon: <image stream\nname>:<tag>. Istag referring to the image\nsha256:47463d94eb5c049b2d23b03a9530bf944f8f967a0fe79147dd6b9135bf7dd13d in\nthe example above would be\norigin-ruby-sample:latest.\n\nWhen the\nintegrated\nregistry receives a new image, it creates and sends an ImageStreamMapping\nto Atomic Registry, providing the image\u2019s namespace (i.e., its project), name,\ntag, and image metadata.\n\nThis information is used to create a new image (if it does not already exist)\nand to tag the image into the image stream. Atomic Registry stores complete\nmetadata about each image, such as commands, entrypoint, and environment\nvariables. Images in Atomic Registry are immutable and the maximum name length\nis 63 characters.\n\nThe following ImageStreamMapping example results in an image being tagged as\ntest/origin-ruby-sample:latest:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/core_concepts/containers_and_images.html", "title": "Atomic Registry Latest | Architecture | Core Concepts | Images", "content": "\nA pod can have init containers in addition to application containers. Init\ncontainers allow you to reorganize setup scripts and binding code. An init\ncontainer differs from a regular container in that it always runs to completion.\nEach init container must complete successfully before the next one is started.\n\nFor more information, see Pods and Services.\n\nContainers in Atomic Registry are based on Docker-formatted container images. An\nimage is a binary that includes all of the requirements for running a single\ncontainer, as well as metadata describing its needs and capabilities.\n\nYou can think of it as a packaging technology. Containers only have access to\nresources defined in the image unless you give the container additional access\nwhen creating it. By deploying the same image in multiple containers across\nmultiple hosts and load balancing between them, Atomic Registry can provide\nredundancy and horizontal scaling for a service packaged into an image.\n\nYou can use the Docker CLI directly to build images, but Atomic Registry also\nsupplies builder images that assist with creating new images by adding your code\nor configuration to existing images.\n\nBecause applications develop over time, a single image name can actually\nrefer to many different versions of the \"same\" image. Each different\nimage is referred to uniquely by its hash (a long hexadecimal number\ne.g. fd44297e2ddb050ec4f\u2026\u200b) which is usually shortened to 12\ncharacters (e.g. fd44297e2ddb).\n\nRather than version numbers, the Docker service allows applying tags (such as\nv1, v2.1, GA, or the default latest) in addition to the image name to\nfurther specify the image desired, so you may see the same image referred to as\ncentos (implying the latest tag), centos:centos7, or fd44297e2ddb.\n\nDo not use the latest tag for any official Atomic Registry images. These are\nimages that start with openshift3/. latest can refer to a number of\nversions, such as 3.4, or 3.5.\n\nHow you tag the images dictates the updating policy. The more specific you are, the less frequently the image will be updated. Use the following to determine your chosen Atomic Registry images policy:\n\nA container registry is a service for storing and retrieving Docker-formatted\ncontainer images. A registry contains a collection of one or more image\nrepositories. Each image repository contains one or more tagged images. Docker\nprovides its own registry, the Docker Hub, and you can also use private or third-party registries. Red Hat provides a\nregistry at registry.access.redhat.com for subscribers. Atomic Registry can\nalso supply its own internal registry for managing custom container images.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/core_concepts/projects_and_users.html", "title": "Atomic Registry Latest | Architecture | Core Concepts | Projects and Users", "content": "\nInteraction with Atomic Registry is associated with a user. An Atomic Registry\nuser object represents an actor which may be granted permissions in the system\nby\n\nSeveral types of users can exist:\n\nEvery user must authenticate in\nsome way in order to access Atomic Registry. API requests with no authentication\nor invalid authentication are authenticated as requests by the anonymous\nsystem user. Once authenticated, policy determines what the user is\nauthorized to do.\n\nA Kubernetes namespace provides a mechanism to scope resources in a cluster.\nIn Atomic Registry, a project is a Kubernetes namespace with\nadditional annotations.\n\nNamespaces provide a unique scope for:\n\nMost objects in the system are scoped by namespace, but some are\nexcepted and have no namespace, including nodes and users.\n\nThe\nKubernetes\ndocumentation has more information on namespaces.\n\nA project is a Kubernetes namespace with additional annotations, and is the central vehicle\nby which access to resources for regular users is managed.\nA project allows a community of users to organize and manage their content in\nisolation from other communities. Users must be given access to projects by administrators,\nor if allowed to create projects, automatically have access to their own projects.\n\nProjects can have a separate name, displayName, and description.\n\nEach project scopes its own set of:\n\nCluster administrators can create projects\nand\nfor the project to any member of the user community.\nCluster administrators can also allow developers to create\n\nDevelopers and administrators can interact\nwith projects using the CLI or the\nweb console.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/core_concepts/routes.html", "title": "Atomic Registry Latest | Architecture | Core Concepts | Routes", "content": "\nAn Atomic Registry route exposes a\nservice at a\nhost name, like www.example.com, so that external clients can reach it by\nname.\n\nDNS resolution for a host name is handled separately from routing.\nYour administrator may have configured a\nDNS wildcard entry\nthat will resolve to the Atomic Registry node that is running the\nAtomic Registry router. If you are using a different host name you may\nneed to modify its DNS records independently to resolve to the node that\nis running the router.\n\nEach route consists of a name (limited to 63 characters), a service selector,\nand an optional security configuration.\n\nAn Atomic Registry administrator can deploy routers to nodes in an\nAtomic Registry cluster, which enable routes\ncreated by developers to be\nused by external clients. The routing layer in Atomic Registry is pluggable, and\ntwo available router plug-ins are provided and\nsupported by default.\n\nA router uses the service selector to find the\nservice and the endpoints backing\nthe service.\nWhen both router and service provide load balancing,\nAtomic Registry uses the router load balancing.\nA router detects relevant changes in the IP addresses of its services\nand adapts its configuration accordingly.\nThis is useful for custom routers to communicate modifications\nof API objects to an external routing solution.\n\nThe path of a request starts with the DNS resolution of a host name\nto one or more routers.\nThe suggested method is to define a cloud domain with\na wildcard DNS entry pointing to one or more virtual IP (VIP)\naddresses backed by multiple router instances.\nRoutes using names and addresses outside the cloud domain require\nconfiguration of individual DNS entries.\n\nWhen there are fewer VIP addresses than routers, the routers corresponding\nto the number of addresses are active and the rest are passive.\nA passive router is also known as a hot-standby router.\nFor example, with two VIP addresses and three routers,\nyou have an \"active-active-passive\" configuration.\nSee\nfor more information on router VIP configuration.\n\nRoutes can be\nsharded\namong the set of routers.\nAdministrators can set up sharding on a cluster-wide basis\nand users can set up sharding for the namespace in their project.\nSharding allows the operator to define multiple router groups.\nEach router in the group serves only a subset of traffic.\n\nAtomic Registry routers provide external host name mapping and load balancing\nof service end points over protocols that\npass distinguishing information directly to the router; the host name\nmust be present in the protocol in order for the router to determine\nwhere to send it.\n\nRouter plug-ins assume they can bind to host ports 80 (HTTP)\nand 443 (HTTPS), by default.\nThis means that routers must be placed on nodes\nwhere those ports are not otherwise in use.\nAlternatively, a router can be configured to listen\non other ports by setting the ROUTER_SERVICE_HTTP_PORT\nand ROUTER_SERVICE_HTTPS_PORT environment variables.\n\nBecause a router binds to ports on the host node,\nonly one router listening on those ports can be on each node\nif the router uses host networking (the default).\nCluster networking is configured such that all routers\ncan access all pods in the cluster.\n\nRouters support the following protocols:\n\nWebSocket traffic uses the same route conventions and supports the same TLS\ntermination types as other traffic.\n\nFor a secure connection to be established, a cipher common to the\nclient and server must be negotiated. As time goes on, new, more secure ciphers\nbecome available and are integrated into client software. As older clients\nbecome obsolete, the older, less secure ciphers can be dropped. By default, the\nrouter supports a broad range of commonly available clients. The router can be\nconfigured to use a selected set of ciphers that support desired clients and\ndo not include the less secure ciphers.\n\nA template router is a type of router that provides certain infrastructure\ninformation to the underlying router implementation, such as:\n\nThe following router plug-ins are provided and supported in Atomic Registry.\n\nThe HAProxy template router implementation is the reference implementation for a\ntemplate router plug-in. It uses the\nrepository to run an HAProxy instance alongside the template router plug-in.\n\nThe following diagram illustrates how data flows from the master through the\nplug-in and finally into an HAProxy configuration:\n\nSticky sessions attempts to ensure that user traffic is directed to the same pod\nin each session, and improves application user experience by caching data. For\nexample, for a cluster with five back-end pods, and two load-balanced routers,\nyou can ensure that the same pod receives the web traffic from the same web\nbrowser regardless of the router that handles it.\n\nWhile placing returning traffic onto the same pod cannot be guaranteed, you can\nuse HTTP headers to set a cookie determining the pod used by the last\nconnection, in an attempt to steer traffic to the same pod. When the application\nuser hits the router the browser re-sends the cookie and knows where to send the\ntraffic.\n\nAs a cluster administrator, you can turn off the stickiness for passthrough\nroutes separately from other connections, or turn off stickiness entirely.\n\nBy default, if the default HAProxy router is using passthrough routes, sticky\nsessions are implemented using the balance source directive in the underlying\nrouter configuration, which balances based on the source IP. Other types of\nroutes use balance leastconn by default. If a cookie is present, it is read\nand sent to the same pod it went to last time if the pod still exists. If no\ncookie is present, because this is a new session, stickiness is disabled, or\nit\u2019s using a passthrough route, the router picks a pod.\n\nCookies cannot be set on passthrough routes, because the HTTP traffic cannot be\nseen. Instead, a number is calculated based on the source IP address, which\ndetermines the back-end.\n\nIf back-ends change, the traffic could head to the wrong server, making it less\nsticky, and if you are using a load-balancer (which hides the source IP) the\nsame number is set for all connections and traffic is sent to the same pod.\n\nIn addition, the template\nrouter plug-in provides the service name and namespace to the underlying\nimplementation. This can be used for more advanced configuration such as\nimplementing stick-tables that synchronize between a set of peers.\n\nSpecific configuration for this router implementation is stored in the\nhaproxy-config.template file located in the /var/lib/haproxy/conf\ndirectory of the router container.\n\nThe balance source directive does not distinguish between external client IP\naddresses; because of the NAT configuration, the originating IP address\n(HAProxy remote) is the same. Unless the HAProxy router is running with\nhostNetwork: true, all external clients will be routed to a single pod.\n\nFor all the items outlined in this section, you can set environment\nvariables on the deployment config for the router to alter its configuration, or use the oc set env command:\n\nFor example:\n\nIf you want to run multiple routers on the same machine, you must change the\nports that the router is listening on, ROUTER_SERVICE_SNI_PORT and\nROUTER_SERVICE_NO_SNI_PORT. These ports can be anything you want as long as\nthey are unique on the machine. These ports will not be exposed externally.\n\nTimeUnits are represented by a number followed by the unit: us\n*(microseconds), ms (milliseconds, default), s (seconds), m (minutes), h\n*(hours), d (days).\n\nThe regular expression is: [1-9][0-9]*(us\\|ms\\|s\\|m\\|h\\|d)\n\nBy default, when a host does not resolve to a route in a HTTPS or TLS SNI\nrequest, the default certificate is returned to the caller as part of the 503\nresponse. This exposes the default certificate and can pose security concerns\nbecause the wrong certificate is served for a site. The HAProxy strict-sni\noption to bind suppresses use of the default certificate.\n\nThe ROUTER_STRICT_SNI environment variable controls bind processing. When set\nto true or TRUE, strict-sni is added to the HAProxy bind. The default\nsetting is false.\n\nThe option can be set when the router is created or added later.\n\nThis sets ROUTER_STRICT_SNI=true.\n\nEach client (for example, Chrome 30, or Java8) includes a suite of ciphers used\nto securely connect with the router. The router must have at least one of the\nciphers for the connection to be complete:\n\nSee the Security/Server\nSide TLS reference guide for more information.\n\nThe router defaults to the intermediate profile. You can select a different\nprofile using the --ciphers option when creating a route, or by changing\nthe ROUTER_CIPHERS environment variable with the values modern,\nintermediate, or old for an existing router. Alternatively, a set of \":\"\nseparated ciphers can be provided. The ciphers must be from the set displayed\nby:\n\nIn order for services to be exposed externally, an Atomic Registry route allows\nyou to associate a service with an externally-reachable host name. This edge\nhost name is then used to route traffic to the service.\n\nWhen multiple routes from different namespaces claim the same host,\nthe oldest route wins and claims it for the namespace. If additional\nroutes with different path fields are defined in the same namespace,\nthose paths are added. If multiple routes with the same path are\nused, the oldest takes priority.\n\nA consequence of this behavior is that if you have two routes for a host name: an\nolder one and a newer one. If someone else has a route for the same host name\nthat they created between when you created the other two routes, then if you\ndelete your older route, your claim to the host name will no longer be in effect.\nThe other namespace now claims the host name and your claim is lost.\n\nIf a host name is not provided as part of the route definition, then\nAtomic Registry automatically generates one for you. The generated host name\nis of the form:\n\nThe following example shows the Atomic Registry-generated host name for the\nabove configuration of a route without a host added to a namespace\nmynamespace:\n\nA cluster administrator can also\nfor their environment.\n\nRoutes can be either secured or unsecured. Secure routes provide the ability to\nuse several types of TLS termination to serve certificates to the client.\nRouters support edge,\npassthrough, and\nre-encryption termination.\n\nUnsecured routes are simplest to configure, as they require no key\nor certificates, but secured routes offer security for connections to\nremain private.\n\nA secured route is one that specifies the TLS termination of the route.\nThe available types of termination are described\nbelow.\n\nPath based routes specify a path component that can be compared against\na URL (which requires that the traffic for the route be HTTP based) such\nthat multiple routes can be served using the same host name, each with a\ndifferent path. Routers should match routes based on the most specific\npath to the least; however, this depends on the router implementation. The\nfollowing table shows example routes and their accessibility:\n\nPath-based routing is not available when using passthrough TLS, as\nthe router does not terminate TLS in that case and cannot read the contents\nof the request.\n\nSecured routes specify the TLS termination of the route and, optionally,\nprovide a key and certificate(s).\n\nTLS termination in Atomic Registry relies on\nSNI for serving\ncustom certificates. Any non-SNI traffic received on port 443 is handled with\nTLS termination and a default certificate (which may not match the requested\nhost name, resulting in validation errors).\n\nSecured routes can use any of the following three types of secure TLS\ntermination.\n\nEdge Termination\n\nWith edge termination, TLS termination occurs at the router, prior to proxying\ntraffic to its destination. TLS certificates are served by the front end of the\nrouter, so they must be configured into the route, otherwise the\nwill be used for TLS termination.\n\nBecause TLS is terminated at the router, connections from the router to\nthe endpoints over the internal network are not encrypted.\n\nEdge-terminated routes can specify an insecureEdgeTerminationPolicy that\nenables traffic on insecure schemes (HTTP) to be disabled, allowed or\nredirected.\nThe allowed values for insecureEdgeTerminationPolicy are:\n  None or empty (for disabled), Allow or Redirect.\nThe default insecureEdgeTerminationPolicy is to disable traffic on the\ninsecure scheme. A common use case is to allow content to be served via a\nsecure scheme but serve the assets (example images, stylesheets and\njavascript) via the insecure scheme.\n\nPassthrough Termination\n\nWith passthrough termination, encrypted traffic is sent straight to the\ndestination without the router providing TLS termination. Therefore no\nkey or certificate is required.\n\nThe destination pod is responsible for serving certificates for the\ntraffic at the endpoint. This is currently the only method that can support\nrequiring client certificates (also known as two-way authentication).\n\nPassthrough routes can also have an insecureEdgeTerminationPolicy. The only\nvalid values are None (or empty, for disabled) or Redirect.\n\nRe-encryption Termination\n\nRe-encryption is a variation on edge termination where the router terminates\nTLS with a certificate, then re-encrypts its connection to the endpoint which\nmay have a different certificate. Therefore the full path of the connection\nis encrypted, even over the internal network. The router uses health\nchecks to determine the authenticity of the host.\n\nRe-encrypt routes can have an insecureEdgeTerminationPolicy with all of the\nsame values as edge-terminated routes.\n\nIn Atomic Registry, each route can have any number of\nlabels\nin its metadata field.\nA router uses selectors (also known as a selection expression)\nto select a subset of routes from the entire pool of routes to serve.\nA selection expression can also involve\nlabels on the route\u2019s namespace.\nThe selected routes form a router shard.\n\nThis design supports traditional sharding as well as overlapped sharding.\nIn traditional sharding, the selection results in no overlapping sets\nand a route belongs to exactly one shard.\nIn overlapped sharding, the selection results in overlapping sets\nand a route can belong to many different shards.\nFor example, a single route may belong to a SLA=high shard\n(but not SLA=medium or SLA=low shards),\nas well as a geo=west shard\n(but not a geo=east shard).\n\nAnother example of overlapped sharding is a\nset of routers that select based on namespace of the route:\n\nBoth router-2 and router-3 serve routes that are in the\nnamespaces Q*, R*, S*, T*.\nTo change this example from overlapped to traditional sharding,\nwe could change the selection of router-2 to K*\u2009\u2014\u2009P*,\nwhich would eliminate the overlap.\n\nWhen routers are sharded,\na given route is bound to zero or more routers in the group.\nThe route binding ensures uniqueness of the route across the shard.\nUniqueness allows secure and non-secure versions of the same route to exist\nwithin a single shard.\nThis implies that routes now have a visible life cycle\nthat moves from created to bound to active.\n\nIn the sharded environment the first route to hit the shard\nreserves the right to exist there indefinitely, even across restarts.\n\nDuring a green/blue deployment a route may be be selected in multiple routers.\nAn Atomic Registry application administrator may wish to bleed traffic from one\nversion of the application to another and then turn off the old version.\n\nSharding can be done by the administrator at a cluster level and by the user\nat a project/namespace level.\nWhen namespace labels are used, the service account for the router\nmust have cluster-reader permission to permit the\nrouter to access the labels in the namespace.\n\nFor two or more routes that claim the same host name, the resolution order\nis based on the age of the route and the oldest route would win the claim to\nthat host.\nIn the case of sharded routers, routes are selected based on their labels\nmatching the router\u2019s selection criteria. There is no consistent way to\ndetermine when labels are added to a route. So if an older route claiming\nan existing host name is \"re-labelled\" to match the router\u2019s selection\ncriteria, it will replace the existing route based on the above mentioned\nresolution order (oldest route wins).\n\nUsing environment variables as defined in Configuration\nParameters, a router can set the default options for all the routes it exposes.\nAn individual route can override some of these defaults by providing specific\nconfigurations in its annotations.\n\nRoute Annotations\n\nFor all the items outlined in this section, you can set annotations on the\nroute definition for the route to alter its configuration\n\nSetting a server-side timeout value for passthrough routes too low can cause\nWebSocket connections to timeout frequently on that route.\n\nYou can restrict access to a route to a select set of IP addresses by adding the\nhaproxy.router.openshift.io/ip_whitelist annotation on the route. The\nwhitelist is a space-separated list of IP addresses and/or CIDRs for the\napproved source addresses. Requests from IP addresses that are not in the\nwhitelist are dropped.\n\nSome examples:\n\nWhen editing a route, add the following annotation to define the desired\nsource IP\u2019s. Alternatively, use oc annotate route <name>.\n\nAllow only one specific IP address:\n\nAllow several IP addresses:\n\nAllow an IP CIDR network:\n\nAllow mixed IP addresses and IP CIDR networks:\n\nA wildcard policy allows a user to define a route that covers all hosts within a\ndomain (when the router is configured to allow it). A route can specify a\nwildcard policy as part of its configuration using the wildcardPolicy field.\nAny routers run with a policy allowing wildcard routes will expose the route\nappropriately based on the wildcard policy.\n\nThe route status field is only set by routers. If changes are made to a route\nso that a router no longer serves a specific route, the status becomes stale.\nThe routers do not clear the route status field. To remove the stale entries\nin the route status, use the\nclear-route-status\nscript.\n\nA router can be configured to deny or allow a specific subset of domains from\nthe host names in a route using the ROUTER_DENIED_DOMAINS and\nROUTER_ALLOWED_DOMAINS environment variables.\n\nThe domains in the list of denied domains take precedence over the list of\nallowed domains. Meaning Atomic Registry first checks the deny list (if\napplicable), and if the host name is not in the list of denied domains, it then\nchecks the list of allowed domains. However, the list of allowed domains is more\nrestrictive, and ensures that the router only admits routes with hosts that\nbelong to that list.\n\nFor example, to deny the [*.]open.header.test, [*.]openshift.org and\n[*.]block.it routes for the myrouter route:\n\nThis means that myrouter will admit the following based on the route\u2019s name:\n\nHowever, myrouter will deny the following:\n\nAlternatively, to block any routes where the host name is not set to [*.]stickshift.org or [*.]kates.net:\n\nThis means that the myrouter router will admit:\n\nHowever, myrouter will deny the following:\n\nTo implement both scenarios, run:\n\nThis will allow any routes where the host name is set to [*.]openshift.org or\n[*.]kates.net, and not allow any routes where the host name is set to\n[*.]ops.openshift.org or [*.]metrics.kates.net.\n\nTherefore, the following will be denied:\n\nHowever, the following will be allowed:\n\nHosts and subdomains are owned by the namespace of the route that first\nmakes the claim. Other routes created in the namespace can make claims on\nthe subdomain. All other namespaces are prevented from making claims on\nthe claimed hosts and subdomains. The namespace that owns the host also\nowns all paths associated with the host, for example www.abc.xyz/path1.\n\nFor example, if the host www.abc.xyz is not claimed by any route.\nCreating route r1 with host www.abc.xyz in namespace ns1 makes\nnamespace ns1 the owner of host www.abc.xyz and subdomain abc.xyz\nfor wildcard routes. If another namespace, ns2, tries to create a route\nwith say a different path www.abc.xyz/path1/path2, it would fail\nbecause a route in another namespace (ns1 in this case) owns that host.\n\nBy disabling the namespace ownership rules, you can disable these restrictions\nand allow hosts (and subdomains) to be claimed across namespaces.\n\nIf you decide to disable the namespace ownership checks in your router,\nbe aware that this allows end users to claim ownership of hosts\nacross namespaces. While this change can be desirable in certain\ndevelopment environments, use this feature with caution in production\nenvironments, and ensure that your cluster policy has locked down untrusted end\nusers from creating routes.\n\nFor example, with ROUTER_DISABLE_NAMESPACE_OWNERSHIP_CHECK=true, if\nnamespace ns1 creates the oldest route r1 www.abc.xyz,  it owns only\nthe hostname (+ path).  Another namespace can create a wildcard route\neven though it does not have the oldest route in that subdomain (abc.xyz)\nand we could potentially have other namespaces claiming other\nnon-wildcard overlapping hosts (for example, foo.abc.xyz, bar.abc.xyz,\nbaz.abc.xyz) and their claims would be granted.\n\nAny other namespace (for example, ns2) can now create\na route r2 www.abc.xyz/p1/p2,  and it would be admitted.  Similarly\nanother namespace (ns3) can also create a route  wildthing.abc.xyz\nwith a subdomain wildcard policy and it can own the wildcard.\n\nAs this example demonstrates, the policy ROUTER_DISABLE_NAMESPACE_OWNERSHIP_CHECK=true is more\nlax and allows claims across namespaces.  The only time the router would\nreject a route with the namespace ownership disabled is if the host+path\nis already claimed.\n\nFor example, if a new route rx tries to claim www.abc.xyz/p1/p2, it\nwould be rejected as route r2 owns that host+path combination.  This is true whether route rx\nis in the same namespace or other namespace since the exact host+path is already claimed.\n\nThis feature can be set during router creation or by setting an environment\nvariable in the router\u2019s deployment configuration.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/index.html", "title": "Atomic Registry Latest | Architecture | Overview", "content": "\nAtomic Registry is based on OpenShift technology which features an\nembedded registry based on the upstream\nDocker Distribution library. Atomic Registry provides the\nfollowing capabilities:\n\nAtomic Registry has a microservices-based architecture of smaller, decoupled units\nthat work together. It runs on top of a\nKubernetes\ncluster, with data about the objects stored in\netcd, a\nreliable clustered key-value store. Those services are broken down by function:\n\nUsers make calls to the REST API to change the state of the system. Controllers\nuse the REST API to read the user\u2019s desired state, and then try to bring the\nother parts of the system into sync. For example, when a user requests a\nbuild they create a\n\"build\" object. The build controller sees that a new build has been created, and\nruns a process on the cluster to perform that build. When the build completes,\nthe controller updates the build object via the REST API and the user sees that\ntheir build is complete.\n\nTo make this possible, controllers leverage a reliable stream of changes to the\nsystem to sync their view of the system with what users are doing. This event\nstream pushes changes from etcd to the REST API and then to the controllers as\nsoon as changes occur, so changes can ripple out through the system very quickly\nand efficiently. However, since failures can occur at any time, the controllers\nmust also be able to get the latest state of the system at startup, and confirm\nthat everything is in the right state. This resynchronization is important,\nbecause it means that even if something goes wrong, then the operator can\nrestart the affected components, and the system double checks everything before\ncontinuing. The system should eventually converge to the user\u2019s intent, since\nthe controllers can always bring the system into sync.\n\nThe Atomic Registry and Kubernetes APIs\nauthenticate users who present\ncredentials, and then authorize\nthem based on their role. Both developers and administrators can be\nauthenticated via a number of means, primarily\nOAuth tokens and SSL\ncertificate authorization.\n\nDevelopers (clients of the system) typically make REST API calls from a\nclient program like oc or to the\nweb console via their browser,\nand use OAuth bearer tokens for most communications. Infrastructure components\n(like nodes) use client certificates generated by the system that contain their\nidentities. Infrastructure components that run in containers use a token\nassociated with their service account\nto connect to the API.\n\nAuthorization is handled in the Atomic Registry policy engine, which defines\nactions like \"create pod\" or \"list services\" and groups them into roles in a\npolicy document. Roles are bound to users or groups by the user or group\nidentifier. When a user or service account attempts an action, the policy engine\nchecks for one or more of the roles assigned to the user (e.g., cluster\nadministrator or administrator of the current project) before allowing it to\ncontinue.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/infrastructure_components/image_registry.html", "title": "Atomic Registry Latest | Architecture | Infrastructure Components | Container Registry", "content": "\nAtomic Registry embeds the upstream Docker Distribution\nlibrary to maintain image format compatibility with the Docker service. New image\nrepositories may be created on the fly. Whenever a new image is pushed to the\nintegrated registry, the registry notifies Atomic Registry API about the new\nimage, passing along all the information about it, such as the namespace, name,\nand image metadata.\n\nAtomic Registry can reference images from external, third-party registries. During\nimport Atomic Registry will fetch tags from the remote registry and watch the\nremote image tag for changes.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/infrastructure_components/kubernetes_infrastructure.html", "title": "Atomic Registry Latest | Architecture | Infrastructure Components | Kubernetes Infrastructure", "content": "\nAtomic Registry is based on OpenShift and Kubernetes. The typical Atomic Registry\ndeployment is much simpler than a deployment of OpenShift. The following\nis provided as a reference, particularly for understanding requirements for a\nhighly available deployment.\n\nThe master is the host or hosts that contain the master components, including\nthe API server, controller manager server, and etcd. The master manages\nnodes in its Kubernetes cluster and schedules\npods to run on nodes.\n\nOptional, used when configuring\nhighly-available masters with the native\nmethod to balance load between API master endpoints.\n\nWhile in a single master configuration, the availability of running applications\nremains if the master or any of its services fail. However, failure of master\nservices reduces the ability of the system to respond to application failures or\ncreation of new applications. You can optionally configure your masters for high\navailability (HA) to ensure that the cluster has no single point of failure.\n\nTo mitigate concerns about availability of the master, two activities are\nrecommended:\n\nWhen using the native HA method with HAProxy, master components have the\nfollowing availability:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/infrastructure_components/web_console.html", "title": "Atomic Registry Latest | Architecture | Infrastructure Components | Web Console", "content": "\nThe Atomic Registry web console is a user interface accessible from a web browser.\nDevelopers can use the web console to visualize, browse, and manage the contents\nof projects.\n\nJavaScript must be enabled to use the web console. For the best experience, use\na web browser that supports\nWebSockets.\n\nThe web console is based on the Cockpit\nProject. It is deployed as a service using an Atomic Registry template. The web\nconsole is an optional component.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/cli_reference/get_started_cli.html", "title": "Atomic Registry Latest | CLI Reference | Get Started with the CLI", "content": "\nThe Atomic Registry CLI exposes commands for managing your applications, as well as\nlower level tools to interact with each component of your system. This topic\nguides you through getting started with the CLI, including installation and\nlogging in to create your first project.\n\nInstallation options for the CLI vary depending on your operating system.\n\nThe CLI for Windows is provided as a zip archive; you can download it from\nthe Releases page of the OpenShift\nOrigin source repository on GitHub:\n\nDownload the CLI from GitHub\n\nThen, unzip the archive with a ZIP program and move the oc binary to a\ndirectory on your PATH. To check your PATH, open the Command Prompt and run:\n\nThe CLI for Mac OS X is provided as a zip archive; you can download it from\nthe Releases page of the OpenShift\nOrigin source repository on GitHub:\n\nDownload the CLI from GitHub\n\nThen,\nunzip the archive with a ZIP program\nand move the oc binary to a directory on your PATH.\nTo check your PATH, open a Terminal window and run:\n\nAlternatively, Mac OS X users can install the CLI using\nHomebrew:\n\nThe CLI for Linux is provided as a tar.gz archive; you can download it from\nthe Releases page of the OpenShift\nOrigin source repository on GitHub:\n\nDownload the CLI from GitHub\n\nThen, unpack the archive and move the oc binary to a directory on your PATH.\nTo check your path, run:\n\nTo unpack the archive:\n\nThe oc login command is the best way to initially set up the CLI,\nand it serves as the entry point for most users. The interactive flow helps you\nestablish a session to an Atomic Registry server with the provided credentials. The\ninformation is automatically saved in a CLI\nconfiguration file that is then used for subsequent commands.\n\nThe following example shows the interactive setup and login using the oc\nlogin command:\n\nWhen you have completed the CLI configuration, subsequent commands use the\nconfiguration file for the server, session token, and project information.\n\nYou can log out of CLI using the oc logout command:\n\nIf you log in after creating or being granted access to a project, a project you\nhave access to is automatically set as the current default, until\nswitching to another one:\n\nAdditional options are also available for\nthe oc login command.\n\nIf you have access to administrator credentials but are no longer logged in as\nthe default\nsystem user system:admin, you can log back in as this user at any time as\nlong as the credentials are still present in your\nCLI\nconfiguration file. The following command logs in and switches to the default\nproject:\n\nA CLI configuration file permanently stores oc options and contains a series\nof authentication\nmechanisms and Atomic Registry server connection information associated with\nnicknames.\n\nAs described in the previous section, the oc login command automatically\ncreates and manages CLI configuration files. All information gathered by the\ncommand is stored in a configuration file located in\n~/.kube/config. The current CLI configuration can be viewed using the following command:\n\nCLI configuration files can be used to setup\nmultiple CLI profiles using various Atomic Registry servers, namespaces, and users so\nthat you can switch easily between them. The CLI can support multiple\nconfiguration files; they are loaded at runtime and merged together along with\nany override options specified from the command line.\n\nMost oc commands run in the context of a\nproject. The oc login\nselects a default project during initial setup to\nbe used with subsequent commands. Use the following command to display the\nproject currently in use:\n\nIf you have access to multiple projects, use the following syntax to switch to a\nparticular project by specifying the project name:\n\nFor example:\n\nAfter you have logged in,\nexplore administrator CLI operations.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/cli_reference/index.html", "title": "Atomic Registry Latest | CLI Reference | Overview", "content": "\nWith the Atomic Registry command line interface (CLI), you can manage\nAtomic Registry projects\nfrom a terminal. The CLI is ideal in situations where you are:\n\nThe CLI is available using the oc command:\n\nSee Get Started with the CLI for\ninstallation and setup instructions.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/cli_reference/manage_cli_profiles.html", "title": "Atomic Registry Latest | CLI Reference | Managing CLI Profiles", "content": "\nA CLI configuration file allows you to configure different profiles, or\ncontexts, for use with the Atomic Registry CLI. A context\nconsists of user\nauthentication andAtomic Registry server information associated with a\nnickname.\n\nContexts allow you to easily switch between multiple users across multiple\nAtomic Registry servers, or clusters, when using issuing CLI operations. Nicknames\nmake managing CLI configuration easier by providing short-hand references to\ncontexts, user credentials, and cluster details.\n\nAfter logging in with the CLI for the first time,\nAtomic Registry creates a ~/.kube/config file if one does not\nalready exist. As more authentication and connection details are provided to the\nCLI, either automatically during an oc login operation or by\nsetting them explicitly, the updated\ninformation is stored in the configuration file:\n\nThe CLI can support multiple configuration files; they are\nloaded at runtime and merged together along\nwith any override options specified from the command line.\n\nAfter you are logged in, you can use the oc status command or the oc\nproject command to verify your current working environment:\n\nTo log in using any other combination of user credentials and cluster details,\nrun the oc login command again and supply the relevant information during the\ninteractive process. A context is constructed based on the supplied information\nif one does not already exist.\n\nIf you are already logged in and want to switch to another project the current\nuser already has access to, use the oc project command and supply the name of\nthe project:\n\nAt any time, you can use the oc config view command to view your current,\nfull CLI configuration, as seen in the above output.\n\nAdditional CLI configuration commands are also available for more\nadvanced usage.\n\nIf you have access to administrator credentials but are no longer logged in as\nthe default\nsystem user system:admin, you can log back in as this user at any time as\nlong as the credentials are still present in your\nCLI\nconfiguration file. The following command logs in and switches to the default\nproject:\n\nThis section covers more advanced usage of CLI configurations. In most\nsituations, you can simply use the oc login and oc project commands to log\nin and switch between contexts and projects.\n\nIf you want to manually configure your CLI configuration files, you can use the\noc config command instead of modifying the files themselves. The oc config\ncommand includes a number of helpful subcommands for this purpose:\n\nset-credentials\n\nSets a user entry in the CLI configuration file. If the referenced user\nnickname already exists, the specified information is merged in.\n\nset-cluster\n\nSets a cluster entry in the CLI configuration file. If the referenced cluster\nnickname already exists, the specified information is merged in.\n\nset-context\n\nSets a context entry in the CLI configuration file. If the referenced context\nnickname already exists, the specified information is merged in.\n\nuse-context\n\nSets the current context using the specified context nickname.\n\nset\n\nSets an individual value in the the CLI configuration file.\n\nThe <property_name> is a dot-delimited name where each token represents either\nan attribute name or a map key. The <property_value> is the new value being\nset.\n\nunset\n\nUnsets individual values in the CLI configuration file.\n\nThe <property_name> is a dot-delimited name where each token represents either\nan attribute name or a map key.\n\nview\n\nDisplays the merged CLI configuration currently in use.\n\nDisplays the result of the specified CLI configuration file.\n\nExample Usage \n\nConsider the following configuration workflow. First, set credentials for a user\nnickname alice that uses an\naccess\ntoken:\n\nSet a cluster entry named openshift1:\n\nSet a context named alice that uses the alice user and the\nopenshift1 cluster:\n\nNow that the alice context has been created, switch to that context:\n\nSet the aliceproject namespace for the alice context:\n\nYou can now view the configuration that has been created:\n\nAll subsequent CLI operations will use the alice context, unless otherwise\nspecified by overriding CLI options or until the context is switched.\n\nWhen issuing CLI operations, the loading and merging order for the CLI\nconfiguration follows these rules:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/dev_guide/authentication.html", "title": "Atomic Registry Latest | User Guide | Authentication", "content": "\nWhen accessing the\nweb console\nfrom a browser, you are automatically redirected\nto a login page.\n\nReview the\nbrowser versions\nand operating systems that can be used to access the web console.\n\nYou can provide your login credentials on this page to obtain a token to make\nAPI calls. After logging in, you can navigate your projects using the\nweb console.\n\nYou can authenticate from the command line using the CLI command oc login.\nYou can get started with the CLI by\nrunning this command\nwithout any options:\n\nThe command\u2019s interactive flow helps you establish a session to an Atomic Registry\nserver with the provided credentials. If any information required to successfully\nlog in to an Atomic Registry server is not provided, the command prompts for user\ninput as required. The\nconfiguration\nis automatically saved and is then used for every subsequent command.\n\nAll configuration options for the oc login command, listed in the oc login\n--help command output, are optional. The following example shows usage with\nsome common options:\n\nThe following table describes these common options:\n\nCLI configuration files allow you to easily\nmanage multiple CLI profiles.\n\nIf you have access to administrator credentials but are no longer logged in as\nthe default\nsystem user system:admin, you can log back in as this user at any time as\nlong as the credentials are still present in your\nCLI\nconfiguration file. The following command logs in and switches to the default\nproject:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/dev_guide/index.html", "title": "Atomic Registry Latest | User Guide | Overview", "content": "\nThis guide helps image developers set up and configure a workstation to\ndevelop images working with the Atomic Registry environment using the web console\nand command-line interface (CLI). This guide provides detailed instructions and\nexamples to help image developers:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/dev_guide/managing_images.html", "title": "Atomic Registry Latest | User Guide | Managing Images", "content": "\nAn\nimage\nstream comprises any number of\ncontainer\nimages identified by tags. It presents a single virtual view of related images,\nsimilar to a Docker image repository.\n\nBy watching an image stream, builds and deployments can receive notifications\nwhen new images are added or modified and react by performing a build or\ndeployment, respectively.\n\nThere are many ways you can interact with images and set up image streams,\ndepending on where the images' registries are located, any authentication\nrequirements around those registries, and how you want your builds and\ndeployments to behave. The following sections cover a range of these topics.\n\nBefore working with Atomic Registry image streams and their tags, it will help\nto first understand image tags in the context of container images generally.\n\nContainer images can have names added to them that make it more intuitive to determine\nwhat they contain, called a tag. Using a tag to specify the version of what is contained\nin the image is a common use case. If you have an image named ruby, you could\nhave a tag named 2.0 for 2.0 version of Ruby, and another named latest to\nindicate literally the latest built image in that repository overall.\n\nWhen interacting directly with images using the docker CLI, the docker tag\ncommand can add tags, which essentially adds an alias to an image that can\nconsist of several parts. Those parts can include:\n\nThe <user_name> part in the above could also refer to a\nproject or\nnamespace\nif the image is being stored in an Atomic Registry environment with an internal\nregistry (the OpenShift Container Registry).\n\nAtomic Registry provides the oc tag command, which is similar to the docker\ntag command, but operates on image streams instead of directly on images.\n\nSee Red Hat Enterprise Linux 7\u2019s\nGetting\nStarted with Containers documentation for more about tagging images directly\nusing the docker CLI.\n\nKeeping in mind that an image stream in Atomic Registry comprises zero or more\ncontainer images identified by tags, you can add tags to an image stream using the\noc tag command:\n\nFor example, to configure the ruby image\u2019s latest tag to always refer to the\ncurrent image for the tag 2.0:\n\nThere are different types of tags available. The default behavior uses a\npermanent tag, which points to a specific image in time; even when the source\nchanges, it will not reflect in the destination tag.\n\nA tracking tag means the destination tag\u2019s metadata will be imported during\nthe import. To ensure the destination tag is updated whenever the source tag\nchanges, use the --alias=true flag:\n\nUse a tracking tag for creating permanent aliases (for example, latest or\nstable). The tag works correctly only within a single image stream. Trying\nto create a cross-image-stream alias will produce an error.\n\nYou can also add the --scheduled=true flag to have the destination tag be\nrefreshed (i.e., re-imported) periodically. The period is configured globally at\nsystem level. See Importing Tag and Image\nMetadata for more details.\n\nIf you want to instruct Docker to always fetch the tagged image from the\nintegrated registry, use --reference-policy=local. The registry uses the\npull-through feature\nto serve the image to the client. By default, the image blobs are\nmirrored locally by the registry. As a result, they are available for a  shorter\ntime the next time they are needed. The flag also allows for pulling from\ninsecure registries without a need to supply --insecure-registry to the Docker\ndaemon if the image stream has an insecure annotation\nor the tag has an insecure import policy.\n\nImages evolve over time and their tags reflect this. An image tag always points\nto the latest image built.\n\nIf there is too much information embedded in a tag name (for example,\nv2.0.1-may-2016), the tag will point to just one revision of an image and will\nnever be updated. Using default image pruning options, such an image will never\nbe removed.\n\nInstead, if the tag is named v2.0, more image revisions are more likely. This\nresults in longer\ntag history and, therefore, the image pruner will more likely remove old and unused images.\n\nAlthough tag naming convention is up to you, here are a few examples in the\nformat <image_name>:<image_tag>:\n\nIf you require dates in tag names, periodically inspect old and unsupported\nimages and istags and remove them. Otherwise, you might experience increasing\nresource usage caused by old images.\n\nTo remove a tag completely from an image stream run:\n\nor:\n\nImages can be referenced in image streams using the following reference types:\n\nThe <id> is an immutable identifier for a specific image, also called a\ndigest.\n\nWhen no tag is specified, it is assumed the latest tag will be used.\n\nYou can also reference a third-party registry:\n\nOr an image with a digest:\n\nWhen viewing example image stream definitions, such as the\nexample\nCentOS image streams, you may notice they contain definitions of\nImageStreamTag and references to DockerImage, but nothing related to\nImageStreamImage.\n\nThis is because the ImageStreamImage objects are automatically created in\nAtomic Registry whenever you import or tag an image into the image stream. You\nshould never have to explicitly define an ImageStreamImage object in any\nimage stream definition that you use to create image streams.\n\nYou can view an image\u2019s object definition by retrieving an ImageStreamImage\ndefinition using the image stream name and ID:\n\nYou can find valid <id> values for a given image stream by running:\n\nFor example, from the ruby image stream asking for the ImageStreamImage\nwith the name and ID of ruby@3a335d7:\n\nYou can access Atomic Registry\u2019s internal registry directly to push or pull\nimages. For example, this could be helpful if you wanted to\ncreate an image\nstream by manually pushing an image, or just to docker pull an image\ndirectly.\n\nThe internal registry authenticates using the same\ntokens\nas the Atomic Registry API. To perform a docker login against the internal\nregistry, you can choose any user name and email, but the password must be a\nvalid Atomic Registry token.\n\nTo log into the internal registry:\n\nContact your cluster administrator if you do not know the registry IP or host\nname and port to use.\n\nIn order to pull an image, the authenticated user must have get rights on the\nrequested imagestreams/layers. In order to push an image, the authenticated\nuser must have update rights on the requested imagestreams/layers.\n\nBy default, all service accounts in a project have rights to pull any image in\nthe same project, and the builder service account has rights to push any image\nin the same project.\n\nAn image stream can be configured to import tag and image metadata from an image\nrepository in an external Docker image registry. You can do this using a few\ndifferent methods.\n\nFor example:\n\nYou can also add the --all flag to import all tags for the image instead of\njust latest.\n\nThen create the object:\n\nWhen you create an image stream that references an image in an external Docker\nregistry, Atomic Registry communicates with the external registry within a short\namount of time to get up to date information about the image.\n\nAfter the tag and image metadata is synchronized, the image stream object would\nlook similar to the following:\n\nYou can set a tag to query external registries at a scheduled interval to\nsynchronize tag and image metadata by setting the --scheduled=true flag with\nthe oc tag command as mentioned in Adding Tags to Image\nStreams.\n\nAlternatively, you can set importPolicy.scheduled to true in the tag\u2019s\ndefinition:\n\nAn image stream can be configured to import tag and image metadata from insecure\nimage registries, such as those signed with a self-signed certificate or using\nplain HTTP instead of HTTPS.\n\nTo configure this, add the openshift.io/image.insecureRepository annotation\nand set it to true. This setting bypasses certificate validation when\nconnecting to the registry:\n\nThis option instructs integrated registry to fall back to an insecure transport\nfor any external image tagged in the image stream when serving it, which is\ndangerous. If possible, avoid this risk by\nmarking just an istag as insecure.\n\nThe above annotation applies to all images and tags of a particular\nImageStream. For a finer-grained control, policies may be set on\nistags.\nSet importPolicy.insecure in the tag\u2019s definition to true to allow a\nfall-back to insecure transport just for images under this tag.\n\nThe fall-back to insecure transport for an image under particular istag will\nbe enabled either when the image stream is annotated as insecure or the istag\nhas insecure import policy. The importPolicy.insecure` set to false can not\noverride the image stream annotation.\n\nThe Reference Policy allows you to specify where the image consumers will pull\nfrom. It is only applicable to remote images (those imported from external\nregistries). There are two options to choose from, Local and Source.\n\nThe Source policy instructs clients to pull directly from the source registry\nof the image. The integrated registry is not involved unless the image is\nmanaged by the cluster. (It is not an external image.) This is the default\npolicy.\n\nThe Local policy instructs clients to always pull from the integrated\nregistry. This is useful if you want to pull from external insecure registries\nwithout modifying Docker daemon settings.\n\nThe\npull-through feature\nof the registry serves the remote image to the client. Additionally, all the\nblobs are mirrored for faster access later.\n\nYou can set the policy in a specification of image stream tag as\nreferencePolicy.type.\n\nAn image stream can also be automatically created by manually pushing an image\nto the internal registry. This is only possible when using an Atomic Registry\ninternal registry.\n\nBefore performing this procedure, the following must be satisfied:\n\nTo create an image stream by manually pushing an image:\n\nImage streams for S2I builders that are displayed in the management\nconsole\u2019s catalog page require additional metadata to provide the best\nexperience for end users.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/dev_guide/service_accounts.html", "title": "Atomic Registry Latest | User Guide | Service Accounts", "content": "\nWhen a person uses the Atomic Registry CLI or web console, their API token\nauthenticates them to the OpenShift API. However, when a regular user\u2019s\ncredentials are not available, it is common for components to make API calls\nindependently.\n\nService accounts provide a flexible way to control API access without sharing a\nregular user\u2019s credentials.\n\nEvery service account has an associated user name that can be granted roles,\njust like a regular user. The user name is derived from its project and name:\n\nFor example, to add the view role to the robot service account in the\ntop-secret project:\n\nEvery service account is also a member of two groups:\n\nFor example, to allow all service accounts in all projects to view resources in\nthe top-secret project:\n\nTo allow all service accounts in the managers project to edit resources in the\ntop-secret project:\n\nThree service accounts are automatically created in every project:\n\nAll service accounts in a project are given the system:image-puller role,\nwhich allows pulling images from any image stream in the project using the\ninternal Docker registry.\n\nAs soon as a service account is created, two secrets are automatically added to\nit:\n\nThese can be seen by describing the service account:\n\nThe system ensures that service accounts always have an API token and internal\nDocker registry credentials.\n\nThe generated API token and Docker registry credentials do not expire, but they\ncan be revoked by deleting the secret. When the secret is deleted, a new one is\nautomatically generated to take its place.\n\nThe same token can be distributed to external applications that need to\nauthenticate to the API.\n\nUse the following syntax to to view a service account\u2019s API token:\n\nFor example:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/adding_hosts_to_existing_cluster.html", "title": "Atomic Registry Latest | Installation and Configuration | Adding Hosts to an Existing Cluster", "content": "\nDepending on how your Atomic Registry cluster was installed, you can add new\nhosts (either nodes or masters) to your installation by using the install tool\nfor quick installations, or by using the scaleup.yml playbook for advanced\ninstallations.\n\nIf you used the quick install tool to install your Atomic Registry cluster, you\ncan use the quick install tool to add a new node host to your existing cluster.\n\nCurrently, you can not use the quick installer tool to add new master hosts. You\nmust use the\nadvanced\ninstallation method to do so.\n\nIf you used the installer in either\ninteractive or\nunattended mode, you can re-run the\ninstallation as long as you have an\ninstallation configuration\nfile at ~/.config/openshift/installer.cfg.yml (or specify a different\nlocation with the -c option).\n\nThe recommended maximum number of nodes is 300.\n\nTo add nodes to your installation:\n\nChoose (y) and follow the on-screen instructions to complete your desired task.\n\nIf you installed using the advanced install, you can add new hosts to your\ncluster by running the scaleup.yml playbook. This playbook queries the\nmaster, generates and distributes new certificates for the new hosts, then runs\nthe configuration playbooks on the new hosts only. Before running the\nscaleup.yml playbook, complete all prerequisite\nhost\npreparation steps.\n\nYou must have an existing inventory file (for example, /etc/ansible/hosts)\nthat is representative of your current cluster configuration in order to run the\nscaleup.yml playbook.\n\nThe recommended maximum number of nodes is 300.\n\nTo add a host to an existing cluster:\n\nFor example, to add a new node host, add new_nodes:\n\nTo add new master hosts, add new_masters.\n\nSee\nConfiguring\nHost Variables for more options.\n\nWhen adding new masters, hosts added to the [new_masters] section must also be\nadded to the [new_nodes] section. This ensures the new master host is part of\nthe OpenShift SDN.\n\nMasters are also automatically marked as unschedulable for pod placement by the\ninstaller.\n\nIf you label a master host with the region=infra label and have no other\ndedicated infrastructure nodes, you must also explicitly mark the host as\nschedulable by adding openshift_schedulable=true to the entry. Otherwise, the\nregistry and router pods cannot be placed anywhere.\n\nFor additional nodes:\n\nFor additional masters:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/advanced_ldap_configuration/configuring_extended_ldap_attributes.html", "title": "Atomic Registry Latest | Installation and Configuration | Advanced LDAP Configuration | Configuring Extended LDAP Attributes", "content": "\nThis topic builds upon\nSetting up SSSD\nfor LDAP Failover and\nConfiguring\nForm-Based Authentication and focuses on configuring extended Lightweight\nDirectory Access Protocol (LDAP) attributes.\n\nYou need to ask System Security Services Daemon (SSSD) to look up attributes in\nLDAP that it normally does not care about for simple system-login use-cases. In\nthe case of Atomic Registry, there is only one such attribute: email. So, you need to:\n\nNow that SSSD is set up and successfully serving extended attributes, configure\nthe web server to ask for them and to insert them in the correct places.\n\nTell Atomic Registry where to find these new attributes during login. To do so:\n\nYou should see their full name appear in the upper-right of the\nscreen. You can also verify with oc get identities -o yaml that both email\naddresses and full names are available.\n\nCurrently, Atomic Registry only saves these attributes to the user at the time\nof the first login and does not update them again after that. So, while you are\ntesting (and only while testing), run oc delete users,identities --all to\nclear the identities out so you can log in again.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/advanced_ldap_configuration/configuring_form_based_authentication.html", "title": "Atomic Registry Latest | Installation and Configuration | Advanced LDAP Configuration | Configuring Form-Based Authentication", "content": "\nThis topic builds upon\nSetting\nup SSSD for LDAP Failover and describes how to set up form-based authentication\nfor signing into the Atomic Registry web console.\n\nThe Atomic Registry upstream repositories have a template for forms. Copy that\nto your authenticating proxy on proxy.example.com:\n\nModify this .html file to change the logo icon and \"Welcome\" content for your\nenvironment.\n\nTo intercept form-based authentication, install an Apache module:\n\nThis tells Apache to listen for POST requests on the\n/login-proxy/oauth/authorize and to pass the user name and password over to\nthe openshift PAM service.\n\nYou should be able to browse to https://openshift.example.com:8443 and use\nyour LDAP credentials to sign in via the login form.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/advanced_ldap_configuration/index.html", "title": "Atomic Registry Latest | Installation and Configuration | Advanced LDAP Configuration | Overview", "content": "\nAtomic Registry Advanced Lightweight Directory Access Protocol (LDAP)\nConfiguration covers the following topics:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/advanced_ldap_configuration/sssd_for_ldap_failover.html", "title": "Atomic Registry Latest | Installation and Configuration | Advanced LDAP Configuration | Setting up SSSD for LDAP Failover", "content": "\nAtomic Registry provides an\nauthentication\nprovider for use with Lightweight Directory Access Protocol (LDAP) setups, but\nit can only connect to a single LDAP server. This can be problematic if that\nLDAP server becomes unavailable. System Security Services Daemon (SSSD) can be\nused to solve the issue.\n\nOriginally designed to manage local and remote authentication to the host\noperating system, SSSD can now be configured to provide identity,\nauthentication, and authorization services to web services like Atomic Registry.\nSSSD provides advantages over the built-in LDAP provider, including the ability\nto connect to any number of failover LDAP servers, as well as the ability to\ncache authentication attempts in case it can no longer reach any of those\nservers.\n\nThe setup for this configuration is advanced and requires a separate\nauthentication server (also called an authenticating proxy) for\nAtomic Registry to communicate with. This topic describes how to do this setup\non a dedicated physical or virtual machine (VM), but the concepts are also\napplicable to a setup in a container.\n\nThese VMs can be configured to run on the same system, but for the examples used\nin this topic they are kept separate.\n\nThis phase generates certificate files that are valid for two years (or five\nyears for the certification authority (CA) certificate). This can be altered\nwith the --expire-days and --signer-expire-days options, but for security\nreasons, it is recommended to not make them greater than these values.\n\nAmong other things, this generates a /etc/origin/master/ca.{cert|key}. Use\nthis signing certificate to generate keys to use on the authenticating proxy.\n\nEnsure that any host names and interface IP addresses that need to access the\nproxy are listed. Otherwise, the HTTPS connection will fail.\n\nThis prevents malicious users from impersonating the proxy and sending fake\nidentities.\n\nThis section guides you through the steps to authenticate the proxy setup.\n\nFrom openshift.example.com, securely copy the necessary certificates to the\nproxy machine:\n\nThis gives you the needed SSSD and the web server components.\n\nFor more advanced case, see the\nSystem-Level Authentication Guide\n\nIf you want to use SSSD to manage failover situations for LDAP, this can be\nconfigured by adding additional entries in /etc/sssd/sssd.conf on the\nldap_uri line. Systems enrolled with FreeIPA can automatically handle\nfailover using DNS SRV records.\n\nIf you do not want LDAP users to be able to log into this machine, it is\nrecommended to modify /etc/pam.d/system-auth and\n/etc/pam.d/password-auth to remove the lines containing pam_sss.so.\n\nYou need to set up Apache to communicate with SSSD. Create a PAM stack file for\nuse with Apache. To do so:\n\nThis configuration enables PAM (the pluggable authentication module) to use\npam_sss.so to determine authentication and access control when an\nauthentication request is issued for the openshift stack.\n\nConfiguring\nForm-Based Authentication explains how to set up a graphical login using SSSD\nas well, but it requires the rest of this setup as a prerequisite.\n\nConfiguring\nForm-Based Authentication explains how to add the login-proxy block to\nsupport form authentication.\n\nThis section describes how to set up an Atomic Registry server from scratch in\nan \"all in one\" configuration.\nMaster and Node\nConfiguration provides more information on alternate configurations.\n\nModify the default configuration to use the new identity provider just\ncreated. To do so:\n\nConfiguring\nForm-Based Authentication explains how to add the login URL to support web\nlogins.\n\nConfiguring\nExtended LDAP Attributes explains how to add the email and full-name\nattributes. Note that the full-name attributes are only stored to the database\non the first login.\n\nIt should now be possible to log in with only valid LDAP credentials.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/certificate_customization.html", "title": "Atomic Registry Latest | Installation and Configuration | Configuring Custom Certificates", "content": "\nAdministrators can configure custom serving certificates for the public host\nnames of the Atomic Registry API and\nweb console.\nThis can be done during an\nadvanced installation or configured after installation.\n\nDuring\nadvanced installations,\ncustom certificates can be configured using the\nopenshift_master_named_certificates and\nopenshift_master_overwrite_named_certificates parameters, which are\nconfigurable in the inventory file. More details are available about\nconfiguring custom certificates with Ansible.\n\nIn the master\nconfiguration file you can list the namedCertificates section in the assetConfig.servingInfo section so the custom certificate serves up for the web console, and in the servingInfo section so the custom certificate serves up for the CLI and other API calls. Multiple certificates can be configured this way and each certificate may be associated with multiple host names or wildcards.\n\nA default certificate must be configured in the servingInfo.certFile and\nservingInfo.keyFile configuration sections in addition to\nnamedCertificates.\n\nThe namedCertificates section should only be configured for the host name\nassociated with the masterPublicURL, assetConfig.publicURL, and\noauthConfig.assetPublicURL settings. Using a custom serving certificate for\nthe host name associated with the masterURL will result in TLS errors as\ninfrastructure components will attempt to contact the master API using the\ninternal masterURL host.\n\nRelative paths are resolved relative to the master configuration file. Restart\nthe server to pick up the configuration changes.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/configuring_authentication.html", "title": "Atomic Registry Latest | Installation and Configuration | Configuring Authentication and User Agent", "content": "\nThe Atomic Registry\nmaster\nincludes a built-in\nOAuth\nserver. Developers and administrators obtain\nOAuth\naccess tokens to authenticate themselves to the API.\n\nAs an administrator, you can configure OAuth using the\nmaster configuration file to specify an\nidentity provider.\nThis can be done during an\nadvanced installation or configured after installation.\n\nIf you installed Atomic Registry using\nthe\nAdvanced Installation\nmethod, the\n\nWhen running a master without a configuration file, the\nAllow All identity provider is used by\ndefault, which allows any non-empty user name and password to log in. This is\nuseful for testing purposes. To use other identity providers, or to modify any\ntoken, grant, or\nsession options, you must run the master from a\nconfiguration file.\n\nRoles need\nto be assigned to administer the setup with an external user.\n\nAfter making changes to an identity provider, you must restart the master service for the changes to take effect:\n\nFor initial advanced installations, the\nDeny All identity provider is configured by default,\nthough it can be\noverridden during installation using the\nopenshift_master_identity_providers parameter, which is configurable in the inventory file.\nSession options in the OAuth configuration are also configurable in the inventory file.\n\nYou can configure the master host for authentication using your desired identity\nprovider by modifying the\nmaster configuration\nfile. The following sections detail the identity providers supported by\nAtomic Registry.\n\nThere are four parameters common to all identity providers:\n\nname\n\nThe provider name is prefixed to provider user names to form an\nidentity name.\n\nchallenge\n\nWhen true, unauthenticated token requests from non-web\nclients (like the CLI) are sent a WWW-Authenticate challenge header. Not\nsupported by all identity providers.\n\nTo prevent cross-site request forgery (CSRF) attacks against browser clients\nBasic authentication challenges are only sent if a X-CSRF-Token header is\npresent on the request. Clients that expect to receive Basic WWW-Authenticate\nchallenges should set this header to a non-empty value.\n\nlogin\n\nWhen true, unauthenticated token requests from web clients\n(like the web console) are redirected to a login page backed by this provider.\nNot supported by all identity providers.\n\nIf you want users to be sent to a branded page before being redirected to\nthe identity provider\u2019s login, then set oauthConfig \u2192 alwaysShowProviderSelection: true\nin the master configuration file. This provider selection page can be\ncustomized.\n\nmappingMethod\n\nDefines how new identities are mapped to users when they login. See Mapping Identities to Users for more information.\n\nSetting the mappingMethod parameter in a\nmaster configuration file\ndetermines how identities are mapped to users:\n\nWhen set to the default claim value, OAuth will fail if the identity is\nmapped to a previously-existing user name. The following table outlines the use\ncases for the available mappingMethod parameter values:\n\nSet AllowAllPasswordIdentityProvider in the identityProviders stanza to\nallow any non-empty user name and password to log in. This is the default\nidentity provider when running Atomic Registry without a\nmaster configuration file.\n\nSet DenyAllPasswordIdentityProvider in the identityProviders stanza to\ndeny access for all user names and passwords.\n\nSet HTPasswdPasswordIdentityProvider in the identityProviders stanza to\nvalidate user names and passwords against a flat file generated using\nhtpasswd.\n\nThe htpasswd utility is in the httpd-tools package:\n\nAtomic Registry supports the Bcrypt, SHA-1, and MD5 cryptographic hash\nfunctions, and MD5 is the default for htpasswd. Plaintext, encrypted text, and\nother hash functions are not currently supported.\n\nThe flat file is reread if its modification time changes, without requiring a\nserver restart.\n\nTo use the htpasswd command:\n\nThen, enter and confirm a clear-text password for the user. The command generates a hashed version of the password.\n\nFor example:\n\nYou can include the -b option to supply the password on the command line:\n\nFor example:\n\nSet KeystonePasswordIdentityProvider in the identityProviders stanza to\nvalidate user names and passwords against an OpenStack Keystone v3 server.\nThis enables shared authentication with an OpenStack server configured to store\nusers in an internal Keystone database.\n\nSet LDAPPasswordIdentityProvider in the identityProviders stanza to\nvalidate user names and passwords against an LDAPv3 server, using simple bind\nauthentication.\n\nDuring authentication, the LDAP directory is searched for an entry that matches\nthe provided user name. If a single unique match is found, a simple bind is\nattempted using the distinguished name (DN) of the entry plus the provided\npassword.\n\nThese are the steps taken:\n\nThe configured url is an RFC 2255 URL, which specifies the LDAP host and\nsearch parameters to use. The syntax of the URL is:\n\nFor the above example:\n\nldap\n\nFor regular LDAP, use the string ldap. For secure LDAP\n(LDAPS), use ldaps instead.\n\nhost:port\n\nThe name and port of the LDAP server. Defaults to\nlocalhost:389 for ldap and localhost:636 for LDAPS.\n\nbasedn\n\nThe DN of the branch of the directory where all searches should\nstart from. At the very least, this must be the top of your directory tree, but\nit could also specify a subtree in the directory.\n\nattribute\n\nThe attribute to search for. Although RFC 2255 allows a\ncomma-separated list of attributes, only the first attribute will be used, no\nmatter how many are provided. If no attributes are provided, the default is to\nuse uid. It is recommended to choose an attribute that will be unique across\nall entries in the subtree you will be using.\n\nscope\n\nThe scope of the search. Can be either either one or sub.\nIf the scope is not provided, the default is to use a scope of sub.\n\nfilter\n\nA valid LDAP search filter. If not provided, defaults to\n(objectClass=*)\n\nWhen doing searches, the attribute, filter, and provided user name are combined\nto create a search filter that looks like:\n\nFor example, consider a URL of:\n\nWhen a client attempts to connect using a user name of bob, the resulting\nsearch filter will be (&(enabled=true)(cn=bob)).\n\nIf the LDAP directory requires authentication to search, specify a bindDN and\nbindPassword to use to perform the entry search.\n\nSet BasicAuthPasswordIdentityProvider in the identityProviders stanza to\nvalidate user names and passwords against a remote server using a\nserver-to-server Basic authentication request. User names and passwords are\nvalidated against a remote URL that is protected by Basic authentication and\nreturns JSON.\n\nA 401 response indicates failed authentication.\n\nA non-200 status, or the presence of a non-empty \"error\" key, indicates an\nerror:\n\nA 200 status with a sub (subject) key indicates success:\n\nA successful response may optionally provide additional data, such as:\n\nSet RequestHeaderIdentityProvider in the identityProviders stanza to\nidentify users from request header values, such as X-Remote-User. It is\ntypically used in combination with an authenticating proxy, which sets the\nrequest header value. This is similar to how\nthe remote user plug-in in OpenShift Enterprise 2 allowed administrators to\nprovide Kerberos, LDAP, and many other forms of enterprise authentication.\n\nFor users to authenticate using this identity provider, they must access\nhttps://<master>/oauth/authorize (and subpaths) via an authenticating proxy.\nTo accomplish this, configure the OAuth server to redirect unauthenticated\nrequests for OAuth tokens to the proxy endpoint that proxies to https://<master>/oauth/authorize.\n\nTo redirect unauthenticated requests from clients expecting browser-based login flows:\n\nTo redirect unauthenticated requests from clients expecting WWW-Authenticate challenges:\n\nThe provider.challengeURL and provider.loginURL parameters can include\nthe following tokens in the query portion of the URL:\n\nFor example: https://www.example.com/sso-login?then=${url}\n\nFor example: https://www.example.com/auth-proxy/oauth/authorize?${query}\n\nIf you expect unauthenticated requests to reach the OAuth server, a clientCA\nparameter MUST be set for this identity provider, so that incoming requests\nare checked for a valid client certificate before the request\u2019s headers are\nchecked for a user name. Otherwise, any direct request to the OAuth server can\nimpersonate any identity from this provider, merely by setting a request header.\n\nThis example configures an authentication proxy on the same host as the master.\nHaving the proxy and master on the same host is merely a convenience and may not\nbe suitable for your environment. For example, if you were already\nrunning a router on the\nmaster, port 443 would not be available.\n\nIt is also important to note that while this reference configuration uses\nApache\u2019s mod_auth_form, it is by no means required and other proxies can\neasily be used if the following requirements are met:\n\nInstalling the Prerequisites\n\nThe mod_auth_form module is shipped as part of the mod_session package that\nis found in the Optional channel:\n\nGenerate a CA for validating requests that submit the trusted header. This CA\nshould be used as the file name for clientCA in the\nmaster\u2019s identity provider configuration.\n\nGenerate a client certificate for the proxy. This can be done using any x509\ncertificate tooling. For convenience, the oadm CLI can be used:\n\nConfiguring Apache\n\nUnlike OpenShift Enterprise 2, this proxy does not need to reside on the same\nhost as the master. It uses a client certificate to connect to the master, which\nis configured to trust the X-Remote-User header.\n\nConfigure Apache per the following:\n\nAdditional mod_auth_form Requirements\n\nA sample login page is available from the\nopenshift_extras\nrepository. This file should be placed in the DocumentRoot location\n(/var/www/html by default).\n\nCreating Users\n\nAt this point, you can create the users in the system Apache is using to store\naccounts information. In this example, file-backed authentication is used:\n\nConfiguring the Master\n\nThe identityProviders stanza in the\n/etc/origin/master/master-config.yaml file must be updated as well:\n\nRestarting Services\n\nFinally, restart the following services:\n\nVerifying the Configuration\n\nSet GitHubIdentityProvider in the identityProviders stanza to use\nGitHub as an identity provider, using the\nOAuth integration.\n\nUsing GitHub as an identity provider requires users to get a token using\n<master>/oauth/token/request to use with command-line tools.\n\nUsing GitHub as an identity provider allows any GitHub user to authenticate to your server.\nYou can limit authentication to members of specific GitHub organizations with the\norganizations configuration attribute, as shown below.\n\nSet GitLabIdentityProvider in the identityProviders stanza to use\nGitLab.com or any other GitLab instance as an identity provider, using the\nOAuth integration.\nThe OAuth provider feature requires GitLab version 7.7.0 or higher.\n\nUsing GitLab as an identity provider requires users to get a token using\n<master>/oauth/token/request to use with command-line tools.\n\nSet GoogleIdentityProvider in the identityProviders stanza to use Google\nas an identity provider, using\nGoogle\u2019s OpenID\nConnect integration.\n\nUsing Google as an identity provider requires users to get a token using\n<master>/oauth/token/request to use with command-line tools.\n\nUsing Google as an identity provider allows any Google user to authenticate to your server.\nYou can limit authentication to members of a specific hosted domain with the\nhostedDomain configuration attribute, as shown below.\n\nSet OpenIDIdentityProvider in the identityProviders stanza to integrate\nwith an OpenID Connect identity provider using an\nAuthorization Code Flow.\n\nID Token and UserInfo decryptions are not supported.\n\nBy default, the openid scope is requested. If required, extra scopes can be\nspecified in the extraScopes field.\n\nClaims are read from the JWT id_token returned from the OpenID identity\nprovider and, if specified, from the JSON returned by the UserInfo URL.\n\nAt least one claim must be configured to use as the user\u2019s identity. The\nstandard identity claim is sub.\n\nYou can also indicate which claims to use as the user\u2019s preferred user name,\ndisplay name, and email address. If multiple claims are specified, the first one\nwith a non-empty value is used. The\nstandard claims are:\n\nUsing an OpenID Connect identity provider requires users to get a token using\n<master>/oauth/token/request to use with command-line tools.\n\nA custom certificate bundle, extra scopes, extra authorization request\nparameters, and userInfo URL can also be specified:\n\nThe OAuth server generates two kinds of tokens:\n\nUse the tokenConfig stanza to set token options:\n\nWhen the OAuth server receives token requests for a client to which the user\nhas not previously granted permission, the action that the OAuth server takes\nis dependent on the OAuth client\u2019s grant strategy.\n\nWhen the OAuth client requesting token does not provide its own grant strategy,\nthe server-wide default strategy is used. To configure the default strategy,\nset the method value in the grantConfig stanza. Valid values for\nmethod are:\n\nThe OAuth server uses a signed and encrypted cookie-based session during login\nand redirect flows.\n\nUse the sessionConfig stanza to set session options:\n\nIf no sessionSecretsFile is specified, a random signing and encryption\nsecret is generated at each start of the master server. This means that any\nlogins in progress will have their sessions invalidated if the master is\nrestarted. It also means that if multiple masters are configured, they will not\nbe able to decode sessions generated by one of the other masters.\n\nTo specify the signing and encryption secret to use, specify a\nsessionSecretsFile. This allows you separate secret values from the\nconfiguration file and keep the configuration file distributable, for example\nfor debugging purposes.\n\nMultiple secrets can be specified in the sessionSecretsFile to enable\nrotation. New sessions are signed and encrypted using the first secret in the\nlist. Existing sessions are decrypted and authenticated by each secret until one\nsucceeds.\n\nAtomic Registry implements a user agent that can be used to prevent an\napplication developer\u2019s CLI accessing the Atomic Registry API.\n\nUser agents for the Atomic Registry CLI are constructed from a set of values\nwithin Atomic Registry:\n\nSo, for example, when:\n\nthe user agent will be:\n\nAs an Atomic Registry administrator, you can prevent clients from accessing the\nAPI with the userAgentMatching configuration setting of a master\nconfiguration. So, if a client is using a particular library or\nbinary, they will be prevented from accessing the API.\n\nThe following user agent example denies the Kubernetes 1.2 client binary,\nOpenShift Origin 1.1.3 binary, and the POST and PUT httpVerbs:\n\nAdministrators can also deny clients that do not exactly match the expected\nclients:\n\nWhen the client\u2019s user agent mismatches the configuration, errors occur. To\nensure that mutating requests match, enforce a whitelist. Rules are mapped to\nspecific verbs, so you can ban mutating requests while allowing non-mutating\nrequests.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/redeploying_certificates.html", "title": "Atomic Registry Latest | Installation and Configuration | Redeploying Certificates", "content": "\nAtomic Registry uses certificates to provide secure connections for the\nfollowing components:\n\nYou can use Ansible playbooks provided with the installer to automate checking\nexpiration dates for cluster certificates. Playbooks are also provided to\nautomate backing up and redeploying these certificates, which can fix common\ncertificate errors.\n\nPossible use cases for redeploying certificates include:\n\nYou can use the installer to warn you about any certificates expiring within a\nconfigurable window of days and notify you about any certificates that have\nalready expired. Certificate expiry playbooks use the Ansible role\nopenshift_certificate_expiry.\n\nCertificates examined by the role include:\n\nThe openshift_certificate_expiry role uses the following variables:\n\nThe Atomic Registry installer provides a set of example certificate expiration\nplaybooks, using different sets of configuration for the\nopenshift_certificate_expiry role.\n\nThese playbooks must be used with an\ninventory file that is representative of the cluster. For best results, run\nansible-playbook with the -v option.\n\nUsing the easy-mode.yaml example playbook, you can try the role out before\ntweaking it to your specifications as needed. This playbook:\n\nTo run the easy-mode.yaml  playbook:\n\nThe other example playbooks are also available to run directly out of the\n/usr/share/ansible/openshift-ansible/playbooks/certificate_expiry/\ndirectory.\n\nTo run any of these example playbooks:\n\nAs noted above, there are two ways to format your check report. In JSON format\nfor machine parsing, or as a stylized HTML page for easy skimming.\n\nAn example of an HTML report is provided with the installer. You can open the\nfollowing file in your browser to view it:\n\n/usr/share/ansible/openshift-ansible/roles/openshift_certificate_expiry/examples/cert-expiry-report.html\n\nThere are two top-level keys in the saved JSON results: data and summary.\n\nThe data key is a hash where the keys are the names of each host examined and\nthe values are the check results for the certificates identified on each\nrespective host.\n\nThe summary key is a hash that summarizes the total number of certificates:\n\nFor an example of the full JSON report, see /usr/share/ansible/openshift-ansible/roles/openshift_certificate_expiry/examples/cert-expiry-report.json.\n\nThe summary from the JSON data can be easily checked for warnings or expirations\nusing a variety of command-line tools. For example, using grep you can look\nfor the word summary and print out the two lines after the match (-A2):\n\nIf available, the jq tool can also be used to pick out specific values. The\nfirst two examples below show how to select just one value, either warning or\nexpired. The third example shows how to select both values at once:\n\nUse the following playbooks to redeploy master, etcd, node, registry, and router\ncertificates on all relevant hosts. You can redeploy all of them at once using\nthe current CA, redeploy certificates for specific components only, or redeploy\na newly generated or custom CA on its own.\n\nJust like the certificate expiry playbooks, these playbooks must be run with an\ninventory file that is representative of the cluster.\n\nIn particular, the inventory must specify or override all host names and IP\naddresses set via the following variables such that they match the current\ncluster configuration:\n\nThe validity (length in days until they expire) for any certificates\nauto-generated while redeploying can be configured via Ansible as well. See\nConfiguring Certificate Validity.\n\nThe redeploy-certificates.yml playbook does not regenerate the\nAtomic Registry CA certificate. New master, etcd, node, registry, and router\ncertificates are created using the current CA certificate to sign new\ncertificates.\n\nThis also includes serial restarts of:\n\nTo redeploy master, etcd, and node certificates using the current\nAtomic Registry CA, run this playbook, specifying your inventory file:\n\nThe redeploy-openshift-ca.yml playbook redeploys the Atomic Registry CA\ncertificate by generating a new CA certificate and distributing an updated\nbundle to all components including client kubeconfig files and the node\u2019s\ndatabase of trusted CAs (the CA-trust).\n\nThis also includes serial restarts of:\n\nAdditionally, you can specify a\ncustom CA certificate when redeploying certificates instead of relying on a CA\ngenerated by Atomic Registry.\n\nWhen the master services are restarted, the registry and routers can continue to\ncommunicate with the master without being redeployed because the master\u2019s\nserving certificate is the same, and the CA the registry and routers have are\nstill valid.\n\nTo redeploy a newly generated or custom CA:\n\nIf you do not set the above, then the current CA will be regenerated in the next\nstep.\n\nWith the new CA in place, you can then use the\nredeploy-certificates.yml playbook at your discretion whenever you want to redeploy certificates signed\nby the new CA on all components.\n\nThe redeploy-master-certificates.yml playbook only redeploys master\ncertificates. This also includes serial restarts of master services.\n\nTo redeploy master certificates, run this playbook, specifying your inventory\nfile:\n\nThe redeploy-etcd-certificates.yml playbook only redeploys etcd certificates\nincluding master client certificates.\n\nThis also include serial restarts of:\n\nTo redeploy etcd certificates, run this playbook, specifying your inventory\nfile:\n\nThe redeploy-node-certificates.yml playbook only redeploys node\ncertificates. This also include serial restarts of node services.\n\nTo redeploy node certificates, run this playbook, specifying your inventory\nfile:\n\nThe redeploy-registry-certificates.yml and\nredeploy-router-certificates.yml playbooks replace installer-created\ncertificates for the registry and router. If custom certificates are in use for\nthese components, see\nRedeploying Custom\nRegistry or Router Certificates to replace them manually.\n\nTo redeploy registry certificates, run the following playbook, specifying your\ninventory file:\n\nTo redeploy router certificates, run the following playbook, specifying your\ninventory file:\n\nWhen nodes are evacuated due to a redeployed CA, registry and router pods are\nrestarted. If the registry and router certificates were not also redeployed with\nthe new CA, this can cause outages because they cannot reach the masters using\ntheir old certificates.\n\nThe playbooks for redeploying certificates cannot redeploy custom registry or\nrouter certificates, so to address this issue, you can manually redeploy the\nregistry and router certificates.\n\nTo redeploy registry certificates manually, you must add new registry\ncertificates to a secret named registry-certificates, then redeploy the\nregistry:\n\nThen, run the following to remove the environment variables:\n\nWhen routers are initially deployed, an annotation is added to the router\u2019s\nservice that automatically creates a\nservice serving certificate secret.\n\nTo redeploy router certificates manually, that service serving certificate can\nbe triggered to be recreated by deleting the secret, removing and re-adding\nannotations to the router service, then redeploying the router:\n\nThen, run the following to remove the environment variables:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/syncing_groups_with_ldap.html", "title": "Atomic Registry Latest | Installation and Configuration | Syncing Groups With LDAP", "content": "\nAs an Atomic Registry administrator, you can use groups to manage users, change\ntheir permissions, and enhance collaboration. Your organization may have already\ncreated user groups and stored them in an LDAP server. Atomic Registry can sync\nthose LDAP records with internal Atomic Registry records, enabling you to manage\nyour groups in one place. Atomic Registry currently supports group sync with\nLDAP servers using three common schemas for defining group membership: RFC 2307,\nActive Directory, and augmented Active Directory.\n\nYou must have\ncluster-admin\nprivileges to sync groups.\n\nBefore you can run LDAP sync, you need a sync\nconfiguration file. This file contains LDAP client configuration details:\n\nA sync configuration file can also contain an administrator-defined list of name\nmappings that maps Atomic Registry Group names to groups in your LDAP server.\n\nSync configurations consist of LDAP query definitions for the entries that are\nrequired for synchronization. The specific definition of an LDAP query depends\non the schema used to store membership information in the LDAP server.\n\nbase\n\nOnly consider the object specified by the base DN given for the query.\n\none\n\nConsider all of the objects on the same level in the tree as the base DN for\nthe query.\n\nsub\n\nConsider the entire subtree rooted at the base DN given for the query.\n\nnever\n\nNever dereference any aliases found in the LDAP tree.\n\nsearch\n\nOnly dereference aliases found while searching.\n\nbase\n\nOnly dereference aliases while finding the base object.\n\nalways\n\nAlways dereference all aliases found in the LDAP tree.\n\nA user-defined name mapping explicitly maps the names of Atomic Registry Groups to\nunique identifiers that find groups on your LDAP server. The mapping uses normal\nYAML syntax. A user-defined mapping can contain an entry for every group in your\nLDAP server or only a subset of those groups. If there are groups on the LDAP\nserver that do not have a user-defined name mapping, the default behavior during\nsync is to use the attribute specified as the Group\u2019s name.\n\nOnce you have created a sync configuration file,\nthen sync can begin. Atomic Registry allows administrators to perform a number of\ndifferent sync types with the same server.\n\nBy default, all group synchronization or pruning operations are dry-run, so you\nmust set the --confirm flag on the oadm groups sync command in order to make\nchanges to Atomic Registry group records.\n\nTo sync all groups from the LDAP server with Atomic Registry:\n\nTo sync all Groups already in Atomic Registry that correspond to groups in the\nLDAP server specified in the configuration file:\n\nTo sync a subset of LDAP groups with Atomic Registry, you can use whitelist files,\nblacklist files, or both:\n\nAny combination of blacklist files, whitelist files, or whitelist literals will\nwork; whitelist literals can be included directly in the command itself. This\napplies to groups found on LDAP servers, as well as Groups already present in\nAtomic Registry. Your files must contain one unique group identifier per line.\n\nAn administrator can also choose to remove groups from Atomic Registry records\nif the records on the LDAP server that created them are no longer present. The\nprune job will accept the same sync configuration file and white- or black-lists\nas used for the sync job.\n\nFor example, if groups had previously been synchronized from LDAP using some\nconfig.yaml file, and some of those groups no longer existed on the LDAP\nserver, the following command would determine which Groups in Atomic Registry\ncorresponded to the deleted groups in LDAP and then remove them from\nAtomic Registry:\n\nThis section contains examples for the RFC 2307,\nActive Directory, and\naugmented Active Directory schemas.\nAll of the following examples synchronize a group named admins that has two\nmembers: Jane and Jim. Each example explains:\n\nThese examples assume that all users are direct members of their respective\ngroups. Specifically, no groups have other groups as members. See\nNested Membership Sync Example for information on\nhow to sync nested groups.\n\nIn the RFC 2307 schema, both users (Jane and Jim) and groups exist on the LDAP\nserver as first-class entries, and group membership is stored in attributes on\nthe group. The following snippet of ldif defines the users and group for this\nschema:\n\nTo sync this group, you must first create the configuration file. The\nRFC 2307 schema requires you to provide an LDAP query definition for both user\nand group entries, as well as the attributes with which to represent them in the\ninternal Atomic Registry records.\n\nFor clarity, the Group you create in Atomic Registry should use attributes other\nthan the distinguished name whenever possible for user- or administrator-facing\nfields. For example, identify the users of a Group by their e-mail, and use the\nname of the group as the common name. The following configuration file creates\nthese relationships:\n\nIf using user-defined name mappings, your\nconfiguration file will differ.\n\nTo run sync with the rfc2307_config.yaml file:\n\nAtomic Registry creates the following Group record as a result of the above sync\noperation:\n\nWhen syncing groups with user-defined name mappings, the configuration file\nchanges to contain these mappings as shown below.\n\nTo run sync with the rfc2307_config_user_defined.yaml file:\n\nAtomic Registry creates the following Group record as a result of the above sync\noperation:\n\nBy default, if the groups being synced contain members whose entries are outside\nof the scope defined in the member query, the group sync fails with an error:\n\nThis often indicates a mis-configured baseDN in the usersQuery field.\nHowever, in cases where the baseDN intentionally does not contain some of the\nmembers of the group, setting tolerateMemberOutOfScopeErrors: true allows\nthe group sync to continue. Out of scope members will be ignored.\n\nSimilarly, when the group sync process fails to locate a member for a group, it\nfails outright with errors:\n\nThis often indicates a mis-configured usersQuery field. However, in cases\nwhere the group contains member entries that are known to be missing, setting\ntolerateMemberNotFoundErrors: true allows the group sync to continue.\nProblematic members will be ignored.\n\nEnabling error tolerances for the LDAP group sync causes the sync process to\nignore problematic member entries. If the LDAP group sync is not configured\ncorrectly, this could result in synced Atomic Registry groups missing members.\n\nIn order to tolerate the errors in the above example, the following additions to\nyour sync configuration file must be made:\n\nTo run sync with the rfc2307_config_tolerating.yaml file:\n\nAtomic Registry creates the following group record as a result of the above sync\noperation:\n\nIn the Active Directory schema, both users (Jane and Jim) exist in the LDAP\nserver as first-class entries, and group membership is stored in attributes on\nthe user. The following snippet of ldif defines the users and group for this\nschema:\n\nTo sync this group, you must first create the configuration file. The\nActive Directory schema requires you to provide an LDAP query definition for\nuser entries, as well as the attributes to represent them with in the internal\nAtomic Registry Group records.\n\nFor clarity, the Group you create in Atomic Registry should use attributes other\nthan the distinguished name whenever possible for user- or administrator-facing\nfields. For example, identify the users of a Group by their e-mail, but define\nthe name of the Group by the name of the group on the LDAP server.\nThe following configuration file creates these relationships:\n\nTo run sync with the active_directory_config.yaml file:\n\nAtomic Registry creates the following Group record as a result of the above sync\noperation:\n\nIn the augmented Active Directory schema, both users (Jane and Jim) and groups\nexist in the LDAP server as first-class entries, and group membership is stored\nin attributes on the user. The following snippet of ldif defines the users and\ngroup for this schema:\n\nTo sync this group, you must first create the configuration file. The\naugmented Active Directory schema requires you to provide an LDAP query\ndefinition for both user entries and group entries, as well as the attributes\nwith which to represent them in the internal Atomic Registry Group records.\n\nFor clarity, the Group you create in Atomic Registry should use attributes other\nthan the distinguished name whenever possible for user- or administrator-facing\nfields. For example, identify the users of a Group by their e-mail,\nand use the name of the Group as the common name. The following configuration\nfile creates these relationships.\n\nTo run sync with the augmented_active_directory_config.yaml file:\n\nAtomic Registry creates the following Group record as a result of the above sync\noperation:\n\nGroups in Atomic Registry do not nest. The LDAP server must flatten group\nmembership before the data can be consumed. Microsoft\u2019s Active Directory Server\nsupports this feature via the\nLDAP_MATCHING_RULE_IN_CHAIN\nrule, which has the OID 1.2.840.113556.1.4.1941. Furthermore, only explicitly\nwhitelisted\ngroups can be synced when using this matching rule.\n\nThis section has an example for the augmented Active Directory schema, which\nsynchronizes a group named admins that has one user Jane and one group\notheradmins as members. The otheradmins group has one user member: Jim.\nThis example explains:\n\nIn the augmented Active Directory schema, both users (Jane and Jim) and\ngroups exist in the LDAP server as first-class entries, and group membership is\nstored in attributes on the user or the group. The following snippet of ldif\ndefines the users and groups for this schema:\n\nTo sync nested groups with Active Directory, you must provide an LDAP query\ndefinition for both user entries and group entries, as well as the attributes\nwith which to represent them in the internal Atomic Registry Group records.\nFurthermore, certain changes are required in this configuration:\n\nFor clarity, the Group you create in Atomic Registry should use attributes other\nthan the distinguished name whenever possible for user- or administrator-facing\nfields. For example, identify the users of a Group by their e-mail, and use the\nname of the Group as the common name. The following configuration file creates\nthese relationships:\n\nTo run sync with the augmented_active_directory_config_nested.yaml file:\n\nYou must explicitly\nwhitelist\nthe cn=admins,ou=groups,dc=example,dc=com group.\n\nAtomic Registry creates the following Group record as a result of the above sync\noperation:\n\nThe object specification for the configuration file is below.  Note that the different schema\nobjects have different fields.  For example, v1.ActiveDirectoryConfig has no groupsQuery\nfield whereas v1.RFC2307Config and v1.AugmentedActiveDirectoryConfig both do.\n\nLDAPSyncConfig holds the necessary configuration options to define an LDAP\ngroup sync.\n\nStringSource allows specifying a string inline, or externally via environment\nvariable or file. When it contains only a string value, it marshals to a simple\nJSON string.\n\nLDAPQuery holds the options necessary to build an LDAP query.\n\nRFC2307Config holds the necessary configuration options to define how an LDAP\ngroup sync interacts with an LDAP server using the RFC2307 schema.\n\nActiveDirectoryConfig holds the necessary configuration options to define how\nan LDAP group sync interacts with an LDAP server using the Active Directory\nschema.\n\nAugmentedActiveDirectoryConfig holds the necessary configuration options to\ndefine how an LDAP group sync interacts with an LDAP server using the augmented\nActive Directory schema.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/web_console_customization.html", "title": "Atomic Registry Latest | Installation and Configuration | Customizing the Web Console", "content": "\nCatalog categories organize the display of builder images and templates on the\nAdd to Project page on the Atomic Registry web console. A builder image or\ntemplate is grouped in a category if it includes a tag with the same name of the\ncategory or category alias. Categories only display if one or more builder\nimages or templates with matching tags are present in the catalog.\n\nSignificant customizations to the catalog categories may affect the user\nexperience and should be done with careful consideration. You may need to update\nthis customization in future upgrades if you modify existing category items.\n\nCreate from URL\nonly works with image streams or templates from namespaces that have been\nexplicitly specified in OPENSHIFT_CONSTANTS.CREATE_FROM_URL_WHITELIST.  To add\nnamespaces to the whitelist, follow these steps:\n\nopenshift is included in the whitelist by default. Do not remove it.\n\nIf you enabled wildcard routes for a router, you can also enable wildcard\nroutes in the web console. This lets users enter hostnames starting with an\nasterisk like *.example.com when creating a route. To enable wildcard routes:\n\nLearn\nhow to configure HAProxy routers to allow wildcard routes.\n\nSometimes features are available in Technology Preview. By default, these\nfeatures are disabled and hidden in the web console.\n\nCurrently, there are no web console features in Technology Preview.\n\nTo enable a Technology Preview feature:\n\nYou can serve other files from the Asset Server as well. For example, you might\nwant to make the CLI executable available for download from the web console or\nadd images to use in a custom stylesheet.\n\nAdd the directory with the files you want using the following configuration\noption:\n\nThe files under the /path/to/my_images directory will be available under the\nURL /<context>/extensions/images in the web console.\n\nTo reference these files from a stylesheet, you should generally use a relative\npath. For example:\n\nThe web console has a special mode for supporting certain static web\napplications that use the HTML5 history API:\n\nSetting html5Mode to true enables two behaviors:\n\nThis is needed for JavaScript frameworks such as AngularJS that require base\nto be set in index.html.\n\nYou can also change the login page, and the login provider selection page for\nthe web console. Run the following commands to create templates you can modify:\n\nEdit the file to change the styles or add content, but be careful not to remove\nany required parameters inside the curly brackets.\n\nTo use your custom login page or provider selection page, set the following\noptions in the master configuration file:\n\nRelative paths are resolved relative to the master configuration file. You must\nrestart the server after changing this configuration.\n\nWhen there are multiple login providers configured or when the\nalwaysShowProviderSelection\noption in the master-config.yaml file is set to true, each time a user\u2019s\ntoken to Atomic Registry expires, the user is presented with this custom page\nbefore they can proceed with other tasks.\n\nCustom login pages can be used to create Terms of Service information. They can\nalso be helpful if you use a third-party login provider, like GitHub or Google,\nto show users a branded page that they trust and expect before being redirected\nto the authentication provider.\n\nWhen errors occur during authentication, you can change the page shown.\n\nYou can use the Error and ErrorCode variables in the template. To use\nyour custom error page, set the following option in the master configuration\nfile:\n\nRelative paths are resolved relative to the master configuration file.\n\nYou can change the location a console user is sent to when logging out of\nthe console by modifying the logoutURL parameter in the\n/etc/origin/master/master-config.yaml file:\n\nThis can be useful when authenticating with\nRequest\nHeader and OAuth or\nOpenID identity\nproviders, which require visiting an external URL to destroy single sign-on\nsessions.\n\nDuring\nadvanced installations,\nmany modifications to the web console can be configured using\nthe following parameters, which are configurable in the inventory file:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/registry_quickstart/administrators/index.html", "title": "Atomic Registry Latest | Quickstart | Administrators | Overview", "content": "\nThe Atomic Registry quickstart installation has been provided to get a system\nrunning as quickly as possible. Several choices have been made to\nenable this installation experience.\n\nWhile Atomic Registry may run on a non-Red Hat-based distributions, most testing\nhas been performed on Red Hat Enterprise Linux, Centos and Fedora, including Atomic Host.\n\nThe installation will use the system hostname if not provided. If the hostname\ndoes not resolve outside of the system via DNS or the external hostname is\ndifferent, use a hostname or external IP address that can be addressed from\noutside the system during the install and run steps below.\n\nThe very first time you start this service, it may take a few seconds to pull\ndown the atomic-registry-master, atomic-registry-console and\natomic-registry container images from Docker Hub, and you will not see the\nimages in docker ps yet until that is complete. Use systemctl status\natomic-registry\\* to watch the progress.\n\nUntil the registry is secured with TLS certificates, configure any docker daemon\nclients by editing file /etc/sysconfig/docker to use insecure registry option and\nrestart the docker service.\n\nHere are some topics you may follow to explore Atomic Registry:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/registry_quickstart/administrators/system_configuration.html", "title": "Atomic Registry Latest | Quickstart | Administrators | System Configuration", "content": "\nAtomic Registry deploys container services managed by systemd.\n\nEach container service sends logs to the system journal because they are managed\nby systemd. For example, here we follow the log from the atomic-registry-master\nservice unit.\n\nChanges to configuration files require a service restart. Under systemd, use systemctl\nto manage container lifecycle. Here we restart the atomic-registry-master service.\n\nThe three services may be restarted\nindependently.\n\nAuthentication configuration, project defaults, datastore, API certificates and\nother settings are defined globally in a configuration file mounted on the host\nat /etc/atomic-registry/master/master-config.yaml. Whenever this file is\nupdated restart the atomic-registry-master service.\n\nGlobal configuration items managed by this configuration file include:\n\nMaster log level is determined using the service unit configuration file mounted\non the host at /etc/sysconfig/atomic-registry-master.\n\nThe registry console is a stateless service that has limited configuration using\nenvironment variables defined in a file mounted on the host at\n/etc/sysconfig/atomic-registry-console.\n\nConfiguration changes require a service restart.\n\nThe docker distribution registry configuration is managed by a file mounted on\nthe host at /etc/atomic-registry/registry/config.yml. See\nregistry configuration reference for complete settings.\n\nEnvironment variables for this service are managed by a file mounted on the host\nat /etc/sysconfig/atomic-registry.\n\nConfiguration changes require a service restart.\n\nThere are many configuration options available in the upstream\ndocker distribution library. Not\nall configuration options\nare supported or enabled. Use this section as a reference when\noverriding the registry\nconfiguration.\n\nUpstream configuration options in this file may also be overridden using\nenvironment variables. However, the\nmiddleware section may\nnot be overridden using environment variables.\nLearn\nhow to override specific configuration options.\n\nUpstream options are supported.\n\nExample:\n\nMail hooks are not supported.\n\nThis section lists the supported registry storage drivers.\n\nThe following list includes storage drivers that need to be configured in the\nregistry\u2019s configuration file:\n\nGeneral registry storage configuration options are supported.\n\nThe following storage options need to be configured through the filesystem driver:\n\nFor more information on supported persistent storage drivers, see Configuring Persistent Storage and Persistent Storage Examples.\n\nAuth options should not be altered. The openshift extension is the only\nsupported option.\n\nThe repository middleware extension allows to configure Atomic Registry\nmiddleware responsible for interaction with Atomic Registry and image proxying.\n\nThe CloudFront\nmiddleware extension can be added to support AWS, CloudFront CDN storage\nprovider. CloudFront middleware speeds up distribution of image content\ninternationally. The blobs are distributed to several edge locations around the\nworld. The client is always directed to the edge with the lowest latency.\n\nThe CloudFront\nmiddleware extension can be only used with\nS3 storage.\nIt is utilized only during blob serving. Therefore, only blob downloads can be\nspeeded up, not uploads.\n\nThe following is an example of minimal configuration of S3 storage driver with a\nCloudFront middleware:\n\nThe middleware section cannot be overridden using environment variables.\nThere are a few exceptions, however. For example:\n\nIf enabled, the registry will attempt to fetch requested blob from a remote\nregistry unless the blob exists locally. The remote candidates are calculated\nfrom DockerImage entries stored in status of the\nimage\nstream, a client pulls from. All the unique remote registry references in\nsuch entries will be tried in turn until the blob is found. The blob, served\nthis way, will not be stored in the registry.\n\nThis feature is on by default. However, it can be disabled using a\nconfiguration option.\n\nBy default, all the remote blobs served this way are stored locally for\nsubsequent faster access unless mirrorpullthrough is disabled. The downside\nof this mirroring feature is an increased storage usage.\n\nThe mirroring starts when a client tries to fetch at least a single byte of the\nblob. To pre-fetch a particular image into integrated registry before it is\nactually needed, you can run the following command:\n\nEach image has a manifest describing its blobs, instructions for running it\nand additional metadata. The manifest is versioned which have different\nstructure and fields as it evolves over time. The same image can be represented\nby multiple manifest versions. Each version will have different digest though.\n\nThe registry currently supports\nmanifest\nv2 schema 1 (schema1) and\nmanifest\nv2 schema 2 (schema2). The former is being obsoleted but will be supported\nfor an extended amount of time.\n\nYou should be wary of compatibility issues with various Docker clients:\n\nThe registry, storing an image with schema1 will always return it unchanged\nto the client. Schema2 will be transferred unchanged only to newer Docker\nclient. For the older one, it will be converted on-the-fly to schema1.\n\nThis has significant consequences. For example an image pushed to the registry\nby a newer Docker client cannot be pulled by the older Docker by its digest.\nThat\u2019s because the stored image\u2019s manifest is of schema2 and its digest can\nbe used to pull only this version of manifest.\n\nFor this reason, the registry is configured by default not to store schema2.\nThis ensures that any docker client will be able to pull from the registry any\nimage pushed there regardless of client\u2019s version.\n\nOnce you\u2019re confident that all the registry clients support schema2, you\u2019ll\nbe safe to enable its support in the registry. See the\nmiddleware\nconfiguration reference above for particular option.\n\nThis section reviews the configuration of global settings related to\nAtomic Registry specific features. In the future, openshift-related settings\nin the middleware\nsection will be obsolete. Currently, this section allows you to configure only\nfor metrics collection.\n\nRefer to accessing metrics for\nusage information.\n\nReporting is unsupported.\n\nUpstream options are\nsupported. Learn how to alter these settings via\nenvironment variables. Only the tls section should be altered. For example:\n\nUpstream\noptions are supported. The REST API Reference\nprovides more comprehensive integration options.\n\nExample:\n\nRedis is not supported.\n\nUpstream options\nare supported. The registry deployment configuration provides an integrated\nhealth check at /healthz.\n\nProxy configuration should not be enabled. This functionality is provided by\nthe Atomic Registry\nrepository middleware extension, pullthrough: true.\n\nThe service endpoints are secured by certificates that are mounted on the host at\n/etc/atomic-registry/. There are separate directories for master and registry certificates.\n\nThe installer generates self-signed certificates during installation. See\ncertificate customization\nfor customizing the API master certificates.\n\nHere we create a self-signed certificate so docker clients can connect using\nTLS. While other tools like openssl may be used to create certificates, the\nmaster API provides a tool that may also be used.\n\nThe oadm ca create-server-cert command generates a certificate that is valid\nfor two years. This can be altered with the --expire-days option, but for\nsecurity reasons, it is recommended to not make it greater than this value.\n\nIf you secure the registry using a self-signed certificate key pair you may want\nto make the public CA certificate available to users so they don\u2019t have to put\ndocker into insecure mode. The registry master service is able to serve\narbitrary files.\n\nRegistry users may then be instructed to save this cert into their docker client\nand restart their docker daemon.\n\nBy default the session token used for docker login is 24 hours. This\nrequires users to run docker login every day. This value may be extended in\nthe master configuration file /etc/atomic-registry/master/master-config.yaml.\nSee below for using\nlong-lived service account tokens.\n\nIn this example the session token expiration is extended to 30 days.\n\nRestart the atomic-registry-master service to update the running configuration.\n\nTypically long-lived, token-based authentication is desired. As an alternative\nto using user session tokens that expire, users may use\nservice account tokens to\nauthenticate with docker. This is particularly useful when integrating automation.\nSee the\nquickstart developer guide\nfor instructions.\n\nThe data that should be persisted is the configuration, image data and the\nmaster database. The required directories are mounted from the container onto\nthe host. See Service Components table for\nspecific paths.\n\nAtomic Registry provides full control using a command line interface (CLI). To\naccess the CLI directly on the host you may enter the atomic-registry-master\ncontainer.\n\nSee CLI reference for how to download the\nremote CLI client and using the CLI.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/registry_quickstart/administrators/uninstall.html", "title": "Atomic Registry Latest | Quickstart | Administrators | Uninstall", "content": "\nTo uninstall the registry quickstart deployment execute the following command.\nThis will retain the etcd datastore and image directories in /var/lib/atomic-registry.\n\nTo uninstall the registry quickstart deployment and remove all configuration and\nimage data in /var/lib/atomic-registry execute the following command.\n\nThis command will remove all configuration files for the registry.\n\nWhen complete you may need to manually remove stopped containers and unused\nimages.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/registry_quickstart/developers.html", "title": "Atomic Registry Latest | Quickstart | Developers", "content": "\nThere are several configuration options for Atomic Registry. Your administrator\nmay have disabled project self-provisioning. Sharing images may also be disabled.\nIn this configuration you may only be able to pull existing images.\n\nImages are pushed and pulled from the docker command line. The docker command\nline reference is found on the bottom of the web interface overview page.\n\nYour username and password credentials are not valid when logging in with Docker\nCLI. Use the token for the password argument.\n\nAtomic Registry uses a session token for the Docker login command. With the\ndefault configuration this token will expire within 24 hours. Contact the\nAtomic Registry administrator for extending the session timeout.\n\nIf a long-lived Docker login is desired use a service account token. This may be\nused in place of the Docker login password value:\n\nThroughout this document sudo is prepended to example docker commands.\nDepending on your environment sudo may not be required. This impacts where the\nDocker credentials file is stored. With sudo, Docker login credentials are\nstored in /root/.docker/config.json. Without sudo, Docker login credentials are stored in\n~/.docker/config.json, which will be a different path if not logged in as\nroot user. This would cause docker commands to fail authentication if sudo\nis used inconsistently.\n\nSee the Atomic Registry User Guide for these topics:\n\nFor long-lived, token-based authentication, users may create\nservice\naccount tokens to authenticate with Docker. This is particularly useful when\nintegrating automation. Service accounts must be configured using the CLI. See\nGetting\nStarted with the CLI.\n\nService accounts may be deleted, which disables further authentication attempts.\nFor example, as soon as the service account is deleted, docker push will no longer\nsucceed if logged in with this service account token.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/registry_quickstart/index.html", "title": "Atomic Registry Latest | Quickstart | Overview", "content": "\nTo get started with Atomic Registry, find the appropriate topic based on your role:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/rest_api/index.html", "title": "Atomic Registry Latest | REST API Reference | Overview", "content": "\n\u00a0\nThe Atomic Registry distribution of Kubernetes includes the\nKubernetes v1 REST\nAPI and the OpenShift\nv1 REST API. These are RESTful APIs accessible via HTTP(s) on the\nAtomic Registry master servers.\n\nThese REST APIs can be used to manage end-user applications, the cluster, and\nthe users of the cluster.\n\nAPI calls must be authenticated with an access token or X.509 certificate. See\nAuthentication\nin the Architecture documentation for an overview.\n\nThis section highlights the token authentication method. With token\nauthentication, a bearer token must be passed in as an\nHTTP\nAuthorization header. There are two types of access tokens: session and service\naccount.\n\nA session token is short-lived, expiring within 24 hours by default. It\nrepresents a\nuser.\nAfter logging in, the session token may be obtained with the oc whoami\ncommand:\n\nService account tokens are long-lived tokens. They are\nJSON Web Token (JWT) formatted tokens\nand are much longer strings than session tokens. See\nUsing\na Service Account\u2019s Credentials Externally for steps on using these tokens to\nauthenticate using the CLI.\n\nA service account token may be obtained with these commands:\n\nThe token value may be used as an in an authorization header to\nauthenticate API calls, the\nCLI\nor in the docker login command. Service accounts may\nbe created and deleted as needed with the appropriate role(s) assigned. See\nAuthorization\nin the Architecture documentation for a deeper discussion on roles.\n\nThese examples provide a quick reference for making successful REST API calls.\nThey use insecure methods. In these examples, a simple GET call is made to\nlist available resources.\n\nThe Atomic Registry integrated Docker registry must be authenticated using\neither a user session or\nservice account token. The value of the\ntoken must be used as the value for the --password argument. The user and\nemail argument values are ignored:\n\nThe API is designed to work via the\nwebsocket protocol. API requests may\ntake the form of \"one-shot\" calls to list resources or by passing in query\nparameter watch=true. When watching an endpoint, changes to the system may be\nobserved through an open endpoint. Using callbacks, dynamic systems may be\ndeveloped that integrate with the API.\n\nFor more information and examples, see the Mozilla Developer Network page on\nWriting\nWebSocket client applications.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/rest_api/kubernetes_v1.html", "title": "Atomic Registry Latest | REST API Reference | Kubernetes v1", "content": "\nThe Kubernetes API allows you to run containerized applications, bind persistent storage, link those applications through service discovery, and manage the cluster infrastructure.\n\nVersion: v1\n\nHost: 127.0.0.1:8443\nBasePath: /\nSchemes: HTTPS\n\nAPIResource specifies the name of a resource and whether it is namespaced.\n\nAPIResourceList is a list of APIResource, it is used to expose the name of the resources supported in a specific group and version, and if the resource is namespaced.\n\nA label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.\n\nA label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.\n\nListMeta describes metadata that synthetic resources must have, including lists and various status objects. A resource may have only one of {ObjectMeta, ListMeta}.\n\nPatch is provided to give a concrete name and type to the Kubernetes PATCH request body.\n\nStatus is a return value for calls that don\u2019t return other objects.\n\nStatusCause provides more information about an api.Status failure, including cases when multiple errors are encountered.\n\nStatusDetails is a set of additional properties that MAY be set by the server to provide additional information about a response. The Reason field of a Status object defines what attributes will be set. Clients must ignore fields that do not match the defined type of each attribute, and should assume that any attribute may be empty, invalid, or under defined.\n\nRepresents a Persistent Disk resource in AWS.\n\nAn AWS EBS disk must exist before mounting to a container. The disk must also be in the same AWS zone as the kubelet. An AWS EBS disk can only be mounted as read/write once. AWS EBS volumes support ownership management and SELinux relabeling.\n\nAttachedVolume describes a volume attached to a node\n\nAzureDisk represents an Azure Data Disk mount on the host and bind mount to the pod.\n\nAzureFile represents an Azure File Service mount on the host and bind mount to the pod.\n\nBinding ties one object to another. For example, a pod is bound to a node by a scheduler.\n\nAdds and removes POSIX capabilities from running containers.\n\nRepresents a Ceph Filesystem mount that lasts the lifetime of a pod Cephfs volumes do not support ownership management or SELinux relabeling.\n\nRepresents a cinder volume resource in Openstack. A Cinder volume must exist before mounting to a container. The volume must also be in the same region as the kubelet. Cinder volumes support ownership management and SELinux relabeling.\n\nInformation about the condition of a component.\n\nComponentStatus (and ComponentStatusList) holds the cluster validation info.\n\nStatus of all the conditions for the component as a list of ComponentStatus objects.\n\nConfigMap holds configuration data for pods to consume.\n\nSelects a key from a ConfigMap.\n\nConfigMapList is a resource containing a list of ConfigMap objects.\n\nAdapts a ConfigMap into a volume.\n\nThe contents of the target ConfigMap\u2019s Data field will be presented in a volume as files using the keys in the Data field as the file names, unless the items element is populated with specific mappings of keys to paths. ConfigMap volumes support ownership management and SELinux relabeling.\n\nA single application container that you want to run within a pod.\n\nDescribe a container image\n\nContainerPort represents a network port in a single container.\n\nContainerState holds a possible state of container. Only one of its members may be specified. If none of them is specified, the default one is ContainerStateWaiting.\n\nContainerStateRunning is a running state of a container.\n\nContainerStateTerminated is a terminated state of a container.\n\nContainerStateWaiting is a waiting state of a container.\n\nContainerStatus contains details for the current status of this container.\n\nDaemonEndpoint contains information about a single Daemon endpoint.\n\nDeleteOptions may be provided when deleting an API object\n\nDeprecatedDownwardAPIVolumeFile represents information to create the file containing the pod field This type is deprecated and should be replaced by use of the downwardAPI volume source.\n\nDeprecatedDownwardAPIVolumeSource represents a volume containing downward API info. This type is deprecated and should be replaced by use of the downwardAPI volume source.\n\nDownwardAPIVolumeFile represents information to create the file containing the pod field\n\nDownwardAPIVolumeSource represents a volume containing downward API info. Downward API volumes support ownership management and SELinux relabeling.\n\nRepresents an empty directory for a pod. Empty directory volumes support ownership management and SELinux relabeling.\n\nEndpointAddress is a tuple that describes single IP address.\n\nEndpointPort is a tuple that describes a single port.\n\nEndpointSubset is a group of addresses with a common set of ports. The expanded set of endpoints is the Cartesian product of Addresses x Ports. For example, given:\n  {\n    Addresses: [{\"ip\": \"10.10.1.1\"}, {\"ip\": \"10.10.2.2\"}],\n    Ports:     [{\"name\": \"a\", \"port\": 8675}, {\"name\": \"b\", \"port\": 309}]\n  }\nThe resulting set of endpoints can be viewed as:\n    a: [ 10.10.1.1:8675, 10.10.2.2:8675 ],\n    b: [ 10.10.1.1:309, 10.10.2.2:309 ]\n\nEndpoints is a collection of endpoints that implement the actual service. Example:\n  Name: \"mysvc\",\n  Subsets: [\n    {\n      Addresses: [{\"ip\": \"10.10.1.1\"}, {\"ip\": \"10.10.2.2\"}],\n      Ports: [{\"name\": \"a\", \"port\": 8675}, {\"name\": \"b\", \"port\": 309}]\n    },\n    {\n      Addresses: [{\"ip\": \"10.10.3.3\"}],\n      Ports: [{\"name\": \"a\", \"port\": 93}, {\"name\": \"b\", \"port\": 76}]\n    },\n ]\n\nEndpointsList is a list of endpoints.\n\nEnvVar represents an environment variable present in a Container.\n\nEnvVarSource represents a source for the value of an EnvVar.\n\nEvent is a report of an event somewhere in the cluster.\n\nEventList is a list of events.\n\nEventSource contains information for an event.\n\nExecAction describes a \"run in container\" action.\n\nRepresents a Fibre Channel volume. Fibre Channel volumes can only be mounted as read/write once. Fibre Channel volumes support ownership management and SELinux relabeling.\n\nFSGroupStrategyOptions defines the strategy type and options used to create the strategy.\n\nFlexVolume represents a generic volume resource that is provisioned/attached using an exec based plugin. This is an alpha feature and may change in future.\n\nRepresents a Flocker volume mounted by the Flocker agent. One and only one of datasetName and datasetUUID should be set. Flocker volumes do not support ownership management or SELinux relabeling.\n\nRepresents a Persistent Disk resource in Google Compute Engine.\n\nA GCE PD must exist before mounting to a container. The disk must also be in the same GCE project and zone as the kubelet. A GCE PD can only be mounted as read/write once or read-only many times. GCE PDs support ownership management and SELinux relabeling.\n\nRepresents a volume that is populated with the contents of a git repository. Git repo volumes do not support ownership management. Git repo volumes support SELinux relabeling.\n\nRepresents a Glusterfs mount that lasts the lifetime of a pod. Glusterfs volumes do not support ownership management or SELinux relabeling.\n\nHTTPGetAction describes an action based on HTTP Get requests.\n\nHTTPHeader describes a custom header to be used in HTTP probes\n\nHandler defines a specific action that should be taken\n\nRepresents a host path mapped into a pod. Host path volumes do not support ownership management or SELinux relabeling.\n\nIDRange provides a min/max of an allowed range of IDs.\n\nRepresents an ISCSI disk. ISCSI volumes can only be mounted as read/write once. ISCSI volumes support ownership management and SELinux relabeling.\n\nMaps a string key to a path within a volume.\n\nLifecycle describes actions that the management system should take in response to container lifecycle events. For the PostStart and PreStop lifecycle handlers, management of the container blocks until the action is complete, unless the container process fails, in which case the handler is aborted.\n\nLimitRange sets resource usage limits for each kind of resource in a Namespace.\n\nLimitRangeItem defines a min/max usage limit for any resource that matches on kind.\n\nLimitRangeList is a list of LimitRange items.\n\nLimitRangeSpec defines a min/max usage limit for resources that match on kind.\n\nLoadBalancerIngress represents the status of a load-balancer ingress point: traffic intended for the service should be sent to an ingress point.\n\nLoadBalancerStatus represents the status of a load-balancer.\n\nLocalObjectReference contains enough information to let you locate the referenced object inside the same namespace.\n\nRepresents an NFS mount that lasts the lifetime of a pod. NFS volumes do not support ownership management or SELinux relabeling.\n\nNamespace provides a scope for Names. Use of multiple namespaces is optional.\n\nNamespaceList is a list of Namespaces.\n\nNamespaceSpec describes the attributes on a Namespace.\n\nNamespaceStatus is information about the current status of a Namespace.\n\nNode is a worker node in Kubernetes. Each node will have a unique identifier in the cache (i.e. in etcd).\n\nNodeAddress contains information for the node\u2019s address.\n\nNodeCondition contains condition information for a node.\n\nNodeDaemonEndpoints lists ports opened by daemons running on the Node.\n\nNodeList is the whole list of all Nodes which have been registered with master.\n\nNodeSpec describes the attributes that a node is created with.\n\nNodeStatus is information about the current status of a node.\n\nNodeSystemInfo is a set of ids/uuids to uniquely identify the node.\n\nObjectFieldSelector selects an APIVersioned field of an object.\n\nObjectMeta is metadata that all persisted resources must have, which includes all objects users must create.\n\nObjectReference contains enough information to let you inspect or modify the referred object.\n\nOwnerReference contains enough information to let you identify an owning object. Currently, an owning object must be in the same namespace, so there is no namespace field.\n\nPersistentVolume (PV) is a storage resource provisioned by an administrator. It is analogous to a node. More info: http://kubernetes.io/docs/user-guide/persistent-volumes\n\nPersistentVolumeClaim is a user\u2019s request for and claim to a persistent volume\n\nPersistentVolumeClaimList is a list of PersistentVolumeClaim items.\n\nPersistentVolumeClaimSpec describes the common attributes of storage devices and allows a Source for provider-specific attributes\n\nPersistentVolumeClaimStatus is the current status of a persistent volume claim.\n\nPersistentVolumeClaimVolumeSource references the user\u2019s PVC in the same namespace. This volume finds the bound PV and mounts that volume for the pod. A PersistentVolumeClaimVolumeSource is, essentially, a wrapper around another type of volume that is owned by someone else (the system).\n\nPersistentVolumeList is a list of PersistentVolume items.\n\nPersistentVolumeSpec is the specification of a persistent volume.\n\nPersistentVolumeStatus is the current status of a persistent volume.\n\nRepresents a Photon Controller persistent disk resource.\n\nPod is a collection of containers that can run on a host. This resource is created by clients and scheduled onto hosts.\n\nPodCondition contains details for the current condition of this pod.\n\nPodList is a list of Pods.\n\nPodSecurityContext holds pod-level security attributes and common container settings. Some fields are also present in container.securityContext.  Field values of container.securityContext take precedence over field values of PodSecurityContext.\n\nPodSpec is a description of a pod.\n\nPodStatus represents information about the status of a pod. Status may trail the actual state of a system.\n\nPodTemplate describes a template for creating copies of a predefined pod.\n\nPodTemplateList is a list of PodTemplates.\n\nPodTemplateSpec describes the data a pod should have when created from a template\n\nPreconditions must be fulfilled before an operation (update, delete, etc.) is carried out.\n\nProbe describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic.\n\nRepresents a Quobyte mount that lasts the lifetime of a pod. Quobyte volumes do not support ownership management or SELinux relabeling.\n\nRepresents a Rados Block Device mount that lasts the lifetime of a pod. RBD volumes support ownership management and SELinux relabeling.\n\nReplicationController represents the configuration of a replication controller.\n\nReplicationControllerCondition describes the state of a replication controller at a certain point.\n\nReplicationControllerList is a collection of replication controllers.\n\nReplicationControllerSpec is the specification of a replication controller.\n\nReplicationControllerStatus represents the current status of a replication controller.\n\nResourceFieldSelector represents container resources (cpu, memory) and their output format\n\nResourceQuota sets aggregate quota restrictions enforced per namespace\n\nResourceQuotaList is a list of ResourceQuota items.\n\nResourceQuotaSpec defines the desired hard limits to enforce for Quota.\n\nResourceQuotaStatus defines the enforced hard limits and observed use.\n\nResourceRequirements describes the compute resource requirements.\n\nRunAsUserStrategyOptions defines the strategy type and any options used to create the strategy.\n\nSELinuxContextStrategyOptions defines the strategy type and any options used to create the strategy.\n\nSELinuxOptions are the labels to be applied to the container\n\nScale represents a scaling request for a resource.\n\nScaleSpec describes the attributes of a scale subresource.\n\nScaleStatus represents the current status of a scale subresource.\n\nSecret holds secret data of a certain type. The total bytes of the values in the Data field must be less than MaxSecretSize bytes.\n\nSecretKeySelector selects a key of a Secret.\n\nSecretList is a list of Secret.\n\nAdapts a Secret into a volume.\n\nThe contents of the target Secret\u2019s Data field will be presented in a volume as files using the keys in the Data field as the file names. Secret volumes support ownership management and SELinux relabeling.\n\nSecurityContext holds security configuration that will be applied to a container. Some fields are present in both SecurityContext and PodSecurityContext.  When both are set, the values in SecurityContext take precedence.\n\nSecurityContextConstraints governs the ability to make requests that affect the SecurityContext that will be applied to a container.\n\nSecurityContextConstraintsList is a list of SecurityContextConstraints objects\n\nService is a named abstraction of software service (for example, mysql) consisting of local port (for example 3306) that the proxy listens on, and the selector that determines which pods will answer requests sent through the proxy.\n\nServiceAccount binds together: * a name, understood by users, and perhaps by peripheral systems, for an identity * a principal that can be authenticated and authorized * a set of secrets\n\nServiceAccountList is a list of ServiceAccount objects\n\nServiceList holds a list of services.\n\nServicePort contains information on service\u2019s port.\n\nServiceSpec describes the attributes that a user creates on a service.\n\nServiceStatus represents the current status of a service.\n\nSupplementalGroupsStrategyOptions defines the strategy type and options used to create the strategy.\n\nTCPSocketAction describes an action based on opening a socket\n\nVolume represents a named volume in a pod that may be accessed by any container in the pod.\n\nVolumeMount describes a mounting of a Volume within a container.\n\nRepresents a vSphere volume resource.\n\nEviction evicts a pod from its node subject to certain policies and safety constraints. This is a subresource of Pod.  A request to cause such an eviction is created by POSTing to \u2026\u200b/pods/<pod name>/evictions.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/rest_api/openshift_v1.html", "title": "Atomic Registry Latest | REST API Reference | OpenShift v1", "content": "\nThe Atomic Registry API exposes operations for managing an enterprise Kubernetes cluster, including security and user management, application deployments, image and source builds, HTTP(s) routing, and project management.\n\nVersion: v1\n\nHost: 127.0.0.1:8443\nBasePath: /\nSchemes: HTTPS\n\nrepresents an object patch, which may be any of: JSON patch (RFC 6902), JSON merge patch (RFC 7396), or the Kubernetes strategic merge patch\n\nthis may be any JSON object with a 'kind' and 'apiVersion' field; and is preserved unmodified by processing\n\nAPIResource specifies the name of a resource and whether it is namespaced.\n\nAPIResourceList is a list of APIResource, it is used to expose the name of the resources supported in a specific group and version, and if the resource is namespaced.\n\nA label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.\n\nA label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.\n\nListMeta describes metadata that synthetic resources must have, including lists and various status objects. A resource may have only one of {ObjectMeta, ListMeta}.\n\nPatch is provided to give a concrete name and type to the Kubernetes PATCH request body.\n\nStatus is a return value for calls that don\u2019t return other objects.\n\nStatusCause provides more information about an api.Status failure, including cases when multiple errors are encountered.\n\nStatusDetails is a set of additional properties that MAY be set by the server to provide additional information about a response. The Reason field of a Status object defines what attributes will be set. Clients must ignore fields that do not match the defined type of each attribute, and should assume that any attribute may be empty, invalid, or under defined.\n\nRepresents a Persistent Disk resource in AWS.\n\nAn AWS EBS disk must exist before mounting to a container. The disk must also be in the same AWS zone as the kubelet. An AWS EBS disk can only be mounted as read/write once. AWS EBS volumes support ownership management and SELinux relabeling.\n\nAppliedClusterResourceQuota mirrors ClusterResourceQuota at a project scope, for projection into a project.  It allows a project-admin to know which ClusterResourceQuotas are applied to his project and their associated usage.\n\nAppliedClusterResourceQuotaList is a collection of AppliedClusterResourceQuotas\n\nAzureDisk represents an Azure Data Disk mount on the host and bind mount to the pod.\n\nAzureFile represents an Azure File Service mount on the host and bind mount to the pod.\n\nBinaryBuildSource describes a binary file to be used for the Docker and Source build strategies, where the file will be extracted and used as the build source.\n\nBuild encapsulates the inputs needed to produce a new deployable image, as well as the status of the execution and a reference to the Pod which executed the build.\n\nBuild configurations define a build process for new Docker images. There are three types of builds possible - a Docker build using a Dockerfile, a Source-to-Image build that uses a specially prepared base image that accepts source code that it can make runnable, and a custom build that can run // arbitrary Docker images as a base and accept the build parameters. Builds run on the cluster and on completion are pushed to the Docker registry specified in the \"output\" section. A build can be triggered via a webhook, when the base image changes, or when a user manually requests a new build be // created.\n\nEach build created by a build configuration is numbered and refers back to its parent configuration. Multiple builds can be triggered at once. Builds that do not have \"output\" set can be used to test code or run a verification build.\n\nBuildConfigList is a collection of BuildConfigs.\n\nBuildConfigSpec describes when and how builds are created\n\nBuildConfigStatus contains current state of the build config object.\n\nBuildList is a collection of Builds.\n\nBuildLog is the (unused) resource associated with the build log redirector\n\nBuildOutput is input to a build strategy and describes the Docker image that the strategy should produce.\n\nA BuildPostCommitSpec holds a build post commit hook specification. The hook executes a command in a temporary container running the build output image, immediately after the last layer of the image is committed and before the image is pushed to a registry. The command is executed with the current working directory ($PWD) set to the image\u2019s WORKDIR.\n\nThe build will be marked as failed if the hook execution fails. It will fail if the script or command return a non-zero exit code, or if there is any other error related to starting the temporary container.\n\nThere are five different ways to configure the hook. As an example, all forms below are equivalent and will execute rake test --verbose.\n\nIt is invalid to provide both Script and Command simultaneously. If none of the fields are specified, the hook is not executed.\n\nBuildRequest is the resource used to pass parameters to build generator\n\nBuildSource is the SCM used for the build.\n\nBuildSpec has the information to represent a build and also additional information about a build\n\nBuildStatus contains the status of a build\n\nBuildStatusOutput contains the status of the built image.\n\nBuildStatusOutputTo describes the status of the built image with regards to image registry to which it was supposed to be pushed.\n\nBuildStrategy contains the details of how to perform a build.\n\nBuildTriggerCause holds information about a triggered build. It is used for displaying build trigger data for each build and build configuration in oc describe. It is also used to describe which triggers led to the most recent update in the build configuration.\n\nBuildTriggerPolicy describes a policy for a single trigger that results in a new Build.\n\nAdds and removes POSIX capabilities from running containers.\n\nRepresents a Ceph Filesystem mount that lasts the lifetime of a pod Cephfs volumes do not support ownership management or SELinux relabeling.\n\nRepresents a cinder volume resource in Openstack. A Cinder volume must exist before mounting to a container. The volume must also be in the same region as the kubelet. Cinder volumes support ownership management and SELinux relabeling.\n\nClusterNetwork describes the cluster network. There is normally only one object of this type, named \"default\", which is created by the SDN network plugin based on the master configuration when the cluster is brought up for the first time.\n\nClusterNetworkList is a collection of ClusterNetworks\n\nClusterPolicy is a object that holds all the ClusterRoles for a particular namespace.  There is at most one ClusterPolicy document per namespace.\n\nClusterPolicyBinding is a object that holds all the ClusterRoleBindings for a particular namespace.  There is one ClusterPolicyBinding document per referenced ClusterPolicy namespace\n\nClusterPolicyBindingList is a collection of ClusterPolicyBindings\n\nClusterPolicyList is a collection of ClusterPolicies\n\nClusterResourceQuota mirrors ResourceQuota at a cluster scope.  This object is easily convertible to synthetic ResourceQuota object to allow quota evaluation re-use.\n\nClusterResourceQuotaList is a collection of ClusterResourceQuotas\n\nClusterResourceQuotaSelector is used to select projects.  At least one of LabelSelector or AnnotationSelector must present.  If only one is present, it is the only selection criteria.  If both are specified, the project must match both restrictions.\n\nClusterResourceQuotaSpec defines the desired quota restrictions\n\nClusterResourceQuotaStatus defines the actual enforced quota and its current usage\n\nClusterRole is a logical grouping of PolicyRules that can be referenced as a unit by ClusterRoleBindings.\n\nClusterRoleBinding references a ClusterRole, but not contain it.  It can reference any ClusterRole in the same namespace or in the global namespace. It adds who information via (Users and Groups) OR Subjects and namespace information by which namespace it exists in. ClusterRoleBindings in a given namespace only have effect in that namespace (excepting the master namespace which has power in all namespaces).\n\nClusterRoleBindingList is a collection of ClusterRoleBindings\n\nClusterRoleList is a collection of ClusterRoles\n\nClusterRoleScopeRestriction describes restrictions on cluster role scopes\n\nSelects a key from a ConfigMap.\n\nAdapts a ConfigMap into a volume.\n\nThe contents of the target ConfigMap\u2019s Data field will be presented in a volume as files using the keys in the Data field as the file names, unless the items element is populated with specific mappings of keys to paths. ConfigMap volumes support ownership management and SELinux relabeling.\n\nA single application container that you want to run within a pod.\n\nContainerPort represents a network port in a single container.\n\nCustomBuildStrategy defines input parameters specific to Custom build.\n\nCustomDeploymentStrategyParams are the input to the Custom deployment strategy.\n\nDeleteOptions may be provided when deleting an API object\n\nDeploymentCause captures information about a particular cause of a deployment.\n\nDeploymentCauseImageTrigger represents details about the cause of a deployment originating from an image change trigger\n\nDeploymentCondition describes the state of a deployment config at a certain point.\n\nDeployment Configs define the template for a pod and manages deploying new images or configuration changes. A single deployment configuration is usually analogous to a single micro-service. Can support many different deployment patterns, including full restart, customizable rolling updates, and  fully custom behaviors, as well as pre- and post- deployment hooks. Each individual deployment is represented as a replication controller.\n\nA deployment is \"triggered\" when its configuration is changed or a tag in an Image Stream is changed. Triggers can be disabled to allow manual control over a deployment. The \"strategy\" determines how the deployment is carried out and may be changed at any time. The latestVersion field is updated when a new deployment is triggered by any means.\n\nDeploymentConfigList is a collection of deployment configs.\n\nDeploymentConfigRollback provides the input to rollback generation.\n\nDeploymentConfigRollbackSpec represents the options for rollback generation.\n\nDeploymentConfigSpec represents the desired state of the deployment.\n\nDeploymentConfigStatus represents the current deployment state.\n\nDeploymentDetails captures information about the causes of a deployment.\n\nDeploymentLog represents the logs for a deployment\n\nDeploymentRequest is a request to a deployment config for a new deployment.\n\nDeploymentStrategy describes how to perform a deployment.\n\nDeploymentTriggerImageChangeParams represents the parameters to the ImageChange trigger.\n\nDeploymentTriggerPolicy describes a policy for a single trigger that results in a new deployment.\n\nDeprecatedDownwardAPIVolumeFile represents information to create the file containing the pod field This type is deprecated and should be replaced by use of the downwardAPI volume source.\n\nDeprecatedDownwardAPIVolumeSource represents a volume containing downward API info. This type is deprecated and should be replaced by use of the downwardAPI volume source.\n\nDockerBuildStrategy defines input parameters specific to Docker build.\n\nDownwardAPIVolumeFile represents information to create the file containing the pod field\n\nDownwardAPIVolumeSource represents a volume containing downward API info. Downward API volumes support ownership management and SELinux relabeling.\n\nEgressNetworkPolicy describes the current egress network policy for a Namespace. When using the 'redhat/openshift-ovs-multitenant' network plugin, traffic from a pod to an IP address outside the cluster will be checked against each EgressNetworkPolicyRule in the pod\u2019s namespace\u2019s EgressNetworkPolicy, in order. If no rule matches (or no EgressNetworkPolicy is present) then the traffic will be allowed by default.\n\nEgressNetworkPolicyList is a collection of EgressNetworkPolicy\n\nEgressNetworkPolicyPeer specifies a target to apply egress network policy to\n\nEgressNetworkPolicyRule contains a single egress network policy rule\n\nEgressNetworkPolicySpec provides a list of policies on outgoing network traffic\n\nRepresents an empty directory for a pod. Empty directory volumes support ownership management and SELinux relabeling.\n\nEnvVar represents an environment variable present in a Container.\n\nEnvVarSource represents a source for the value of an EnvVar.\n\nExecAction describes a \"run in container\" action.\n\nExecNewPodHook is a hook implementation which runs a command in a new pod based on the specified container which is assumed to be part of the deployment template.\n\nRepresents a Fibre Channel volume. Fibre Channel volumes can only be mounted as read/write once. Fibre Channel volumes support ownership management and SELinux relabeling.\n\nFlexVolume represents a generic volume resource that is provisioned/attached using an exec based plugin. This is an alpha feature and may change in future.\n\nRepresents a Flocker volume mounted by the Flocker agent. One and only one of datasetName and datasetUUID should be set. Flocker volumes do not support ownership management or SELinux relabeling.\n\nRepresents a Persistent Disk resource in Google Compute Engine.\n\nA GCE PD must exist before mounting to a container. The disk must also be in the same GCE project and zone as the kubelet. A GCE PD can only be mounted as read/write once or read-only many times. GCE PDs support ownership management and SELinux relabeling.\n\nGenericWebHookCause holds information about a generic WebHook that triggered a build.\n\nGitBuildSource defines the parameters of a Git SCM\n\nGitHubWebHookCause has information about a GitHub webhook that triggered a build.\n\nRepresents a volume that is populated with the contents of a git repository. Git repo volumes do not support ownership management. Git repo volumes support SELinux relabeling.\n\nGitSourceRevision is the commit information from a git source for a build\n\nRepresents a Glusterfs mount that lasts the lifetime of a pod. Glusterfs volumes do not support ownership management or SELinux relabeling.\n\nGroup represents a referenceable set of Users\n\nGroupList is a collection of Groups\n\nGroupRestriction matches a group either by a string match on the group name or a label selector applied to group labels.\n\nHTTPGetAction describes an action based on HTTP Get requests.\n\nHTTPHeader describes a custom header to be used in HTTP probes\n\nHandler defines a specific action that should be taken\n\nRepresents a host path mapped into a pod. Host path volumes do not support ownership management or SELinux relabeling.\n\nHostSubnet describes the container subnet network on a node. The HostSubnet object must have the same name as the Node object it corresponds to.\n\nHostSubnetList is a collection of HostSubnets\n\nRepresents an ISCSI disk. ISCSI volumes can only be mounted as read/write once. ISCSI volumes support ownership management and SELinux relabeling.\n\nIdentity records a successful authentication of a user with an identity provider. The information about the source of authentication is stored on the identity, and the identity is then associated with a single user object. Multiple identities can reference a single user. Information retrieved from the authentication provider is stored in the extra field using a schema determined by the provider.\n\nIdentityList is a collection of Identities\n\nImage is an immutable representation of a Docker image and metadata at a point in time.\n\nImageChangeCause contains information about the image that triggered a build\n\nImageChangeTrigger allows builds to be triggered when an ImageStream changes\n\nImageImportSpec describes a request to import a specific image.\n\nImageImportStatus describes the result of an image import.\n\nImageLabel represents a label applied to the resulting image.\n\nImageLayer represents a single layer of the image. Some images may have multiple layers. Some may have none.\n\nImageList is a list of Image objects.\n\nImageSignature holds a signature of an image. It allows to verify image identity and possibly other claims as long as the signature is trusted. Based on this information it is possible to restrict runnable images to those matching cluster-wide policy. Mandatory fields should be parsed by clients doing image verification. The others are parsed from signature\u2019s content by the server. They serve just an informative purpose.\n\nImageSource is used to describe build source that will be extracted from an image. A reference of type ImageStreamTag, ImageStreamImage or DockerImage may be used. A pull secret can be specified to pull the image from an external registry or override the default service account secret if pulling from the internal registry. A list of paths to copy from the image and their respective destination within the build directory must be specified in the paths array.\n\nImageSourcePath describes a path to be copied from a source image and its destination within the build directory.\n\nImageStream stores a mapping of tags to images, metadata overrides that are applied when images are tagged in a stream, and an optional reference to a Docker image repository on a registry.\n\nImageStreamImage represents an Image that is retrieved by image name from an ImageStream.\n\nThe image stream import resource provides an easy way for a user to find and import Docker images from other Docker registries into the server. Individual images or an entire image repository may be imported, and users may choose to see the results of the import prior to tagging the resulting images into the specified image stream.\n\nThis API is intended for end-user tools that need to see the metadata of the image prior to import (for instance, to generate an application from it). Clients that know the desired image can continue to create spec.tags directly into their image streams.\n\nImageStreamImportSpec defines what images should be imported.\n\nImageStreamImportStatus contains information about the status of an image stream import.\n\nImageStreamList is a list of ImageStream objects.\n\nImageStreamMapping represents a mapping from a single tag to a Docker image as well as the reference to the Docker image stream the image came from.\n\nImageStreamSpec represents options for ImageStreams.\n\nImageStreamStatus contains information about the state of this image stream.\n\nImageStreamTag represents an Image that is retrieved by tag name from an ImageStream.\n\nImageStreamTagList is a list of ImageStreamTag objects.\n\nJenkinsPipelineBuildStrategy holds parameters specific to a Jenkins Pipeline build. This strategy is in tech preview.\n\nMaps a string key to a path within a volume.\n\nLifecycle describes actions that the management system should take in response to container lifecycle events. For the PostStart and PreStop lifecycle handlers, management of the container blocks until the action is complete, unless the container process fails, in which case the handler is aborted.\n\nLifecycleHook defines a specific deployment lifecycle action. Only one type of action may be specified at any time.\n\nLocalObjectReference contains enough information to let you locate the referenced object inside the same namespace.\n\nLocalResourceAccessReview is a means to request a list of which users and groups are authorized to perform the action specified by spec in a particular namespace\n\nLocalSubjectAccessReview is an object for requesting information about whether a user or group can perform an action in a particular namespace\n\nRepresents an NFS mount that lasts the lifetime of a pod. NFS volumes do not support ownership management or SELinux relabeling.\n\nNamedClusterRole relates a name with a cluster role\n\nNamedClusterRoleBinding relates a name with a cluster role binding\n\nNamedRole relates a Role with a name\n\nNamedRoleBinding relates a role binding with a name\n\nNamedTagEventList relates a tag to its image history.\n\nNetNamespace describes a single isolated network. When using the redhat/openshift-ovs-multitenant plugin, every Namespace will have a corresponding NetNamespace object with the same name. (When using redhat/openshift-ovs-subnet, NetNamespaces are not used.)\n\nNetNamespaceList is a collection of NetNamespaces\n\nOAuthAccessToken describes an OAuth access token\n\nOAuthAccessTokenList is a collection of OAuth access tokens\n\nOAuthAuthorizeToken describes an OAuth authorization token\n\nOAuthAuthorizeTokenList is a collection of OAuth authorization tokens\n\nOAuthClient describes an OAuth client\n\nOAuthClientAuthorization describes an authorization created by an OAuth client\n\nOAuthClientAuthorizationList is a collection of OAuth client authorizations\n\nOAuthClientList is a collection of OAuth clients\n\nObjectFieldSelector selects an APIVersioned field of an object.\n\nObjectMeta is metadata that all persisted resources must have, which includes all objects users must create.\n\nObjectReference contains enough information to let you inspect or modify the referred object.\n\nOwnerReference contains enough information to let you identify an owning object. Currently, an owning object must be in the same namespace, so there is no namespace field.\n\nParameter defines a name/value variable that is to be processed during the Template to Config transformation.\n\nPersistentVolumeClaimVolumeSource references the user\u2019s PVC in the same namespace. This volume finds the bound PV and mounts that volume for the pod. A PersistentVolumeClaimVolumeSource is, essentially, a wrapper around another type of volume that is owned by someone else (the system).\n\nRepresents a Photon Controller persistent disk resource.\n\nPodSecurityContext holds pod-level security attributes and common container settings. Some fields are also present in container.securityContext.  Field values of container.securityContext take precedence over field values of PodSecurityContext.\n\nPodSecurityPolicyReview checks which service accounts (not users, since that would be cluster-wide) can create the PodTemplateSpec in question.\n\nPodSecurityPolicyReviewSpec defines specification for PodSecurityPolicyReview\n\nPodSecurityPolicyReviewStatus represents the status of PodSecurityPolicyReview.\n\nPodSecurityPolicySelfSubjectReview checks whether this user/SA tuple can create the PodTemplateSpec\n\nPodSecurityPolicySelfSubjectReviewSpec contains specification for PodSecurityPolicySelfSubjectReview.\n\nPodSecurityPolicySubjectReview checks whether a particular user/SA tuple can create the PodTemplateSpec.\n\nPodSecurityPolicySubjectReviewSpec defines specification for PodSecurityPolicySubjectReview\n\nPodSecurityPolicySubjectReviewStatus contains information/status for PodSecurityPolicySubjectReview.\n\nPodSpec is a description of a pod.\n\nPodTemplateSpec describes the data a pod should have when created from a template\n\nPolicy is a object that holds all the Roles for a particular namespace.  There is at most one Policy document per namespace.\n\nPolicyBinding is a object that holds all the RoleBindings for a particular namespace.  There is one PolicyBinding document per referenced Policy namespace\n\nPolicyBindingList is a collection of PolicyBindings\n\nPolicyList is a collection of Policies\n\nPolicyRule holds information that describes a policy rule, but does not contain information about who the rule applies to or which namespace the rule applies to.\n\nPreconditions must be fulfilled before an operation (update, delete, etc.) is carried out.\n\nProbe describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic.\n\nProjects are the unit of isolation and collaboration in OpenShift. A project has one or more members, a quota on the resources that the project may consume, and the security controls on the resources in the project. Within a project, members may have different roles - project administrators can set membership, editors can create and manage the resources, and viewers can see but not access running containers. In a normal cluster project administrators are not able to alter their quotas - that is restricted to cluster administrators.\n\nListing or watching projects will return only projects the user has the reader role on.\n\nAn OpenShift project is an alternative representation of a Kubernetes namespace. Projects are exposed as editable to end users while namespaces are not. Direct creation of a project is typically restricted to administrators, while end users should use the requestproject resource.\n\nProjectList is a list of Project objects.\n\nProjectRequest is the set of options necessary to fully qualify a project request\n\nProjectSpec describes the attributes on a Project\n\nProjectStatus is information about the current status of a Project\n\nRepresents a Quobyte mount that lasts the lifetime of a pod. Quobyte volumes do not support ownership management or SELinux relabeling.\n\nRepresents a Rados Block Device mount that lasts the lifetime of a pod. RBD volumes support ownership management and SELinux relabeling.\n\nRecreateDeploymentStrategyParams are the input to the Recreate deployment strategy.\n\nRepositoryImportSpec describes a request to import images from a Docker image repository.\n\nRepositoryImportStatus describes the result of an image repository import\n\nResourceAccessReview is a means to request a list of which users and groups are authorized to perform the action specified by spec\n\nResourceFieldSelector represents container resources (cpu, memory) and their output format\n\nResourceQuotaSpec defines the desired hard limits to enforce for Quota.\n\nResourceQuotaStatus defines the enforced hard limits and observed use.\n\nResourceQuotaStatusByNamespace gives status for a particular project\n\nResourceRequirements describes the compute resource requirements.\n\nRole is a logical grouping of PolicyRules that can be referenced as a unit by RoleBindings.\n\nRoleBinding references a Role, but not contain it.  It can reference any Role in the same namespace or in the global namespace. It adds who information via (Users and Groups) OR Subjects and namespace information by which namespace it exists in. RoleBindings in a given namespace only have effect in that namespace (excepting the master namespace which has power in all namespaces).\n\nRoleBindingList is a collection of RoleBindings\n\nRoleBindingRestriction is an object that can be matched against a subject (user, group, or service account) to determine whether rolebindings on that subject are allowed in the namespace to which the RoleBindingRestriction belongs.  If any one of those RoleBindingRestriction objects matches a subject, rolebindings on that subject in the namespace are allowed.\n\nRoleBindingRestrictionList is a collection of RoleBindingRestriction objects.\n\nRoleBindingRestrictionSpec defines a rolebinding restriction.  Exactly one field must be non-nil.\n\nRoleList is a collection of Roles\n\nRollingDeploymentStrategyParams are the input to the Rolling deployment strategy.\n\nA route allows developers to expose services through an HTTP(S) aware load balancing and proxy layer via a public DNS entry. The route may further specify TLS options and a certificate, or specify a public CNAME that the router should also accept for HTTP and HTTPS traffic. An administrator typically configures their router to be visible outside the cluster firewall, and may also add additional security, caching, or traffic controls on the service content. Routers usually talk directly to the service endpoints.\n\nOnce a route is created, the host field may not be changed. Generally, routers use the oldest route with a given host when resolving conflicts.\n\nRouters are subject to additional customization and may support additional controls via the annotations field.\n\nBecause administrators may configure multiple routers, the route status field is used to return information to clients about the names and states of the route under each router. If a client chooses a duplicate name, for instance, the route status conditions are used to indicate the route cannot be chosen.\n\nRouteIngress holds information about the places where a route is exposed.\n\nRouteIngressCondition contains details for the current condition of this route on a particular router.\n\nRouteList is a collection of Routes.\n\nRoutePort defines a port mapping from a router to an endpoint in the service endpoints.\n\nRouteSpec describes the hostname or path the route exposes, any security information, and one or more backends the route points to. Weights on each backend can define the balance of traffic sent to each backend - if all weights are zero the route will be considered to have no backends and return a standard 503 response.\n\nThe tls field is optional and allows specific certificates or behavior for the route. Routers typically configure a default certificate on a wildcard domain to terminate routes without explicit certificates, but custom hostnames usually must choose passthrough (send traffic directly to the backend via the TLS Server-Name- Indication field) or provide a certificate.\n\nRouteStatus provides relevant info about the status of a route, including which routers acknowledge it.\n\nRouteTargetReference specifies the target that resolve into endpoints. Only the 'Service' kind is allowed. Use 'weight' field to emphasize one over others.\n\nSELinuxOptions are the labels to be applied to the container\n\nScopeRestriction describe one restriction on scopes.  Exactly one option must be non-nil.\n\nSecret holds secret data of a certain type. The total bytes of the values in the Data field must be less than MaxSecretSize bytes.\n\nSecretBuildSource describes a secret and its destination directory that will be used only at the build time. The content of the secret referenced here will be copied into the destination directory instead of mounting.\n\nSecretKeySelector selects a key of a Secret.\n\nSecretList is a list of Secret.\n\nSecretSpec specifies a secret to be included in a build pod and its corresponding mount point\n\nAdapts a Secret into a volume.\n\nThe contents of the target Secret\u2019s Data field will be presented in a volume as files using the keys in the Data field as the file names. Secret volumes support ownership management and SELinux relabeling.\n\nSecurityContext holds security configuration that will be applied to a container. Some fields are present in both SecurityContext and PodSecurityContext.  When both are set, the values in SecurityContext take precedence.\n\nSelfSubjectRulesReview is a resource you can create to determine which actions you can perform in a namespace\n\nSelfSubjectRulesReviewSpec adds information about how to conduct the check\n\nServiceAccountPodSecurityPolicyReviewStatus represents ServiceAccount name and related review status\n\nServiceAccountReference specifies a service account and namespace by their names.\n\nServiceAccountRestriction matches a service account by a string match on either the service-account name or the name of the service account\u2019s namespace.\n\nSignatureCondition describes an image signature condition of particular kind at particular probe time.\n\nSignatureIssuer holds information about an issuer of signing certificate or key.\n\nSignatureSubject holds information about a person or entity who created the signature.\n\nSourceBuildStrategy defines input parameters specific to an Source build.\n\nSourceControlUser defines the identity of a user of source control\n\nSourceRevision is the revision or commit information from the source for the build\n\nSubjectAccessReview is an object for requesting information about whether a user or group can perform an action\n\nSubjectRulesReview is a resource you can create to determine which actions another user can perform in a namespace\n\nSubjectRulesReviewSpec adds information about how to conduct the check\n\nSubjectRulesReviewStatus is contains the result of a rules check\n\nTCPSocketAction describes an action based on opening a socket\n\nTLSConfig defines config used to secure a route and provide termination\n\nTagEvent is used by ImageStreamStatus to keep a historical record of images associated with a tag.\n\nTagEventCondition contains condition information for a tag event.\n\nTagImageHook is a request to tag the image in a particular container onto an ImageStreamTag.\n\nTagImportPolicy controls how images related to this tag will be imported.\n\nTagReference specifies optional annotations for images using this tag and an optional reference to an ImageStreamTag, ImageStreamImage, or DockerImage this tag should track.\n\nTagReferencePolicy describes how pull-specs for images in this image stream tag are generated when image change triggers in deployment configs or builds are resolved. This allows the image stream author to control how images are accessed.\n\nTemplate contains the inputs needed to produce a Config.\n\nTemplateList is a list of Template objects.\n\nUpon log in, every user of the system receives a User and Identity resource. Administrators may directly manipulate the attributes of the users for their own tracking, or set groups via the API. The user name is unique and is chosen based on the value provided by the identity provider - if a user already exists with the incoming name, the user name may have a number appended to it depending on the configuration of the system.\n\nUserIdentityMapping maps a user to an identity\n\nUserList is a collection of Users\n\nUserRestriction matches a user either by a string match on the user name, a string match on the name of a group to which the user belongs, or a label selector applied to the user labels.\n\nVolume represents a named volume in a pod that may be accessed by any container in the pod.\n\nVolumeMount describes a mounting of a Volume within a container.\n\nRepresents a vSphere volume resource.\n\nWebHookTrigger is a trigger that gets invoked using a webhook type of post\n\nrepresents a scaling request for a resource.\n\ndescribes the attributes of a scale subresource\n\nrepresents the current status of a scale subresource.\n", "site_name": "https://projectatomic.io/registry"}]