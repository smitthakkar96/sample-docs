[{"topic_url": "/latest/admin_guide/allocating_node_resources.html", "title": "Atomic Registry Latest | Cluster Administration | Allocating Node Resources", "content": "To provide more reliable scheduling and minimize node resource overcommitment,each node can reserve a portion of its resources for use by all underlyingnodecomponents (e.g., kubelet, kube-proxy, Docker) and the remaining systemcomponents (e.g., sshd, NetworkManager) on the host. Once specified, thescheduler has more information about the resources (e.g., memory, CPU) a nodehas allocated for pods.\nResources reserved for node components are based on two node settings:\nYou can set these in the kubeletArguments section of thenodeconfiguration file (the /etc/origin/node/node-config.yaml file by default)using a set of <resource_type>=<resource_quantity> pairs (e.g.,cpu=200m,memory=30G). Add the section if it does not already exist:\nCurrently, the cpu and memory resource types are supported. For cpu,the resource quantity is specified in units of cores (e.g., 200m, 100Ki, 50M).For memory, it is specified in units of bytes (e.g., 200Ki, 100M, 50Gi).\nSee Compute Resources for moredetails.\nIf a flag is not set, it defaults to 0. If none of the flags are set, theallocated resource is set to the node\u2019s capacity as it was before theintroduction of allocatable resources.\nAn allocated amount of a resource is computed based on the following formula:\nThe withholding of Hard-Eviction-Thresholds from allocatable is a change in behavior to improvesystem reliability now that allocatable is enforced for end-user pods at the node level.The experimental-allocatable-ignore-eviction setting is available to preserve legacy behavior,but it will be deprecated in a future release.\nIf [Allocatable] is negative, it is set to 0.\nTo see a node\u2019s current capacity and allocatable resources, you can run:\nStarting with Atomic Registryeach node reports system resources utilized by the container runtime and kubelet.To better aid your ability to configure --system-reserved and --kube-reserved,you can introspect corresponding node\u2019s resource usage using the node summary API,which is accessible at <master>/api/v1/nodes/<node>/proxy/stats/summary.\nFor instance, to access the resources from cluster.node22 node, you can run:\nSee REST API Overview for more details about certificate details.\nThe node is able to limit the total amount of resources that podsmay consume based on the configured allocatable value.  This feature significantlyimproves the reliability of the node by preventing pods from starvingsystem services (for example: container runtime, node agent, etc.) for resources.It is strongly encouraged that administrators reserveresources based on the desired node utilization targetin order to improve node reliability.\nThe node enforces resource constraints using a new cgroup hierarchythat enforces quality of service.  All pods are launched in adedicated cgroup hierarchy separate from system daemons.\nTo configure this ability, the following kubelet arguments are provided.\nOptionally, the node can be made to enforce kube-reserved and system-reserved byspecifying those tokens in the enforce-node-allocatable flag.  If specified, thecorresponding --kube-reserved-cgroup or --system-reserved-cgroup needs to be provided.In future releases, the node and container runtime will be packaged in a common cgroupseparate from system.slice.  Until that time, we do not recommend userschange the default value of enforce-node-allocatable flag.\nAdministrators should treat system daemons similar to Guaranteed pods.  System daemonscan burst within their bounding control groups and this behavior needs to be managedas part of cluster deployments.  Enforcing system-reserved limitscan lead to critical system services being CPU starved or OOM killed on the node. Therecommendation is to enforce system-reserved only if operators have profiled their nodesexhaustively to determine precise estimates and are confident in their ability torecover if any process in that group is OOM killed.\nAs a result, we strongly recommended that users only enforce node allocatable forpods by default, and set aside appropriate reservations for system daemons to maintainoverall node reliability.\nIf a node is under memory pressure, it can impact the entire node and all pods running onit.  If a system daemon is using more than its reserved amount of memory, an OOMevent may occur that can impact the entire node and all pods running on it.  To avoid(or reduce the probability of) system OOMs the nodeprovides Out Of Resource Handling.\nBy reserving some memory via the --eviction-hard flag, the node attempts to evictpods whenever memory availability on the node drops below the absolute value or percentage.If system daemons did not exist on a node, pods are limited to the memorycapacity - eviction-hard. For this reason, resources set aside as a buffer for evictionbefore reaching out of memory conditions are not available for pods.\nHere is an example to illustrate the impact of node allocatable for memory:\nFor this node, the effective node allocatable value is 28.9Gi. If the nodeand system components use up all their reservation, the memory available for pods is 28.9Gi,and kubelet will evict pods when it exceeds this usage.\nIf we enforce node allocatable (28.9Gi) via top level cgroups, then pods can never exceed 28.9Gi.Evictions would not be performed unless system daemons are consuming more than 3.1Gi of memory.\nIf system daemons do not use up all their reservation, with the above example,pods would face memcg OOM kills from their bounding cgroup before node evictions kick in.To better enforce QoS under this situation, the node applies the hard eviction thresholds tothe top-level cgroup for all pods to be Node Allocatable + Eviction Hard Thresholds.\nIf system daemons do not use up all their reservation, the node will evict pods wheneverthey consume more than 28.9Gi of memory. If eviction does not occur in time, a podwill be OOM killed if pods consume 29Gi of memory.\nThe scheduler now uses the value of node.Status.Allocatable instead ofnode.Status.Capacity to decide if a node will become a candidate for podscheduling.\nBy default, the node will report its machine capacity as fully schedulable bythe cluster.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/backup_restore.html", "title": "Atomic Registry Latest | Cluster Administration | Backup and Restore", "content": "In Atomic Registry, you can back up (saving state to separate storage) andrestore (recreating state from separate storage) at the cluster level. Thereis also some preliminary support for per-project backup.The full state of a cluster installation includes:\nThis topic does not cover how to back up and restorepersistentstorage, as those topics are left to the underlying storage provider. However,an example of how to perform a generic backup ofapplication data is provided.\nThis topic only provides a generic way of backing up applications and theAtomic Registry cluster. It can not take into account custom requirements.Therefore, you should create a full backup and restore procedure. To preventdata loss, necessary precautions should be taken.\nNote the location of the etcd data directory (or $ETCD_DATA_DIR in thefollowing sections), which depends on how etcd is deployed.\nAlthough this step is not strictly necessary, doing so ensures that the etcddata is fully synchronized.\nIf etcd is running on more than one host,the various instances regularly synchronize their data,so creating a backup for one of them is sufficient.\nFor a container-based installation, you must use docker exec to run etcdctlinside the container.\nTo restore the cluster:\nThis should be done in thesame way thatAtomic Registry was previously installed.\nTo do so, edit the /usr/lib/systemd/system/etcd.service file, and add--force-new-cluster:\nThen, restart the etcd service:\nWhen using an external etcd host, you must first restore the etcd backupby creating a new, single node etcd cluster. If using external etcd withmultiple members, you must then also add any additional etcd members to thecluster one by one.\nHowever, the details of the restoration process differ betweenembedded andexternal etcd. See the followingsection and follow the relevant stepsbeforeBringing OpenShiftServices Back Online.\nRestore your etcd backup and configuration:\nThe $ETCD_DIR location differs between external and embedded etcd.\nVerify etcd has started successfully by checking the output from the abovecommand, which should look similar to the following near the end:\nChoose a system to be the initial etcd member, and restore its etcd backup andconfiguration:\nThe $ETCD_DIR location differs between external and embedded etcd.\nTo do so, edit the /usr/lib/systemd/system/etcd.service file, and add--force-new-cluster:\nThen restart the etcd service:\nTo add additional etcd members to the cluster, you must first adjust the defaultlocalhost peer in the peerURLs value for the first member:\nAlternatively, you can use curl:\nEach member must be fully added and brought online one at a time. When addingeach additional member to the cluster, the peerURLs list must be correct forthat point in time, so it will grow by one for each member added. The etcdctlmember add command will output the values that need to be set in theetcd.conf file as you add each member, as described in the followinginstructions.\nIn cases where etcd members have failed and you still have a quorum of etcdcluster members running, you can use the surviving members toadd additional etcd members without downtime.\nSuggested Cluster Size\nHaving a cluster with an odd number of etcd hosts can account for faulttolerance. Having an odd number of etcd hosts does not change the number neededfor a quorum, but increases the tolerance for failure. For example, a clustersize of three members, quorum is two leaving a failure tolerance ofone. This ensures the cluster will continue to operate if two of the members arehealthy.\nHaving an in-production cluster of three etcd hosts is recommended.\nThe following presumes you have a backup of the /etc/etcd configuration forthe etcd hosts.\nEnsure version etcd-2.3.7-4.el7.x86_64 or greater is installed, and that thesame version is installed on each host.\nEnsure version etcd-2.3.7-4.el7.x86_64 or greater is installed, and that thesame version is installed on the new host.\nStop the etcd service on the failed etcd member:\nCopy the three environment variables in the etcdctl member add output. They will be used later.\nReplace the IP address with the \"NEW_ETCD\" value for:\nFor replacing failed members, you will need to remove the failed hosts from theetcd configuration.\nOn a single master cluster installation:\nOn a multi-master cluster installation, on each master:\nThe procedure to add an etcd member is complete.\nOn each Atomic Registry master, restore your master and node configuration frombackup and enable and restart all relevant services.\nOn the master in a single master cluster:\nOn each master in a multi-master cluster:\nOn each Atomic Registry node, restore your node-config.yaml file from backupand enable and restart the atomic-openshift-node service:\nYour Atomic Registry cluster should now be back online.\nA future release of Atomic Registry will feature specific support forper-project back up and restore.\nFor now, to back up API objects at the project level, use oc export for eachobject to be saved. For example, to save the deployment configuration frontendin YAML format:\nTo back up all of the project (with the exception of cluster objects likenamespaces and projects):\nSometimes custom policyrolebindings are used in a project. For example, a project administrator can giveanother user a certain role in the project and grant that user project access.\nThese role bindings can be exported:\nIf custom service accounts are created in a project, these need to be exported:\nCustom secrets like source control management secrets (SSH Public Keys,Username/Password) should be exported if they are used:\nIf the an application within a project uses a persistent volume through apersistent volume claim (PVC), these should be backed up:\nTo restore a project, recreate the project and recreate all all of the objectsthat were exported during the backup:\nSome resources can fail to be created (for example, pods and default serviceaccounts).\nIn many cases, application data can be backed up using the oc rsync command,assuming rsync is installed within the container image. The Red Hat rhel7base image does contain rsync. Therefore, all images that are based on rhel7contain it as well. See Troubleshooting and Debugging CLI Operations - rsync.\nThis is a generic backup of application data and does not take into accountapplication-specific backup procedures, for example special export/importprocedures for database systems.\nOther means of backup may exist depending on the type of the persistent volume(for example, Cinder, NFS, Gluster, or others).\nThe paths to back up are also application specific. You can determinewhat path to back up by looking at the mountPath for volumes in thedeploymentconfig.\nThis type of application data backup can only be performed while an applicationpod is currently running.\nThe process for restoring application data is similar to theapplication backup procedure using the oc rsynctool. The same restrictions apply and the process of restoring application datarequires a persistent volume.\nDepending on the application, you may be required to restart the application.\nAlternatively, you can scale down the deployment to 0, and then up again:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/idling_applications.html", "title": "Atomic Registry Latest | Cluster Administration | Idling Applications", "content": "As an Atomic Registry administrator, you can idle applications to reduceresource consumption. This is useful when deployed on a public cloud where costis related to resource consumption.\nIf any scalable resources are not in use, Atomic Registry discovers, then idlesthem, by scaling them to 0 replicas. When network traffic is directed to theresources, they are unidled by scaling up the replicas, then operationcontinues.\nApplications are made of services, as well as other scalable resources, such asdeployment configurations. The action of idling an application involves idlingall associated resources.\nIdling an application involves finding the scalable resources (deploymentconfigurations, replication controllers, and others) associated with a service.Idling an application finds the service and marks it as idled, scaling down theresources to zero replicas.\nYou can use the oc idle command toidlea single service, or use the --resource-names-file option toidlemultiple services.\nIdle a single service with the following command:\nIdle multiple services by creating a list of the desired services, then using the --resource-names-file option with the oc idle command.\nThis is helpful if an application spans across a set of services, or when idlingmultiples services in conjunction with a script in order to idle applications inbulk.\nApplication services become active again when they receive network traffic andwill be scaled back up their previous state. This includes both traffic to theservices and traffic passing through routes.\nAutomatic unidling by a router is currently only supported by the default HAProxy router.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/image_policy.html", "title": "Atomic Registry Latest | Cluster Administration | Image Policy", "content": "You can control which images are allowed to run on your cluster using the ImagePolicyadmission plug-in (currently considered beta). It allows you to control:\nTo enable this feature, configure the plug-in in master-config.yaml:\nFor example, use the information above, then test like this:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/index.html", "title": "Atomic Registry Latest | Cluster Administration | Overview", "content": "\u00a0These Cluster Administration topics cover the day-to-day tasks for managingyour Atomic Registry cluster and other advanced configuration topics.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/limits.html", "title": "Atomic Registry Latest | Cluster Administration | Setting Limit Ranges", "content": "A limit range, defined by a LimitRange object, enumeratescompute resourceconstraints in a project at the pod,container, image, image stream, and persistent volume claim level, and specifies the amount of resourcesthat a pod, container, image, image stream, or persistent volume claim can consume.\nAll resource create and modification requests are evaluated against eachLimitRange object in the project. If the resource violates any of theenumerated constraints, then the resource is rejected. If the resource does notset an explicit value, and if the constraint supports a default value, then thedefault value is applied to the resource.\nBoth core and Atomic Registry resources can be specified in just one limit rangeobject. They are separated here into two examples for clarity.\nSupported Resources:\nSupported Constraints:\nPer container, the following must hold true if specified:\nMin\nMin[resource] less than or equal to container.resources.requests[resource](required) less than or equal to container/resources.limits[resource](optional)\nIf the configuration defines a min CPU, then the request value must be greaterthan the CPU value. A limit value does not need to be specified.\nMax\ncontainer.resources.limits[resource] (required) less than or equal toMax[resource]\nIf the configuration defines a max CPU, then you do not need to define arequest value, but a limit value does need to be set that satisfies the maximumCPU constraint.\nMaxLimitRequestRatio\nMaxLimitRequestRatio[resource] less than or equal to (container.resources.limits[resource] /container.resources.requests[resource])\nIf a configuration defines a maxLimitRequestRatio value, then any newcontainers must have both a request and limit value. Additionally,Atomic Registry calculates a limit to request ratio by dividing the limit by therequest.\nFor example, if a container has cpu: 500 in the limit value, andcpu: 100 in the request value, then its limit to request ratio for cpu is5. This ratio must be less than or equal to the maxLimitRequestRatio.\nSupported Defaults:\nSupported Resources:\nSupported Constraints:\nAcross all containers in a pod, the following must hold true:\nMin\nMin[resource] less than or equal to container.resources.requests[resource](required) less than or equal to container.resources.limits[resource](optional)\nMax\ncontainer.resources.limits[resource] (required) less than or equal toMax[resource]\nMaxLimitRequestRatio\nMaxLimitRequestRatio[resource] less than or equal to (container.resources.limits[resource] /container.resources.requests[resource])\nSupported Resources:\nResource type name:\nPer image, the following must hold true if specified:\nMax\nimage.dockerimagemetadata.size less than or equal to Max[resource]\nThe image size is not always available in the manifest of an uploaded image.This is especially the case for images built with Docker 1.10 or higher andpushed to a v2 registry. If such an image is pulled with an older Docker daemon,the image manifest will be converted by the registry to schema v1 lacking allthe size information. No storage limit set on images will prevent it from beinguploaded.\nThe issue is beingaddressed.\nSupported Resources:\nResource type name:\nPer image stream, the following must hold true if specified:\nMax[openshift.io/image-tags]\nlength( uniqueimagetags( imagestream.spec.tags ) ) less than or equal to Max[openshift.io/image-tags]\nuniqueimagetags returns unique references to images of given spec tags.\nMax[openshift.io/images]\nlength( uniqueimages( imagestream.status.tags ) ) less than or equal to Max[openshift.io/images]\nuniqueimages returns unique image names found in status tags. The name equalsimage\u2019s digest.\nResource openshift.io/image-tags represents uniqueimagereferences. Possible references are an ImageStreamTag, anImageStreamImage and a DockerImage. They may be created using commandsoc tag and oc import-image or by usingtag tracking. No distinctionis made between internal and external references. However, each unique referencetagged in the image stream\u2019s specification is counted just once. It does notrestrict pushes to an internal container registry in any way, but is useful for tagrestriction.\nResource openshift.io/images represents unique image names recorded in imagestream status. It allows for restriction of a number of images that can bepushed to the internal registry. Internal and external references are notdistinguished.\nSupported Resources:\nSupported Constraints:\nAcross all persistent volume claims in a project, the following must hold true:\nMin\nMin[resource] \u21d0 claim.spec.resources.requests[resource] (required)\nMax\nclaim.spec.resources.requests[resource] (required) \u21d0 Max[resource]\nTo apply a limit range to a project, create a limit rangeobject definition on your file system to your desired specifications, then run:\nYou can view any limit ranges defined in a project by navigating in the webconsole to the project\u2019s Settings tab.\nYou can also use the CLI to view limit range details:\nRemove any active limit range to no longer enforce the limits of a project:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/manage_authorization_policy.html", "title": "Atomic Registry Latest | Cluster Administration | Managing Authorization Policies", "content": "You can use the CLI to viewauthorizationpolicies and the administrator CLI to manage theroles and bindingswithin a policy.\nThe web console also provides some basic management of authorization roles.Through the web console users and groups may be assigned registry-admin,registry-editor or registry-viewer.\nRoles grantvarious levels of access in the system-wideclusterpolicy as well as project-scopedlocalpolicies.Usersand groups can be associated with, or bound to, multiple roles at the sametime.  You can view details about the roles and their bindings using the ocdescribe command.\nUsers with the cluster-admindefault rolein the cluster policy can view cluster policy and all local policies. Users withthe admindefault rolein a given local policy can view that project-scoped policy.\nReview a full list of verbs in theEvaluatingAuthorization section.\nTo view the cluster roles and their associated rule sets in the cluster policy:\nTo view the current set of cluster bindings, which shows the users and groups that are bound to various roles:\nWhile the list of local roles and their associated rule sets are not viewablewithin a local policy, all of thedefault rolesare still applicable and can be added to users or groups, other than thecluster-admin default role. The local bindings, however, are viewable.\nTo view the current set of local bindings, which shows the users and groups thatare bound to various roles:\nBy default, the current project is used when viewing local policy.Alternatively, a project can be specified with the -n flag. This is useful forviewing the local policy of another project, if the user already has the admindefault rolein it.\nBy default in a local policy, only the binding for the admin role isimmediately listed. However, if otherdefault rolesare added to users and groups within a local policy, they become listed as well.\nAdding, or binding, arole tousersor groups gives the user or group the relevant access granted by the role. Youcan add and remove roles to and from users and groups using oadm policycommands.\nWhen managing a user or group\u2019s associated roles for a local policy using thefollowing operations, a project may be specified with the -n flag. If it isnot specified, then the current project is used.\nYou can also manage role bindings for the cluster policy using the followingoperations. The -n flag is not used for these operations because thecluster policy uses non-namespaced resources.\nFor example, you can add the admin role to the alice user in joe-projectby running:\nYou can then view the local bindings and verify the addition in the output:\nBy default, project developers do not have the permission to createdaemonsets. As a clusteradministrator, you can grant them the abilities.\nTo create a local role for a project, you can either copy and modify an existingrole or build a new role from scratch. It is recommended that you build it fromscratch so that you understand each of the permissions assigned.\nTo copy the cluster role view to use as a local role, run:\nTo create a new role from scratch, save this snippet into the filerole_exampleview.yaml:\nThen, to use the current project, run:\nOptionally, annotate it with a description.\nTo use the new role, run:\nA clusterrolebinding is a role binding that exists at the cluster level. Arolebinding exists at the project level. This can be confusing. Theclusterrolebinding view must be assigned to a user within a project for thatuser to view the project. Local roles are only created if a cluster role doesnot provide the set of permissions needed for a particular situation, which isunlikely.\nSome cluster role names are initially confusing. The clusterroleclusteradmin can be assigned to a user within a project, making it appear thatthis user has the privileges of a cluster administrator. This is not the case.The clusteradmin cluster role bound to a certain project is more like a superadministrator for that project, granting the permissions of the cluster roleadmin, plus a few additional permissions like the ability to edit rate limits.This can appear especially confusing via the web console UI, which does not listcluster policy (where cluster administrators exist). However, it does list localpolicy (where a locally bound clusteradmin may exist).\nWithin a project, project administrators should be able to see rolebindings,not clusterrolebindings.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/manage_users.html", "title": "Atomic Registry Latest | Cluster Administration | Managing Users", "content": "This topic describes the management ofuser accounts,including how new user accounts are created in Atomic Registry and how they canbe deleted.\nAfter new users log in to Atomic Registry, an account is created for that userper the identityprovider configured on the master. The cluster administrator canmanage the access level ofeach user.\nAtomic Registry user configuration is stored in several locations withinAtomic Registry. Regardless of the identity provider, Atomic Registry internallystores details like role-based access control (RBAC) information and groupmembership. To completely remove user information, this data must be removed inaddition to the user account.\nIn Atomic Registry, two object types contain user data outside theidentification provider: user and identity.\nTo get the current list of users:\nTo get the current list of identities:\nNote the matching UID between the two object types. If you attempt to change theauthentication provider after starting to use Atomic Registry, the user namesthat overlap will not work because of the entries in the identity list, whichwill still point to the old authentication method.\nTo add a label to a user or group:\nFor example, if the user name is theuser and the label is level=gold:\nTo remove the label:\nTo show labels for a user or group:\nTo delete a user:\nThe identity of the user is related to the identification provider you use. Getthe provider name from the user record in oc get user.\nIn this example, the identity provider name is htpasswd_auth. The command is:\nIf you skip this step, the user will not be able to log in again.\nAfter you complete these steps, a new account will be created in Atomic Registrywhen the user logs in again.\nIf your intention is to prevent the user from being able to log in again (forexample, if an employee has left the company and you want to permanently deletethe account), you can also remove the user from your authentication back end(like htpasswd, kerberos, or others) for the configured identityprovider.\nFor example, if you are using htpasswd, delete the entry in the htpasswdfile that is configured for Atomic Registry with the user name and password.\nFor external identification management like Lightweight Directory AccessProtocol (LDAP) or Red Hat Identity Management (IdM), use the user managementtools to remove the user entry.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/managing_networking.html", "title": "Atomic Registry Latest | Cluster Administration | Managing Networking", "content": "This topic describes the management of the overallclusternetwork, including project isolation and outbound traffic control.\nPod-level networking features, such as per-pod bandwidth limits, are discussedin ManagingPods.\nWhen your cluster is configured to usethe ovs-multitenant SDNplug-in, you can manage the separate pod overlay networks for projects usingthe administrator CLI.\nTo join projects to an existing project network:\nIn the above example, all the pods and services in <project2> and <project3>can now access any pods and services in <project1> and vice versa. Servicescan be accessed either by IP or fully-qualified DNS name(<service>.<pod_namespace>.svc.cluster.local). For example, to access aservice named db in a project myproject, use db.myproject.svc.cluster.local.\nAlternatively, instead of specifying specific project names, you can use the--selector=<project_selector> option.\nTo isolate the project network in the cluster and vice versa, run:\nIn the above example, all of the pods and services in <project1> and<project2> can not access any pods and services from other non-globalprojects in the cluster and vice versa.\nAlternatively, instead of specifying specific project names, you can use the--selector=<project_selector> option.\nTo allow projects to access all pods and services in the cluster and vice versa:\nIn the above example, all the pods and services in <project1> and <project2>can now access any pods and services in the cluster and vice versa.\nAlternatively, instead of specifying specific project names, you can use the--selector=<project_selector> option.\nIn Atomic Registry, host name collision prevention for routes and ingressobjects is enabled by default. This means that the host name in a route oringress object can only be set on creation and not edited afterwards. Disablinghost name collision prevention lets you edit a host name for ingress objects after creation.However, because Atomic Registry uses the object creation timestamp to determinethe oldest route or ingress object for a given host name, the route or ingressobject can hijack a host name with a newer route. This can happen if an olderroute changes its host name, or if an ingress object is introduced.\nThis is relevant to Atomic Registry installations that depend upon Kubernetesbehavior, including allowing the host names in ingress objects be edited.\nAs a cluster administrator you can allocate a number of static IP addresses to aspecific node at the host level. If an application developer needs a dedicatedIP address for their application service, they can request one during theprocess they use to ask for firewall access. They can then deploy an egressrouter from the developer\u2019s project, using a nodeSelector in the deploymentconfiguration to ensure that the pod lands on the host with the pre-allocatedstatic IP address.\nThe egress pod\u2019s deployment declares one of the source IPs, the destination IPof the protected service, and a gateway IP to reach the destination. After thepod is deployed, you cancreatea service to access the egress router pod, then add that source IP to thecorporate firewall. The developer then has access information to the egressrouter service that was created in their project, for example,service.project.cluster.domainname.com.\nWhen the developer needs to access the external, firewalled service, they cancall out to the egress router pod\u2019s service(service.project.cluster.domainname.com) in their application (for example,the JDBC connection information) rather than the actual protected service URL.\nAs an Atomic Registry cluster administrator, you can control egress traffic in three ways:\nAs an Atomic Registry cluster administrator, you can use egress firewall policyto limit the external addresses that some or all pods can access from within thecluster, so that:\nOr,\nOr,\nYou can configure projects to have different egress policies. For example,allowing <project A> access to a specified IP range, but denying the sameaccess to <project B>. Or restrict application developers from updating from(Python) pip mirrors, and forcing updates to only come from desired sources.\nYou must have theovs-multitenant plug-in enabled in order to limit pod access via egress policy.\nProject administrators can neither create EgressNetworkPolicy objects, noredit the ones you create in their project. There are also several otherrestrictions on where EgressNetworkPolicy can be created:\nViolating any of these restrictions results in broken egress policy for theproject, and may cause all external network traffic to be dropped.\nUse the oc command or the REST API to configure egress policy. You can useoc [create|replace|delete] to manipulate EgressNetworkPolicy objects. Theapi/swagger-spec/oapi-v1.json file has API-level details on how the objectsactually work.\nTo configure egress policy:\nWhen the example above is added to a project, it allows traffic to IP range1.2.3.0/24 and domain name www.foo.com, but denies access to all otherexternal IP addresses. Traffic to other pods is not affected because the policyonly applies to external traffic.\nThe rules in an EgressNetworkPolicy are checked in order, and the first onethat matches takes effect. If the three rules in the above example werereversed, then traffic would not be allowed to 1.2.3.0/24 and www.foo.combecause the 0.0.0.0/0 rule would be checked first, and it would match and denyall traffic.\nDomain name updates are polled based on the TTL (time to live) value of thedomain of the local non-authoritative server, or 30 minutes if the TTL is unableto be fetched. The pod should also resolve the domain from the same localnon-authoritative server when necessary, otherwise the IP addresses for thedomain perceived by the egress network policy controller and the pod will bedifferent, and the egress network policy may not be enforced as expected. In theabove example, suppose www.foo.com resolved to 10.11.12.13 and has a DNS TTLof one minute, but was later changed to 20.21.22.23. Atomic Registry will thentake up to one minute to adapt to these changes.\nThe egress firewall always allows pods access to the external interface of thenode the pod is on for DNS resolution. If your DNS resolution is not handled bysomething on the local node, then you will need to add egress firewall rulesallowing access to the DNS server\u2019s IP addresses if you are using domain namesin your pods. The default installersets up a local dnsmasq, so if you are using that setup you will not need to add extra rules.\nExposing services by creatingroutes will ignoreEgressNetworkPolicy. Egress network policy service endpoint filtering is doneat the node kubeproxy. When the router is involved, kubeproxy is bypassedand egress network policy enforcement is not applied. Administrators can preventthis bypass by limiting access to create routes.\nThe Atomic Registry egress router runs a service that redirects traffic to aspecified remote server, using a private source IP address that is not used foranything else. The service allows pods to talk to servers that are set upto only allow access from whitelisted IP addresses.\nThe egress router is not intended for every outgoing connection. Creating largenumbers of egress routers can push the limits of your network hardware. Forexample, creating an egress router for every project or application could exceedthe number of local MAC addresses that the network interface can handle beforefalling back to filtering MAC addresses in software.\nDeployment Considerations\nThe Egress router adds a second IP address and MAC address to the node\u2019s primarynetwork interface. If you are not running Atomic Registry on bare metal, you mayneed to configure your hypervisor or cloud provider to allow the additionaladdress.\nEgress Router Modes\nThe egress router can run in two different modes:redirect mode andHTTP proxy mode.Redirect mode works for all services except for HTTP and HTTPS. For HTTP andHTTPS services, use HTTP proxy mode.\nIn redirect mode, the egress router sets up iptables rules to redirect trafficfrom its own IP address to one or more destination IP addresses. Client podsthat want to make use of the reserved source IP address must be modified toconnect to the egress router rather than connecting directly to the destinationIP.\nTo check to see if the pod has been created:\nYour pods can now connect to this service. Their connections are redirected tothe corresponding ports on the external server, using the reserved egress IPaddress.\nThe egress router setup is performed by an \"init container\" created from theimage, and that container is run privileged so that it can configure the Macvlaninterface and set up iptables rules. After it finishes setting upthe iptables rules, it exits and thecontainer will run (doing nothing) until the pod is killed.\nThe environment variables tell the egress-router image what addresses to use; itwill configure the Macvlan interface to use EGRESS_SOURCE as its IP address,with EGRESS_GATEWAY as its gateway.\nNAT rules are set up so that connections to any TCP or UDP port on thepod\u2019s cluster IP address are redirected to the same port onEGRESS_DESTINATION.\nIf only some of the nodes in your cluster are capable of claiming the specifiedsource IP address and using the specified gateway, you can specify anodeName or nodeSelector indicating which nodes are acceptable.\nIn the previous example, connections to the egress pod (or its correspondingservice) on any port are redirected to a single destination IP. You can alsoconfigure different destination IPs depending on the port:\nEach line of EGRESS_DESTINATION can be one of three types:\nFor a large or frequently-changing set of destination mappings, youcan use a ConfigMap to externally maintain the list, and have the egress routerpod read it from there. This comes with the advantage of project administratorsbeing able to edit the ConfigMap, whereas they may not be able to edit the Poddefinition directly, because it contains a privileged container.\nNote that you can put blank lines and comments into this file\nHere egress-routes is the name of the ConfigMap object beingcreated and my-egress-destination.txt is the name of the file thedata is being read from.\nThe egress router does not automatically update when the ConfigMap changes.Restart the pod to get updates.\nIn HTTP proxy mode, the egress router runs as an HTTP proxy on port 8080.This only works for clients talking to HTTP or HTTPS-based services, but usuallyrequires fewer changes to the client pods to get them to work. Programs can betold to use an HTTP proxy by setting an environment variable.\nYou can specify any of the following for the EGRESS_HTTP_PROXY_DESTINATIONvalue. You can also use *, meaning \"allow connections to all remotedestinations\". Each line in the configuration specifies one group of connectionsto allow or deny:\nUsing the http_proxy and https_proxy environment variables is not necessaryfor all setups. If the above does not create a working setup, then consult thedocumentation for the tool or software you are running in the pod.\nYou can also specify the EGRESS_HTTP_PROXY_DESTINATION using aConfigMap, similarly tothe redirecting egress router example above.\nUsing a replication controller, you can ensure that there is always one copy of the egress router pod in order to prevent downtime.\nSome cluster administrators may want to perform actions on outgoingtraffic that do not fit within the model of EgressNetworkPolicy or theegress router. In some cases, this can be done by creating iptablesrules directly.\nFor example, you could create rules that log traffic to particulardestinations, or to prevent more than a certain number of outgoingconnections per second.\nAtomic Registry does not provide a way to add custom iptables rulesautomatically, but it does provide a place where such rules can beadded manually by the administrator. Each node, on startup, willcreate an empty chain called OPENSHIFT-ADMIN-OUTPUT-RULES in thefilter table (assuming that the chain does not already exist). Anyrules added to that chain by an administrator will be applied to alltraffic going from a pod to a destination outside the cluster (and notto any other traffic).\nThere are a few things to watch out for when using this functionality:\nAt this time, multicast is best used for low bandwidth coordination or servicediscovery and not a high-bandwidth solution.\nMulticast traffic between Atomic Registry pods is disabled by default. You canenable Multicast on a per-project basis by setting an annotation on theproject\u2019s corresponding netnamespace object:\nDisable multicast by removing the annotation:\nIf you havejoinednetworks together, you will need to enable Multicast in each projects'netnamespace in order for it to take effect in any of the projects. To enableMulticast in the default project, you must also enable it in all otherprojects that have beenmadeglobal.\nMulticast global projects are not \"global\", but instead communicate with onlyother global projects via Multicast, not with all projects in the cluster, as isthe case with unicast.\nEnabling the Kubernetes NetworkPolicy is a Technology Preview feature only.\nKubernetes NetworkPolicy is not currently fully supported by Atomic Registry,and the ovs-subnet and ovs-multitenant plug-ins ignore NetworkPolicyobjects. However, a Technology Preview of NetworkPolicy support is available byusing the ovs-networkpolicy plug-in.\nIn a clusterconfiguredto use the ovs-networkpolicy plug-in, network isolation is controlledentirely by NetworkPolicy objects and the associated Namespace annotation. In particular, by default, all projects are able to access podsin all other projects. The project to be isolated must first be configured toopt in to isolation by setting the proper annotation on its Namespace object,and then creating NetworkPolicy objects indicating the incoming connections tobe allowed.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/managing_pods.html", "title": "Atomic Registry Latest | Cluster Administration | Managing Pods", "content": "This topic describes the management ofpods, includinglimiting their run-once duration, and how much bandwidth they can use.\nYou can apply quality-of-service traffic shaping to a pod and effectively limitits available bandwidth. Egress traffic (from the pod) is handled by policing,which simply drops packets in excess of the configured rate. Ingress traffic (tothe pod) is handled by shaping queued packets to effectively handle data. Thelimits you place on a pod do not affect the bandwidth of other pods.\nTo limit the bandwidth on a pod:\nA pod disruption budget is part of theKubernetes API, which can bemanaged with oc commands like otherobject types. Theyallow the specification of safety constraints on pods during operations, such asdraining a node for maintenance.\nPodDisruptionBudget is an API object that specifies the minimum number orpercentage of replicas that must be up at a time. Setting these in projects canbe helpful during node maintenance (such as scaling a cluster down or a clusterupgrade) and is only honored on voluntary evictions (not on node failures).\nA PodDisruptionBudget object\u2019s configuration consists of the following keyparts:\nThe following is an example of a PodDisruptionBudget resource:\nIf you created a YAML file with the above object definition, you could add it to project with the following:\nYou can check for pod disruption budgets across all projects with the following:\nThe PodDisruptionBudget is considered healthy when there are at leastminAvailable pods running in the system. Every pod above that limit can beevicted.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/managing_projects.html", "title": "Atomic Registry Latest | Cluster Administration | Managing Projects", "content": "In Atomic Registry, projects are used to group and isolate related objects. As an administrator, you can give developers access to certain projects, allow them to create their own, and give them administrative rights within individual projects.\nYou can allow developers to create their own projects. There is an endpointthat will provision a project according to atemplate. The web console and oc new-projectcommand use this endpoint when a developer creates a new project.\nThe API server automatically provisions projects based on the template that isidentified by the projectRequestTemplate parameter of the master-config.yamlfile. If the parameter is not defined, the API server creates a default templatethat creates a project with the requested name, and assigns the requesting userto the \"admin\" role for that project.\nTo create your own custom project template:\nWhen a project request is submitted, the API substitutes the following parameters into the template:\nAccess to the API is granted to developers with theself-provisionerrole and the self-provisioners cluster role binding. This role is availableto all authenticated developers by default.\nRemoving the self-provisionerscluster rolefrom authenticated user groups will deny permissions for self-provisioning any new projects.\nWhen disabling self-provisioning, set the projectRequestMessage parameter in themaster-config.yaml file to instruct developers on how to request a newproject. This parameter is a string that will be presented to the developer inthe web console and command line when they attempt to self-provision a project.For example:\nor:\nThe number of self-provisioned projects requested by a given user can be limitedwith the ProjectRequestLimitadmissioncontrol plug-in.\nIf your project request template was created in Atomic Registry 3.1 or earlierusing the process described inModifying the Template for NewProjects, then the generated template does not include the annotationopenshift.io/requester: ${PROJECT_REQUESTING_USER}, which is used for theProjectRequestLimitConfig. You must add the annotation.\nIn order to specify limits for users, a configuration must be specified for theplug-in within the master configuration file(/etc/origin/master/master-config.yaml). The plug-in configuration takes alist of user label selectors and the associated maximum project requests.\nSelectors are evaluated in order. The first one matching the current user willbe used to determine the maximum number of projects. If a selector is notspecified, a limit applies to all users. If a maximum number of projects is notspecified, then an unlimited number of projects are allowed for a specificselector.\nThe following configuration sets a global limit of 2 projects per user while allowing 10projects for users with a label of level=advanced and unlimited projects forusers with a label of level=admin.\nManagingUser and Group Labels provides further guidance on how to add, remove, or showlabels for users and groups.\nOnce your changes are made, restart Atomic Registry for the changes to takeeffect.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/multiproject_quota.html", "title": "Atomic Registry Latest | Cluster Administration | Setting Multi-Project Quotas", "content": "A multi-project quota, defined by a ClusterResourceQuota object, allowsquotas to be shared acrossmultiple projects. Resources used in each selected project will be aggregatedand that aggregate will be used to limit resources across all the selectedprojects.\nProjects can be selected based on either annotation selection, label selection, or both.For example:\ncreates:\nThis multi-project quota document controls all projects requested by<user-name> using the default project request endpoint. You are limited to 10pods and 20 secrets.\nA project administrator is not allowed to create or modify the multi-projectquota that limits his or her project, but the administrator is allowed to view themulti-project quota documents that are applied to his or her project. Theproject administrator can do this via the AppliedClusterResourceQuotaresource.\nproduces:\nDue to the locking consideration when claiming quota allocations, the number ofactive projects selected by a multi-project quota is an important consideration.Selecting more than 100 projects under a single multi-project quota may havedetrimental effects on API server responsiveness in those projects.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/quota.html", "title": "Atomic Registry Latest | Cluster Administration | Setting Quotas", "content": "A resource quota, defined by a ResourceQuota object, provides constraintsthat limit aggregate resource consumption per project. It can limit the quantityof objects that can be created in a project by type, as well as the total amountof compute resources and storage that may be consumed by resources in that project.\nThe following describes the set of compute resources and object types that may bemanaged by a quota.\nA pod is in a terminal state if status.phase in (Failed, Succeeded) is true.\ncpu\nThe sum of CPU requests across all pods in a non-terminal state cannot exceedthis value. cpu and requests.cpu are the same value and can be usedinterchangeably.\nmemory\nThe sum of memory requests across all pods in a non-terminal state cannotexceed this value. memory and requests.memory are the same value and canbe used interchangeably.\nrequests.cpu\nThe sum of CPU requests across all pods in a non-terminal state cannot exceedthis value. cpu and requests.cpu are the same value and can be usedinterchangeably.\nrequests.memory\nThe sum of memory requests across all pods in a non-terminal state cannotexceed this value. memory and requests.memory are the same value and canbe used interchangeably.\nlimits.cpu\nThe sum of CPU limits across all pods in a non-terminal state cannot exceedthis value.\nlimits.memory\nThe sum of memory limits across all pods in a non-terminal state cannot exceedthis value.\nrequests.storage\nThe sum of storage requests across all persistent volume claims in any state cannotexceed this value.\npersistentvolumeclaims\nThe total number of persistent volume claims that can exist in the project.\n<storage-class-name>.storageclass.storage.k8s.io/requests.storage\nThe sum of storage requests across all persistent volume claims in any state that have a matching storage class, cannot exceed this value.\n<storage-class-name>.storageclass.storage.k8s.io/persistentvolumeclaims\nThe total number of persistent volume claims with a matching storage class that can exist in the project.\npods\nThe total number of pods in a non-terminal state that can exist in the project.\nreplicationcontrollers\nThe total number of replication controllers that can exist in the project.\nresourcequotas\nThe total number of resource quotas that can exist in the project.\nservices\nThe total number of services that can exist in the project.\nsecrets\nThe total number of secrets that can exist in the project.\nconfigmaps\nThe total number of ConfigMap objects that can exist in the project.\npersistentvolumeclaims\nThe total number of persistent volume claims that can exist in the project.\nopenshift.io/imagestreams\nThe total number of image streams that can exist in the project.\nEach quota can have an associated set of scopes. A quota will onlymeasure usage for a resource if it matches the intersection of enumeratedscopes.\nAdding a scope to a quota restricts the set of resources to which that quota canapply. Specifying a resource outside of the allowed set results in a validationerror.\nTerminating\nMatch pods where spec.activeDeadlineSeconds >= 0.\nNotTerminating\nMatch pods where spec.activeDeadlineSeconds is nil.\nBestEffort\nMatch pods that have best effort quality of service for either cpu ormemory.\nNotBestEffort\nMatch pods that do not have best effort quality of service for cpu andmemory.\nA BestEffort scope restricts a quota to limiting the following resources:\nA Terminating, NotTerminating, or NotBestEffort scope restricts a quotato tracking the following resources:\nAfter a resource quota for a project is first created, the project restricts theability to create any new resources that may violate a quota constraint until ithas calculated updated usage statistics.\nAfter a quota is created and usage statistics are updated, the project acceptsthe creation of new content. When you create or modify resources, your quotausage is incremented immediately upon the request to create or modify theresource.\nWhen you delete a resource, your quota use is decremented during the next fullrecalculation of quota statistics for the project.A configurable amount of time determineshow long it takes to reduce quota usage statistics to their current observedsystem value.\nIf project modifications exceed a quota usage limit, the server denies theaction, and an appropriate error message is returned to the user explaining thequota constraint violated, and what their currently observed usage stats are inthe system.\nWhen allocatingcomputeresources, each container may specify a request and a limit value each forCPU and memory. Quotas can restrict any of these values.\nIf the quota has a value specified for requests.cpu or requests.memory,then it requires that every incoming container make an explicit request forthose resources. If the quota has a value specified for limits.cpu orlimits.memory, then it requires that every incoming container specify anexplicit limit for those resources.\nTo create a quota, first define the quota to your specifications in a file, forexample as seen inSample ResourceQuota Definitions. Then, create using that file to apply it to a project:\nFor example:\nYou can view usage statistics related to any hard limits defined in a project\u2019squota by navigating in the web console to the project\u2019s Settings tab.\nYou can also use the CLI to view quota details:\nWhen a set of resources are deleted, the synchronization time frame of resourcesis determined by the resource-quota-sync-period setting in the/etc/origin/master/master-config.yaml file.\nBefore quota usage is restored, a user may encounter problems when attempting toreuse the resources. You can change the resource-quota-sync-period settingto have the set of resources regenerate at the desired amount of time (inseconds) and for the resources to be available again:\nAfter making any changes, restart the master service to apply them.\nAdjusting the regeneration time can be helpful for creating resources anddetermining resource usage when automation is used.\nThe resource-quota-sync-period setting is designed to balance systemperformance. Reducing the sync period can result in a heavy load on the master.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/scoped_tokens.html", "title": "Atomic Registry Latest | Cluster Administration | Scoped Tokens", "content": "A user may want to give another entity the power to act as they have, but onlyin a limited way. For example, a project administrator may want to delegate thepower to create pods. One way to do this is to create a scoped token.\nA scoped token is a token that identifies as a given user, but is limited tocertain actions by its scope. Right now, only a cluster-admin can createscoped tokens.\nScopes are evaluated by converting the set of scopes for a token into a set ofPolicyRules. Then, the request is matched against those rules. The requestattributes must match at least one of the scope rules to be passed to the\"normal\" authorizer for further authorization checks.\nUser scopes are focused on getting information about a given user. They areintent-based, so the rules are automatically created for you:\nThe role scope allows you to have the same level of access as a given rolefiltered by namespace.\nCaveat: This prevents escalating access. Even if the role allows access toresources like secrets, rolebindings, and roles, this scope will deny accessto those resources. This helps prevent unexpected escalations. Many people donot think of a role like edit as being an escalating role, but with access toa secret it is.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/seccomp.html", "title": "Atomic Registry Latest | Cluster Administration | Restricting Application Capabilities Using Seccomp", "content": "Seccomp (secure computing mode) is used to restrict the set of system callsapplications can make, allowing cluster administrators greater control over thesecurity of workloads running in Atomic Registry.\nSeccomp support is achieved via two annotations in the pod configuration:\nContainers are run with unconfined seccomp settings by default.\nFor detailed design information, refer to theseccompdesign document.\nSeccomp is a feature of the Linux kernel. To ensure seccomp is enabled on yoursystem, run:\nA seccomp profile is a json file providing syscalls and the appropriate actionto take when a syscall is invoked.\nThedefaultprofile is sufficient in many cases, but the cluster administrator must definethe security constraints of an individual system.\nTo create your own custom profile, create a file on every node in theseccomp-profile-root directory.\nIf you are using the default docker/default profile, you do not need tocreate one.\nThe allowable formats of the seccompProfiles field include:\nFor example, if you are using the default docker/default profile, configure the restricted SCC with:\nTo ensure pods in your cluster run with a custom profile in the restricted SCC:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/service_accounts.html", "title": "Atomic Registry Latest | Cluster Administration | Configuring Service Accounts", "content": "Service accounts provide a flexible way to control API access without sharing aregular user\u2019s credentials.\nEvery service account has an associated user name that can be granted roles,just like a regular user. The user name is derived from its project and name:\nFor example, to add the view role to the robot service account in thetop-secret project:\nEvery service account is also a member of two groups:\nFor example, to allow all service accounts in all projects to view resources inthe top-secret project:\nTo allow all service accounts in the managers project to edit resources in thetop-secret project:\nService accounts authenticate to the API using tokens signed by a private RSAkey. The authentication layer verifies the signature using a matching public RSAkey.\nTo enable service account token generation, update the serviceAccountConfigstanza in the /etc/origin/master/master-config.yml file on the master tospecify a privateKeyFile (for signing), and a matching public key file inthe publicKeyFiles list:\nService accounts are required in each project to run builds, deployments, andother pods. The managedNames setting in the/etc/origin/master/master-config.yml file on the master controls whichservice accounts are automatically created in every project:\nAll service accounts in a project are given the system:image-puller role,which allows pulling images from any image stream in the project using theinternal container registry.\nSeveral infrastructure controllers run using service account credentials. Thefollowing service accounts are created in the Atomic Registry infrastructureproject (openshift-infra) at server start, and given the following rolescluster-wide:\nTo configure the project where those service accounts are created, set theopenshiftInfrastructureNamespace field in in the/etc/origin/master/master-config.yml file on the master:\nSet the limitSecretReferences field in the/etc/origin/master/master-config.yml file on the master to true to requirepod secret references to be whitelisted by their service accounts. Set its valueto false to allow pods to reference any secret in the project.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/admin_guide/tcp_ingress_external_ports.html", "title": "Atomic Registry Latest | Cluster Administration | Assigning Unique External IPs for Ingress Traffic", "content": "One approach to gettingexternaltraffic into the cluster is by using ExternalIP or IngressIP addresses.\nThis feature is only supported in non-cloud deployments. For cloud (GCE, AWS, and OpenStack) deployments,loadBalancer services can be used to automatically deploy a cloud load balancer to target the service\u2019s endpoints.\nAtomic Registry supports two pools of IP addresses:\nBoth have to be configured to a device on an Atomic Registry host to be used,whether with network interface controller (NIC) or virtual ethernet, as well asexternal routing. Ipfailover is recommended for this, because it selects thehost and configures the NIC.\nIngressIP and ExternalIP both allow external traffic access to the cluster, and,if routed correctly, external traffic can reach that service\u2019s endpoints via anyTCP/UDP port the service exposes. This can be simpler than having to manage theport space of a limited number of shared IP addresses when manually assigningexternal IPs to services. Also, these addresses can be used as virtual IPs(VIPs) when configuringhighavailability.\nAtomic Registry supports both the automatic and manual assignment of IPaddresses, and each address is guaranteed to be assigned to a maximum of oneservice. This ensures that each service can expose its chosen ports regardlessof the ports exposed by other services.\nTo use an ExternalIP, you can:\nYou must ensure that the IP address pool you assign terminates at one or more nodes in your cluster. You can use the existingoadm ipfailover to ensure that the external IPs are highly available.\nFor manually-configured external IPs, potential port clashes are handled on a first-come, first-served basis. If you request a port, it is only available if it has not yet been assigned for that IP address. For example:\nTwo services have been manually configured with the same externalIP address of 172.7.7.7.\nMongoDB service A requests port 27017, and thenMongoDB service B requests the same port; the first request gets the port.\nHowever, port clashes are not an issue for external IPs assigned by the ingress controller, because the controller assigns each service a unique address.\nIngress IPs can only be assigned if the cluster is not running in the cloud. In cloud environments, LoadBalancer-type services configure cloud-specific load balancers.\nIn non-cloud clusters, IngressIPNetworkCIDR is set by default to172.29.0.0/16. If your cluster environment is not already using this privaterange, you can use the default. However, if you want to use a different range,then you must setingressIPNetworkCIDRin the /etc/origin/master/master-config.yaml file before you assign aningress IP. Then, restart the master service.\nExternal IPs assigned to services of type LoadBalancer will always be in therange of IngressIPNetworkCIDR. If IngressIPNetworkCIDR is changed such thatthe assigned external IPs are no longer in range, the affected services will beassigned new external IPs compatible with the new range.\nTo assign an ingress IP:\nWhen your LoadBalancer-type service has an external IP assigned, the outputdisplays the IP:\nAdd a static route directing traffic for the ingress CIDR to a node in thecluster. For example:\nIn the example above, 172.29.0.0/16 is the ingressIPNetworkCIDR, and 10.66.140.17 is the node IP.\nIn addition to the cluster\u2019s internal IP addresses, the application developercan configure IP addresses that are external to the cluster. As theAtomic Registry administrator, you are responsible for ensuring that trafficarrives at a node with this IP.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/additional_concepts/authentication.html", "title": "Atomic Registry Latest | Architecture | Additional Concepts | Authentication", "content": "The authentication layer identifies the user associated with requests to theAtomic Registry API. The authorization layer then uses information about therequesting user to determine if the request should be allowed.\nA user in Atomic Registry is an entity that can make requests to theAtomic Registry API. Typically, this represents the account of a developer oradministrator that is interacting with Atomic Registry.\nA user can be assigned to one or more groups, each of which represent acertain set of users. Groups are useful whento grant permissions to multiple users at once, for example allowingaccess to objects within aproject, versus grantingthem to users individually.\nIn addition to explicitly defined groups, there are alsosystem groups, or virtual groups, that are automatically provisioned byOpenShift.\nIn the default set of virtual groups, note the following inparticular:\nRequests to the Atomic Registry API are authenticated using the followingmethods:\nAny request with an invalid access token or an invalid certificate is rejectedby the authentication layer with a 401 error.\nIf no access token or certificate is presented, the authentication layer assignsthe system:anonymous virtual user and the system:unauthenticated virtualgroup to the request. This allows the authorization layer to determine whichrequests, if any, an anonymous user is allowed to make.\nSee the REST API Overview for more informationand examples.\nA request to the Atomic Registry API may include an Impersonate-User header,which indicates that the requester wants to have the request handled as thoughit came from the specified user. This can be done on the command line by passingthe --as=username flag.\nBefore User A is allowed to impersonate User B, User A is first authenticated.Then, an authorization check occurs to ensure that User A is allowed toimpersonate the user named User B. If User A is requesting to impersonate aservice account (system:serviceaccount:namespace:name), Atomic Registry checksto ensure that User A can impersonate the serviceaccount named name innamespace. If the check fails, the request fails with a 403 (Forbidden) errorcode.\nBy default, project administrators and editors are allowed to impersonateservice accounts in their namespace. The sudoers role allows a user toimpersonate system:admin, which in turn has cluster administrator permissions.This grants some protection against typos (but not security) for someoneadministering the cluster. For example, oc delete nodes --all would beforbidden, but oc delete nodes --all --as=system:admin would be allowed. Youcan add a user to that group using oadm policy add-cluster-role-to-user sudoer<username>.\nThe Atomic Registry master includes a built-in OAuth server. Users obtain OAuthaccess tokens to authenticate themselves to the API.\nWhen a person requests a new OAuth token, the OAuth server uses the configuredto determine the identity of the person making the request.\nIt then determines what user that identity maps to, creates an access token forthat user, and returns the token for use.\nEvery request for an OAuth token must specify the OAuth client that willreceive and use the token. The following OAuth clients are automatically createdwhen starting the Atomic Registry API:\nTo register additional clients:\nAserviceaccount can be used as a constrained form of OAuth client. Service accounts canonly request a subset ofscopes thatallow access to some basic user information and role-based power inside of theservice account\u2019s own namespace:\nWhen using a service account as an OAuth client:\nAnnotation keys must have the prefixserviceaccounts.openshift.io/oauth-redirecturi. orserviceaccounts.openshift.io/oauth-redirectreference. such as:\nIn its simplest form, the annotation can be used to directly specify validredirect URIs. For example:\nThe first and second postfixes in the above example are used to separate thetwo valid redirect URIs.\nIn more complex configurations, static redirect URIs may not be enough. Forexample, perhaps you want all ingresses for a route to be considered valid. Thisis where dynamic redirect URIs via theserviceaccounts.openshift.io/oauth-redirectreference. prefix come into play.\nFor example:\nSince the value for this annotation contains serialized JSON data, it is easierto see in an expanded format:\nNow you can see that an OAuthRedirectReference allows us to reference theroute named jenkins. Thus, all ingresses for that route will now be consideredvalid.  The full specification for an OAuthRedirectReference is:\nBoth annotation prefixes can be combined to override the data provided by thereference object. For example:\nThe first postfix is used to tie the annotations together. Assuming that thejenkins route had an ingress of https://example.com, nowhttps://example.com/custompath is considered valid, buthttps://example.com is not.  The format for partially supplying overridedata is as follows:\nScheme\n\"https://\"\nHostname\n\"//website.com\"\nPort\n\"//:8000\"\nPath\n\"examplepath\"\nSpecifying a host name override will replace the host name data from thereferenced object, which is not likely to be desired behavior.\nAny combination of the above syntax can be combined using the following format:\n<scheme:>//<hostname><:port>/<path>\nThe same object can be referenced more than once for more flexibility:\nAssuming that the route named jenkins has an ingress ofhttps://example.com, then both https://example.com:8000 andhttps://example.com/custompath are considered valid.\nStatic and dynamic annotations can be used at the same time to achieve thedesired behavior:\nAll requests for OAuth tokens involve a request to <master>/oauth/authorize.Most authentication integrations place an authenticating proxy in front of thisendpoint, or configure Atomic Registry to validate credentials against a backingRequests to <master>/oauth/authorize can come from user-agents that cannotdisplay interactive login pages, such as the CLI. Therefore, Atomic Registrysupports authenticating using a WWW-Authenticate challenge in addition tointeractive login flows.\nIf an authenticating proxy is placed in front of the<master>/oauth/authorize endpoint, it should send unauthenticated,non-browser user-agents WWW-Authenticate challenges, rather than displaying aninteractive login page or redirecting to an interactive login flow.\nTo prevent cross-site request forgery (CSRF) attacks against browser clients, Basic authentication challengesshould only be sent if a X-CSRF-Token header is present on the request. Clients that expectto receive Basic WWW-Authenticate challenges should set this header to a non-empty value.\nIf the authenticating proxy cannot support WWW-Authenticate challenges, or ifAtomic Registry is configured to use an identity provider that does not supportWWW-Authenticate challenges, users can visit <master>/oauth/token/requestusing a browser to obtain an access token manually.\nApplications running in Atomic Registry may need to discover information aboutthe built-in OAuth server. For example, they may need to discover what theaddress of the <master> server is without manual configuration.  To aid inthis, Atomic Registry implements the IETFOAuth 2.0Authorization Server Metadata draft specification.\nThus, any application running inside the cluster can issue a GET request tohttps://openshift.default.svc/.well-known/oauth-authorization-server to fetchthe following information:\nThe OAuth server supports standardauthorization code grantand the implicit grantOAuth authorization flows.\nWhen requesting an OAuth token using the implicit grant flow(response_type=token) with a client_id configured to request WWW-Authenticatechallenges (like openshift-challenging-client), these are the possible serverresponses from /oauth/authorize, and how they should be handled:\n302\nLocation header containing an access_token parameter in the URL fragment (RFC 4.2.2)\nUse the access_token value as the OAuth token\n302\nLocation header containing an error query parameter (RFC 4.1.2.1)\nFail, optionally surfacing the error (and optional error_description) query values to the user\n302\nOther Location header\nFollow the redirect, and process the result using these rules\n401\nWWW-Authenticate header present\nRespond to challenge if type is recognized (e.g. Basic, Negotiate, etc), resubmit request, and process the result using these rules\n401\nWWW-Authenticate header missing\nNo challenge authentication is possible. Fail and show response body (which might contain links or details on alternate methods to obtain an OAuth token)\nOther\nOther\nFail, optionally surfacing response body to the user\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/additional_concepts/authorization.html", "title": "Atomic Registry Latest | Architecture | Additional Concepts | Authorization", "content": "Authorization policies determine whether a user is allowed to perform a givenaction within a project. This allows platform administrators touse the cluster policy to control who hasvarious access levels to the Atomic Registry platform itself and all projects. It alsoallows developers to use local policy tocontrol who has access to theirprojects. Note thatauthorization is a separate step from authentication,which is more about determining the identity of who is taking the action.\nAuthorization is managed using:\nCluster administrators can visualize rules, roles, and bindingsFor example, consider the following excerpt from viewing a policy, showing rulesets for the admin and basic-user default roles:\nThe following excerpt from viewing policy bindings shows the above roles boundto various users and groups:\nThe relationships between the the policy roles, policy bindings, users, anddevelopers are illustrated below.\nSeveral factors are combined to make the decision when Atomic Registry evaluatesauthorization:\nThe action being performed. In most cases, this consists of:\nAtomic Registry evaluates authorizations using the following steps:\nThere are two levels of authorization policy:\nThis two-level hierarchy allows re-usability over multiple projects through thecluster policy while allowing customization inside of individual projectsthrough local policies.\nDuring evaluation, both the cluster bindings and the local bindings are used.For example:\nRoles are collections of policy rules, which are sets ofpermitted verbs that can be performed on a set of resources. Atomic Registryincludes a set of default roles that can be added to users and groups in thecluster policy or in alocal policy.\nCluster administrators can visualize these roles, including a matrix of theverbs and resources each are associated using the CLI toAdditional system: roles are listed as well, whichare used for various Atomic Registry system and component operations.\nBy default in a local policy, only the binding for the admin role isimmediately listed when using the CLI toHowever, if other default roles are added to users and groups within a localpolicy, they become listed in the CLI output, as well.\nThe cluster- role assigned by the project administrator is limited in aproject. It is not the same cluster- role granted by the cluster-admin orsystem:admin.\nCluster roles are roles defined at the cluster level, but can be bound either atthe cluster level or at the project level.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/additional_concepts/other_api_objects.html", "title": "Atomic Registry Latest | Architecture | Additional Concepts | Other API Objects", "content": "An OAuthClient represents an OAuth client, as described inRFC 6749, section 2.\nThe following OAuthClient objects are automatically created:\nAn OAuthClientAuthorization represents an approval by a User for aparticular OAuthClient to be given an OAuthAccessToken with particularscopes.\nCreation of OAuthClientAuthorization objects is done during anauthorization request to the OAuth server.\nAn OAuthAuthorizeToken represents an OAuth authorization code, asdescribed in RFC 6749, section1.3.1.\nAn OAuthAuthorizeToken is created by a request to the /oauth/authorize endpoint,as described in RFC 6749,section 4.1.1.\nAn OAuthAuthorizeToken can then be used to obtain an OAuthAccessTokenwith a request to the /oauth/token endpoint, as described inRFC 6749, section 4.1.3.\nAn OAuthAccessToken represents an OAuth access token, as described inRFC 6749, section 1.4.\nAn OAuthAccessToken is created by a request to the /oauth/token endpoint,as described in RFC 6749,section 4.1.3.\nAccess tokens are used as bearer tokens to authenticate to the API.\nWhen a user logs into Atomic Registry, they do so using a configuredThis determines the user\u2019s identity, and provides that information toAtomic Registry.\nAtomic Registry then looks for a UserIdentityMapping for that Identity:\nA User represents an actor in the system. Users are granted permissions by\nUser objects are created automatically on first login, or can be created via theAPI.\nA UserIdentityMapping maps an Identity to a User.\nCreating, updating, or deleting a UserIdentityMapping modifies thecorresponding fields in the Identity and  User objects.\nAn Identity can only map to a single User, so logging in as a particularidentity unambiguously determines the User.\nA User can have multiple identities mapped to it. This allows multiple loginmethods to identify the same User.\nA Group represents a list of users in the system. Groups are granted permissions by\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/core_concepts/builds_and_image_streams.html", "title": "Atomic Registry Latest | Architecture | Core Concepts | Image Streams", "content": "An image stream comprises any number ofDocker-formattedcontainer images identified by tags. It presents a single virtual view ofrelated images, similar to an image repository, and may contain images from anyof the following:\nAn image stream image is a virtual resource that allows you to retrieve animage from a particular image stream where it is tagged. It is oftenabbreviated as isimage. It consists of two parts delimited by an at sign:<image stream name>@<image name>. To refer to the image in theexample above, the isimage looks like:\nUsers, without permission to read or list images on the cluster level, can stillretrieve the images tagged in a project they have access to using this resource.\nAn image stream tag is a named pointer to an image in an image stream. Itis often abbreviated as istag. It can reference any local or externallymanaged image. It contains a history of images represented as a stack of allimages the tag ever pointed to. Whenever a new or existing image is taggedunder particular istag, it is just placed at the first position in thehistory stack. Image previously occupying the top position will be available atthe second position etc. This allows for easy rollbacks to make tags point tohistorical images again.\nThe istag is composed of two parts separated by a colon: <image streamname>:<tag>. Istag referring to the imagesha256:47463d94eb5c049b2d23b03a9530bf944f8f967a0fe79147dd6b9135bf7dd13d inthe example above would beorigin-ruby-sample:latest.\nWhen theintegratedregistry receives a new image, it creates and sends an ImageStreamMappingto Atomic Registry, providing the image\u2019s namespace (i.e., its project), name,tag, and image metadata.\nThis information is used to create a new image (if it does not already exist)and to tag the image into the image stream. Atomic Registry stores completemetadata about each image, such as commands, entrypoint, and environmentvariables. Images in Atomic Registry are immutable and the maximum name lengthis 63 characters.\nThe following ImageStreamMapping example results in an image being tagged astest/origin-ruby-sample:latest:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/core_concepts/containers_and_images.html", "title": "Atomic Registry Latest | Architecture | Core Concepts | Images", "content": "A pod can have init containers in addition to application containers. Initcontainers allow you to reorganize setup scripts and binding code. An initcontainer differs from a regular container in that it always runs to completion.Each init container must complete successfully before the next one is started.\nFor more information, see Pods and Services.\nContainers in Atomic Registry are based on Docker-formatted container images. Animage is a binary that includes all of the requirements for running a singlecontainer, as well as metadata describing its needs and capabilities.\nYou can think of it as a packaging technology. Containers only have access toresources defined in the image unless you give the container additional accesswhen creating it. By deploying the same image in multiple containers acrossmultiple hosts and load balancing between them, Atomic Registry can provideredundancy and horizontal scaling for a service packaged into an image.\nYou can use the Docker CLI directly to build images, but Atomic Registry alsosupplies builder images that assist with creating new images by adding your codeor configuration to existing images.\nBecause applications develop over time, a single image name can actuallyrefer to many different versions of the \"same\" image. Each differentimage is referred to uniquely by its hash (a long hexadecimal numbere.g. fd44297e2ddb050ec4f\u2026\u200b) which is usually shortened to 12characters (e.g. fd44297e2ddb).\nRather than version numbers, the Docker service allows applying tags (such asv1, v2.1, GA, or the default latest) in addition to the image name tofurther specify the image desired, so you may see the same image referred to ascentos (implying the latest tag), centos:centos7, or fd44297e2ddb.\nDo not use the latest tag for any official Atomic Registry images. These areimages that start with openshift3/. latest can refer to a number ofversions, such as 3.4, or 3.5.\nHow you tag the images dictates the updating policy. The more specific you are, the less frequently the image will be updated. Use the following to determine your chosen Atomic Registry images policy:\nA container registry is a service for storing and retrieving Docker-formattedcontainer images. A registry contains a collection of one or more imagerepositories. Each image repository contains one or more tagged images. Dockerprovides its own registry, the Docker Hub, and you can also use private or third-party registries. Red Hat provides aregistry at registry.access.redhat.com for subscribers. Atomic Registry canalso supply its own internal registry for managing custom container images.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/core_concepts/projects_and_users.html", "title": "Atomic Registry Latest | Architecture | Core Concepts | Projects and Users", "content": "Interaction with Atomic Registry is associated with a user. An Atomic Registryuser object represents an actor which may be granted permissions in the systemby\nSeveral types of users can exist:\nEvery user must authenticate insome way in order to access Atomic Registry. API requests with no authenticationor invalid authentication are authenticated as requests by the anonymoussystem user. Once authenticated, policy determines what the user isauthorized to do.\nA Kubernetes namespace provides a mechanism to scope resources in a cluster.In Atomic Registry, a project is a Kubernetes namespace withadditional annotations.\nNamespaces provide a unique scope for:\nMost objects in the system are scoped by namespace, but some areexcepted and have no namespace, including nodes and users.\nTheKubernetesdocumentation has more information on namespaces.\nA project is a Kubernetes namespace with additional annotations, and is the central vehicleby which access to resources for regular users is managed.A project allows a community of users to organize and manage their content inisolation from other communities. Users must be given access to projects by administrators,or if allowed to create projects, automatically have access to their own projects.\nProjects can have a separate name, displayName, and description.\nEach project scopes its own set of:\nCluster administrators can create projectsandfor the project to any member of the user community.Cluster administrators can also allow developers to create\nDevelopers and administrators can interactwith projects using the CLI or theweb console.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/core_concepts/routes.html", "title": "Atomic Registry Latest | Architecture | Core Concepts | Routes", "content": "An Atomic Registry route exposes aservice at ahost name, like www.example.com, so that external clients can reach it byname.\nDNS resolution for a host name is handled separately from routing.Your administrator may have configured aDNS wildcard entrythat will resolve to the Atomic Registry node that is running theAtomic Registry router. If you are using a different host name you mayneed to modify its DNS records independently to resolve to the node thatis running the router.\nEach route consists of a name (limited to 63 characters), a service selector,and an optional security configuration.\nAn Atomic Registry administrator can deploy routers to nodes in anAtomic Registry cluster, which enable routescreated by developers to beused by external clients. The routing layer in Atomic Registry is pluggable, andtwo available router plug-ins are provided andsupported by default.\nA router uses the service selector to find theservice and the endpoints backingthe service.When both router and service provide load balancing,Atomic Registry uses the router load balancing.A router detects relevant changes in the IP addresses of its servicesand adapts its configuration accordingly.This is useful for custom routers to communicate modificationsof API objects to an external routing solution.\nThe path of a request starts with the DNS resolution of a host nameto one or more routers.The suggested method is to define a cloud domain witha wildcard DNS entry pointing to one or more virtual IP (VIP)addresses backed by multiple router instances.Routes using names and addresses outside the cloud domain requireconfiguration of individual DNS entries.\nWhen there are fewer VIP addresses than routers, the routers correspondingto the number of addresses are active and the rest are passive.A passive router is also known as a hot-standby router.For example, with two VIP addresses and three routers,you have an \"active-active-passive\" configuration.Seefor more information on router VIP configuration.\nRoutes can beshardedamong the set of routers.Administrators can set up sharding on a cluster-wide basisand users can set up sharding for the namespace in their project.Sharding allows the operator to define multiple router groups.Each router in the group serves only a subset of traffic.\nAtomic Registry routers provide external host name mapping and load balancingof service end points over protocols thatpass distinguishing information directly to the router; the host namemust be present in the protocol in order for the router to determinewhere to send it.\nRouter plug-ins assume they can bind to host ports 80 (HTTP)and 443 (HTTPS), by default.This means that routers must be placed on nodeswhere those ports are not otherwise in use.Alternatively, a router can be configured to listenon other ports by setting the ROUTER_SERVICE_HTTP_PORTand ROUTER_SERVICE_HTTPS_PORT environment variables.\nBecause a router binds to ports on the host node,only one router listening on those ports can be on each nodeif the router uses host networking (the default).Cluster networking is configured such that all routerscan access all pods in the cluster.\nRouters support the following protocols:\nWebSocket traffic uses the same route conventions and supports the same TLStermination types as other traffic.\nFor a secure connection to be established, a cipher common to theclient and server must be negotiated. As time goes on, new, more secure ciphersbecome available and are integrated into client software. As older clientsbecome obsolete, the older, less secure ciphers can be dropped. By default, therouter supports a broad range of commonly available clients. The router can beconfigured to use a selected set of ciphers that support desired clients anddo not include the less secure ciphers.\nA template router is a type of router that provides certain infrastructureinformation to the underlying router implementation, such as:\nThe following router plug-ins are provided and supported in Atomic Registry.\nThe HAProxy template router implementation is the reference implementation for atemplate router plug-in. It uses therepository to run an HAProxy instance alongside the template router plug-in.\nThe following diagram illustrates how data flows from the master through theplug-in and finally into an HAProxy configuration:\nSticky sessions attempts to ensure that user traffic is directed to the same podin each session, and improves application user experience by caching data. Forexample, for a cluster with five back-end pods, and two load-balanced routers,you can ensure that the same pod receives the web traffic from the same webbrowser regardless of the router that handles it.\nWhile placing returning traffic onto the same pod cannot be guaranteed, you canuse HTTP headers to set a cookie determining the pod used by the lastconnection, in an attempt to steer traffic to the same pod. When the applicationuser hits the router the browser re-sends the cookie and knows where to send thetraffic.\nAs a cluster administrator, you can turn off the stickiness for passthroughroutes separately from other connections, or turn off stickiness entirely.\nBy default, if the default HAProxy router is using passthrough routes, stickysessions are implemented using the balance source directive in the underlyingrouter configuration, which balances based on the source IP. Other types ofroutes use balance leastconn by default. If a cookie is present, it is readand sent to the same pod it went to last time if the pod still exists. If nocookie is present, because this is a new session, stickiness is disabled, orit\u2019s using a passthrough route, the router picks a pod.\nCookies cannot be set on passthrough routes, because the HTTP traffic cannot beseen. Instead, a number is calculated based on the source IP address, whichdetermines the back-end.\nIf back-ends change, the traffic could head to the wrong server, making it lesssticky, and if you are using a load-balancer (which hides the source IP) thesame number is set for all connections and traffic is sent to the same pod.\nIn addition, the templaterouter plug-in provides the service name and namespace to the underlyingimplementation. This can be used for more advanced configuration such asimplementing stick-tables that synchronize between a set of peers.\nSpecific configuration for this router implementation is stored in thehaproxy-config.template file located in the /var/lib/haproxy/confdirectory of the router container.\nThe balance source directive does not distinguish between external client IPaddresses; because of the NAT configuration, the originating IP address(HAProxy remote) is the same. Unless the HAProxy router is running withhostNetwork: true, all external clients will be routed to a single pod.\nFor all the items outlined in this section, you can set environmentvariables on the deployment config for the router to alter its configuration, or use the oc set env command:\nFor example:\nIf you want to run multiple routers on the same machine, you must change theports that the router is listening on, ROUTER_SERVICE_SNI_PORT andROUTER_SERVICE_NO_SNI_PORT. These ports can be anything you want as long asthey are unique on the machine. These ports will not be exposed externally.\nTimeUnits are represented by a number followed by the unit: us*(microseconds), ms (milliseconds, default), s (seconds), m (minutes), h*(hours), d (days).\nThe regular expression is: [1-9][0-9]*(us\\|ms\\|s\\|m\\|h\\|d)\nBy default, when a host does not resolve to a route in a HTTPS or TLS SNIrequest, the default certificate is returned to the caller as part of the 503response. This exposes the default certificate and can pose security concernsbecause the wrong certificate is served for a site. The HAProxy strict-snioption to bind suppresses use of the default certificate.\nThe ROUTER_STRICT_SNI environment variable controls bind processing. When setto true or TRUE, strict-sni is added to the HAProxy bind. The defaultsetting is false.\nThe option can be set when the router is created or added later.\nThis sets ROUTER_STRICT_SNI=true.\nEach client (for example, Chrome 30, or Java8) includes a suite of ciphers usedto securely connect with the router. The router must have at least one of theciphers for the connection to be complete:\nSee the Security/ServerSide TLS reference guide for more information.\nThe router defaults to the intermediate profile. You can select a differentprofile using the --ciphers option when creating a route, or by changingthe ROUTER_CIPHERS environment variable with the values modern,intermediate, or old for an existing router. Alternatively, a set of \":\"separated ciphers can be provided. The ciphers must be from the set displayedby:\nIn order for services to be exposed externally, an Atomic Registry route allowsyou to associate a service with an externally-reachable host name. This edgehost name is then used to route traffic to the service.\nWhen multiple routes from different namespaces claim the same host,the oldest route wins and claims it for the namespace. If additionalroutes with different path fields are defined in the same namespace,those paths are added. If multiple routes with the same path areused, the oldest takes priority.\nA consequence of this behavior is that if you have two routes for a host name: anolder one and a newer one. If someone else has a route for the same host namethat they created between when you created the other two routes, then if youdelete your older route, your claim to the host name will no longer be in effect.The other namespace now claims the host name and your claim is lost.\nIf a host name is not provided as part of the route definition, thenAtomic Registry automatically generates one for you. The generated host nameis of the form:\nThe following example shows the Atomic Registry-generated host name for theabove configuration of a route without a host added to a namespacemynamespace:\nA cluster administrator can alsofor their environment.\nRoutes can be either secured or unsecured. Secure routes provide the ability touse several types of TLS termination to serve certificates to the client.Routers support edge,passthrough, andre-encryption termination.\nUnsecured routes are simplest to configure, as they require no keyor certificates, but secured routes offer security for connections toremain private.\nA secured route is one that specifies the TLS termination of the route.The available types of termination are describedbelow.\nPath based routes specify a path component that can be compared againsta URL (which requires that the traffic for the route be HTTP based) suchthat multiple routes can be served using the same host name, each with adifferent path. Routers should match routes based on the most specificpath to the least; however, this depends on the router implementation. Thefollowing table shows example routes and their accessibility:\nPath-based routing is not available when using passthrough TLS, asthe router does not terminate TLS in that case and cannot read the contentsof the request.\nSecured routes specify the TLS termination of the route and, optionally,provide a key and certificate(s).\nTLS termination in Atomic Registry relies onSNI for servingcustom certificates. Any non-SNI traffic received on port 443 is handled withTLS termination and a default certificate (which may not match the requestedhost name, resulting in validation errors).\nSecured routes can use any of the following three types of secure TLStermination.\nEdge Termination\nWith edge termination, TLS termination occurs at the router, prior to proxyingtraffic to its destination. TLS certificates are served by the front end of therouter, so they must be configured into the route, otherwise thewill be used for TLS termination.\nBecause TLS is terminated at the router, connections from the router tothe endpoints over the internal network are not encrypted.\nEdge-terminated routes can specify an insecureEdgeTerminationPolicy thatenables traffic on insecure schemes (HTTP) to be disabled, allowed orredirected.The allowed values for insecureEdgeTerminationPolicy are:  None or empty (for disabled), Allow or Redirect.The default insecureEdgeTerminationPolicy is to disable traffic on theinsecure scheme. A common use case is to allow content to be served via asecure scheme but serve the assets (example images, stylesheets andjavascript) via the insecure scheme.\nPassthrough Termination\nWith passthrough termination, encrypted traffic is sent straight to thedestination without the router providing TLS termination. Therefore nokey or certificate is required.\nThe destination pod is responsible for serving certificates for thetraffic at the endpoint. This is currently the only method that can supportrequiring client certificates (also known as two-way authentication).\nPassthrough routes can also have an insecureEdgeTerminationPolicy. The onlyvalid values are None (or empty, for disabled) or Redirect.\nRe-encryption Termination\nRe-encryption is a variation on edge termination where the router terminatesTLS with a certificate, then re-encrypts its connection to the endpoint whichmay have a different certificate. Therefore the full path of the connectionis encrypted, even over the internal network. The router uses healthchecks to determine the authenticity of the host.\nRe-encrypt routes can have an insecureEdgeTerminationPolicy with all of thesame values as edge-terminated routes.\nIn Atomic Registry, each route can have any number oflabelsin its metadata field.A router uses selectors (also known as a selection expression)to select a subset of routes from the entire pool of routes to serve.A selection expression can also involvelabels on the route\u2019s namespace.The selected routes form a router shard.\nThis design supports traditional sharding as well as overlapped sharding.In traditional sharding, the selection results in no overlapping setsand a route belongs to exactly one shard.In overlapped sharding, the selection results in overlapping setsand a route can belong to many different shards.For example, a single route may belong to a SLA=high shard(but not SLA=medium or SLA=low shards),as well as a geo=west shard(but not a geo=east shard).\nAnother example of overlapped sharding is aset of routers that select based on namespace of the route:\nBoth router-2 and router-3 serve routes that are in thenamespaces Q*, R*, S*, T*.To change this example from overlapped to traditional sharding,we could change the selection of router-2 to K*\u2009\u2014\u2009P*,which would eliminate the overlap.\nWhen routers are sharded,a given route is bound to zero or more routers in the group.The route binding ensures uniqueness of the route across the shard.Uniqueness allows secure and non-secure versions of the same route to existwithin a single shard.This implies that routes now have a visible life cyclethat moves from created to bound to active.\nIn the sharded environment the first route to hit the shardreserves the right to exist there indefinitely, even across restarts.\nDuring a green/blue deployment a route may be be selected in multiple routers.An Atomic Registry application administrator may wish to bleed traffic from oneversion of the application to another and then turn off the old version.\nSharding can be done by the administrator at a cluster level and by the userat a project/namespace level.When namespace labels are used, the service account for the routermust have cluster-reader permission to permit therouter to access the labels in the namespace.\nFor two or more routes that claim the same host name, the resolution orderis based on the age of the route and the oldest route would win the claim tothat host.In the case of sharded routers, routes are selected based on their labelsmatching the router\u2019s selection criteria. There is no consistent way todetermine when labels are added to a route. So if an older route claimingan existing host name is \"re-labelled\" to match the router\u2019s selectioncriteria, it will replace the existing route based on the above mentionedresolution order (oldest route wins).\nUsing environment variables as defined in ConfigurationParameters, a router can set the default options for all the routes it exposes.An individual route can override some of these defaults by providing specificconfigurations in its annotations.\nRoute Annotations\nFor all the items outlined in this section, you can set annotations on theroute definition for the route to alter its configuration\nSetting a server-side timeout value for passthrough routes too low can causeWebSocket connections to timeout frequently on that route.\nYou can restrict access to a route to a select set of IP addresses by adding thehaproxy.router.openshift.io/ip_whitelist annotation on the route. Thewhitelist is a space-separated list of IP addresses and/or CIDRs for theapproved source addresses. Requests from IP addresses that are not in thewhitelist are dropped.\nSome examples:\nWhen editing a route, add the following annotation to define the desiredsource IP\u2019s. Alternatively, use oc annotate route <name>.\nAllow only one specific IP address:\nAllow several IP addresses:\nAllow an IP CIDR network:\nAllow mixed IP addresses and IP CIDR networks:\nA wildcard policy allows a user to define a route that covers all hosts within adomain (when the router is configured to allow it). A route can specify awildcard policy as part of its configuration using the wildcardPolicy field.Any routers run with a policy allowing wildcard routes will expose the routeappropriately based on the wildcard policy.\nThe route status field is only set by routers. If changes are made to a routeso that a router no longer serves a specific route, the status becomes stale.The routers do not clear the route status field. To remove the stale entriesin the route status, use theclear-route-statusscript.\nA router can be configured to deny or allow a specific subset of domains fromthe host names in a route using the ROUTER_DENIED_DOMAINS andROUTER_ALLOWED_DOMAINS environment variables.\nThe domains in the list of denied domains take precedence over the list ofallowed domains. Meaning Atomic Registry first checks the deny list (ifapplicable), and if the host name is not in the list of denied domains, it thenchecks the list of allowed domains. However, the list of allowed domains is morerestrictive, and ensures that the router only admits routes with hosts thatbelong to that list.\nFor example, to deny the [*.]open.header.test, [*.]openshift.org and[*.]block.it routes for the myrouter route:\nThis means that myrouter will admit the following based on the route\u2019s name:\nHowever, myrouter will deny the following:\nAlternatively, to block any routes where the host name is not set to [*.]stickshift.org or [*.]kates.net:\nThis means that the myrouter router will admit:\nHowever, myrouter will deny the following:\nTo implement both scenarios, run:\nThis will allow any routes where the host name is set to [*.]openshift.org or[*.]kates.net, and not allow any routes where the host name is set to[*.]ops.openshift.org or [*.]metrics.kates.net.\nTherefore, the following will be denied:\nHowever, the following will be allowed:\nHosts and subdomains are owned by the namespace of the route that firstmakes the claim. Other routes created in the namespace can make claims onthe subdomain. All other namespaces are prevented from making claims onthe claimed hosts and subdomains. The namespace that owns the host alsoowns all paths associated with the host, for example www.abc.xyz/path1.\nFor example, if the host www.abc.xyz is not claimed by any route.Creating route r1 with host www.abc.xyz in namespace ns1 makesnamespace ns1 the owner of host www.abc.xyz and subdomain abc.xyzfor wildcard routes. If another namespace, ns2, tries to create a routewith say a different path www.abc.xyz/path1/path2, it would failbecause a route in another namespace (ns1 in this case) owns that host.\nBy disabling the namespace ownership rules, you can disable these restrictionsand allow hosts (and subdomains) to be claimed across namespaces.\nIf you decide to disable the namespace ownership checks in your router,be aware that this allows end users to claim ownership of hostsacross namespaces. While this change can be desirable in certaindevelopment environments, use this feature with caution in productionenvironments, and ensure that your cluster policy has locked down untrusted endusers from creating routes.\nFor example, with ROUTER_DISABLE_NAMESPACE_OWNERSHIP_CHECK=true, ifnamespace ns1 creates the oldest route r1  www.abc.xyz,  it owns onlythe hostname (+ path).  Another namespace can create a wildcard routeeven though it does not have the oldest route in that subdomain (abc.xyz)and we could potentially have other namespaces claiming othernon-wildcard overlapping hosts (for example, foo.abc.xyz, bar.abc.xyz,baz.abc.xyz) and their claims would be granted.\nAny other namespace (for example, ns2) can now createa route r2  www.abc.xyz/p1/p2,  and it would be admitted.  Similarlyanother namespace (ns3) can also create a route  wildthing.abc.xyzwith a subdomain wildcard policy and it can own the wildcard.\nAs this example demonstrates, the policy ROUTER_DISABLE_NAMESPACE_OWNERSHIP_CHECK=true is morelax and allows claims across namespaces.  The only time the router wouldreject a route with the namespace ownership disabled is if the host+pathis already claimed.\nFor example, if a new route rx tries to claim www.abc.xyz/p1/p2, itwould be rejected as route r2 owns that host+path combination.  This is true whether route rxis in the same namespace or other namespace since the exact host+path is already claimed.\nThis feature can be set during router creation or by setting an environmentvariable in the router\u2019s deployment configuration.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/index.html", "title": "Atomic Registry Latest | Architecture | Overview", "content": "Atomic Registry is based on OpenShift technology which features anembedded registry based on the upstreamDocker Distribution library. Atomic Registry provides thefollowing capabilities:\nAtomic Registry has a microservices-based architecture of smaller, decoupled unitsthat work together. It runs on top of aKubernetescluster, with data about the objects stored inetcd, areliable clustered key-value store. Those services are broken down by function:\nUsers make calls to the REST API to change the state of the system. Controllersuse the REST API to read the user\u2019s desired state, and then try to bring theother parts of the system into sync. For example, when a user requests abuild they create a\"build\" object. The build controller sees that a new build has been created, andruns a process on the cluster to perform that build. When the build completes,the controller updates the build object via the REST API and the user sees thattheir build is complete.\nTo make this possible, controllers leverage a reliable stream of changes to thesystem to sync their view of the system with what users are doing. This eventstream pushes changes from etcd to the REST API and then to the controllers assoon as changes occur, so changes can ripple out through the system very quicklyand efficiently. However, since failures can occur at any time, the controllersmust also be able to get the latest state of the system at startup, and confirmthat everything is in the right state. This resynchronization is important,because it means that even if something goes wrong, then the operator canrestart the affected components, and the system double checks everything beforecontinuing. The system should eventually converge to the user\u2019s intent, sincethe controllers can always bring the system into sync.\nThe Atomic Registry and Kubernetes APIsauthenticate users who presentcredentials, and then authorizethem based on their role. Both developers and administrators can beauthenticated via a number of means, primarilyOAuth tokens and SSLcertificate authorization.\nDevelopers (clients of the system) typically make REST API calls from aclient program like oc or to theweb console via their browser,and use OAuth bearer tokens for most communications. Infrastructure components(like nodes) use client certificates generated by the system that contain theiridentities. Infrastructure components that run in containers use a tokenassociated with their service accountto connect to the API.\nAuthorization is handled in the Atomic Registry policy engine, which definesactions like \"create pod\" or \"list services\" and groups them into roles in apolicy document. Roles are bound to users or groups by the user or groupidentifier. When a user or service account attempts an action, the policy enginechecks for one or more of the roles assigned to the user (e.g., clusteradministrator or administrator of the current project) before allowing it tocontinue.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/infrastructure_components/image_registry.html", "title": "Atomic Registry Latest | Architecture | Infrastructure Components | Container Registry", "content": "Atomic Registry embeds the upstream Docker Distributionlibrary to maintain image format compatibility with the Docker service. New imagerepositories may be created on the fly. Whenever a new image is pushed to theintegrated registry, the registry notifies Atomic Registry API about the newimage, passing along all the information about it, such as the namespace, name,and image metadata.\nAtomic Registry can reference images from external, third-party registries. Duringimport Atomic Registry will fetch tags from the remote registry and watch theremote image tag for changes.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/infrastructure_components/kubernetes_infrastructure.html", "title": "Atomic Registry Latest | Architecture | Infrastructure Components | Kubernetes Infrastructure", "content": "Atomic Registry is based on OpenShift and Kubernetes. The typical Atomic Registrydeployment is much simpler than a deployment of OpenShift. The followingis provided as a reference, particularly for understanding requirements for ahighly available deployment.\nThe master is the host or hosts that contain the master components, includingthe API server, controller manager server, and etcd. The master managesnodes in its Kubernetes cluster and schedulespods to run on nodes.\nOptional, used when configuringhighly-available masters with the nativemethod to balance load between API master endpoints.\nWhile in a single master configuration, the availability of running applicationsremains if the master or any of its services fail. However, failure of masterservices reduces the ability of the system to respond to application failures orcreation of new applications. You can optionally configure your masters for highavailability (HA) to ensure that the cluster has no single point of failure.\nTo mitigate concerns about availability of the master, two activities arerecommended:\nWhen using the native HA method with HAProxy, master components have thefollowing availability:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/architecture/infrastructure_components/web_console.html", "title": "Atomic Registry Latest | Architecture | Infrastructure Components | Web Console", "content": "The Atomic Registry web console is a user interface accessible from a web browser.Developers can use the web console to visualize, browse, and manage the contentsof projects.\nJavaScript must be enabled to use the web console. For the best experience, usea web browser that supportsWebSockets.\nThe web console is based on the CockpitProject. It is deployed as a service using an Atomic Registry template. The webconsole is an optional component.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/cli_reference/get_started_cli.html", "title": "Atomic Registry Latest | CLI Reference | Get Started with the CLI", "content": "The Atomic Registry CLI exposes commands for managing your applications, as well aslower level tools to interact with each component of your system. This topicguides you through getting started with the CLI, including installation andlogging in to create your first project.\nInstallation options for the CLI vary depending on your operating system.\nThe CLI for Windows is provided as a zip archive; you can download it fromthe Releases page of the OpenShiftOrigin source repository on GitHub:\nDownload the CLI from GitHub\nThen, unzip the archive with a ZIP program and move the oc binary to adirectory on your PATH. To check your PATH, open the Command Prompt and run:\nThe CLI for Mac OS X is provided as a zip archive; you can download it fromthe Releases page of the OpenShiftOrigin source repository on GitHub:\nDownload the CLI from GitHub\nThen,unzip the archive with a ZIP programand move the oc binary to a directory on your PATH.To check your PATH, open a Terminal window and run:\nAlternatively, Mac OS X users can install the CLI usingHomebrew:\nThe CLI for Linux is provided as a tar.gz archive; you can download it fromthe Releases page of the OpenShiftOrigin source repository on GitHub:\nDownload the CLI from GitHub\nThen, unpack the archive and move the oc binary to a directory on your PATH.To check your path, run:\nTo unpack the archive:\nThe oc login command is the best way to initially set up the CLI,and it serves as the entry point for most users. The interactive flow helps youestablish a session to an Atomic Registry server with the provided credentials. Theinformation is automatically saved in a CLIconfiguration file that is then used for subsequent commands.\nThe following example shows the interactive setup and login using the oclogin command:\nWhen you have completed the CLI configuration, subsequent commands use theconfiguration file for the server, session token, and project information.\nYou can log out of CLI using the oc logout command:\nIf you log in after creating or being granted access to a project, a project youhave access to is automatically set as the current default, untilswitching to another one:\nAdditional options are also available forthe oc login command.\nIf you have access to administrator credentials but are no longer logged in asthe defaultsystem user system:admin, you can log back in as this user at any time aslong as the credentials are still present in yourCLIconfiguration file. The following command logs in and switches to the defaultproject:\nA CLI configuration file permanently stores oc options and contains a seriesof authenticationmechanisms and Atomic Registry server connection information associated withnicknames.\nAs described in the previous section, the oc login command automaticallycreates and manages CLI configuration files. All information gathered by thecommand is stored in a configuration file located in~/.kube/config. The current CLI configuration can be viewed using the following command:\nCLI configuration files can be used to setupmultiple CLI profiles using various Atomic Registry servers, namespaces, and users sothat you can switch easily between them. The CLI can support multipleconfiguration files; they are loaded at runtime and merged together along withany override options specified from the command line.\nMost oc commands run in the context of aproject. The oc loginselects a default project during initial setup tobe used with subsequent commands. Use the following command to display theproject currently in use:\nIf you have access to multiple projects, use the following syntax to switch to aparticular project by specifying the project name:\nFor example:\nAfter you have logged in,explore administrator CLI operations.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/cli_reference/index.html", "title": "Atomic Registry Latest | CLI Reference | Overview", "content": "With the Atomic Registry command line interface (CLI), you can manageAtomic Registry projectsfrom a terminal. The CLI is ideal in situations where you are:\nThe CLI is available using the oc command:\nSee Get Started with the CLI forinstallation and setup instructions.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/cli_reference/manage_cli_profiles.html", "title": "Atomic Registry Latest | CLI Reference | Managing CLI Profiles", "content": "A CLI configuration file allows you to configure different profiles, orcontexts, for use with the Atomic Registry CLI. A contextconsists of userauthentication andAtomic Registry server information associated with anickname.\nContexts allow you to easily switch between multiple users across multipleAtomic Registry servers, or clusters, when using issuing CLI operations. Nicknamesmake managing CLI configuration easier by providing short-hand references tocontexts, user credentials, and cluster details.\nAfter logging in with the CLI for the first time,Atomic Registry creates a ~/.kube/config file if one does notalready exist. As more authentication and connection details are provided to theCLI, either automatically during an oc login operation or bysetting them explicitly, the updatedinformation is stored in the configuration file:\nThe CLI can support multiple configuration files; they areloaded at runtime and merged together alongwith any override options specified from the command line.\nAfter you are logged in, you can use the oc status command or the ocproject command to verify your current working environment:\nTo log in using any other combination of user credentials and cluster details,run the oc login command again and supply the relevant information during theinteractive process. A context is constructed based on the supplied informationif one does not already exist.\nIf you are already logged in and want to switch to another project the currentuser already has access to, use the oc project command and supply the name ofthe project:\nAt any time, you can use the oc config view command to view your current,full CLI configuration, as seen in the above output.\nAdditional CLI configuration commands are also available for moreadvanced usage.\nIf you have access to administrator credentials but are no longer logged in asthe defaultsystem user system:admin, you can log back in as this user at any time aslong as the credentials are still present in yourCLIconfiguration file. The following command logs in and switches to the defaultproject:\nThis section covers more advanced usage of CLI configurations. In mostsituations, you can simply use the oc login and oc project commands to login and switch between contexts and projects.\nIf you want to manually configure your CLI configuration files, you can use theoc config command instead of modifying the files themselves. The oc configcommand includes a number of helpful subcommands for this purpose:\nset-credentials\nSets a user entry in the CLI configuration file. If the referenced usernickname already exists, the specified information is merged in.\nset-cluster\nSets a cluster entry in the CLI configuration file. If the referenced clusternickname already exists, the specified information is merged in.\nset-context\nSets a context entry in the CLI configuration file. If the referenced contextnickname already exists, the specified information is merged in.\nuse-context\nSets the current context using the specified context nickname.\nset\nSets an individual value in the the CLI configuration file.\nThe <property_name> is a dot-delimited name where each token represents eitheran attribute name or a map key. The <property_value> is the new value beingset.\nunset\nUnsets individual values in the CLI configuration file.\nThe <property_name> is a dot-delimited name where each token represents eitheran attribute name or a map key.\nview\nDisplays the merged CLI configuration currently in use.\nDisplays the result of the specified CLI configuration file.\nExample Usage \nConsider the following configuration workflow. First, set credentials for a usernickname alice that uses anaccesstoken:\nSet a cluster entry named openshift1:\nSet a context named alice that uses the alice user and theopenshift1 cluster:\nNow that the alice context has been created, switch to that context:\nSet the aliceproject namespace for the alice context:\nYou can now view the configuration that has been created:\nAll subsequent CLI operations will use the alice context, unless otherwisespecified by overriding CLI options or until the context is switched.\nWhen issuing CLI operations, the loading and merging order for the CLIconfiguration follows these rules:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/dev_guide/authentication.html", "title": "Atomic Registry Latest | User Guide | Authentication", "content": "When accessing theweb consolefrom a browser, you are automatically redirectedto a login page.\nReview thebrowser versionsand operating systems that can be used to access the web console.\nYou can provide your login credentials on this page to obtain a token to makeAPI calls. After logging in, you can navigate your projects using theweb console.\nYou can authenticate from the command line using the CLI command oc login.You can get started with the CLI byrunning this commandwithout any options:\nThe command\u2019s interactive flow helps you establish a session to an Atomic Registryserver with the provided credentials. If any information required to successfullylog in to an Atomic Registry server is not provided, the command prompts for userinput as required. Theconfigurationis automatically saved and is then used for every subsequent command.\nAll configuration options for the oc login command, listed in the oc login--help command output, are optional. The following example shows usage withsome common options:\nThe following table describes these common options:\nCLI configuration files allow you to easilymanage multiple CLI profiles.\nIf you have access to administrator credentials but are no longer logged in asthe defaultsystem user system:admin, you can log back in as this user at any time aslong as the credentials are still present in yourCLIconfiguration file. The following command logs in and switches to the defaultproject:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/dev_guide/index.html", "title": "Atomic Registry Latest | User Guide | Overview", "content": "This guide helps image developers set up and configure a workstation todevelop images working with the Atomic Registry environment using the web consoleand command-line interface (CLI). This guide provides detailed instructions andexamples to help image developers:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/dev_guide/managing_images.html", "title": "Atomic Registry Latest | User Guide | Managing Images", "content": "Animagestream comprises any number ofcontainerimages identified by tags. It presents a single virtual view of related images,similar to a Docker image repository.\nBy watching an image stream, builds and deployments can receive notificationswhen new images are added or modified and react by performing a build ordeployment, respectively.\nThere are many ways you can interact with images and set up image streams,depending on where the images' registries are located, any authenticationrequirements around those registries, and how you want your builds anddeployments to behave. The following sections cover a range of these topics.\nBefore working with Atomic Registry image streams and their tags, it will helpto first understand image tags in the context of container images generally.\nContainer images can have names added to them that make it more intuitive to determinewhat they contain, called a tag. Using a tag to specify the version of what is containedin the image is a common use case. If you have an image named ruby, you couldhave a tag named 2.0 for 2.0 version of Ruby, and another named latest toindicate literally the latest built image in that repository overall.\nWhen interacting directly with images using the docker CLI, the docker tagcommand can add tags, which essentially adds an alias to an image that canconsist of several parts. Those parts can include:\nThe <user_name> part in the above could also refer to aproject ornamespaceif the image is being stored in an Atomic Registry environment with an internalregistry (the OpenShift Container Registry).\nAtomic Registry provides the oc tag command, which is similar to the dockertag command, but operates on image streams instead of directly on images.\nSee Red Hat Enterprise Linux 7\u2019sGettingStarted with Containers documentation for more about tagging images directlyusing the docker CLI.\nKeeping in mind that an image stream in Atomic Registry comprises zero or morecontainer images identified by tags, you can add tags to an image stream using theoc tag command:\nFor example, to configure the ruby image\u2019s latest tag to always refer to thecurrent image for the tag 2.0:\nThere are different types of tags available. The default behavior uses apermanent tag, which points to a specific image in time; even when the sourcechanges, it will not reflect in the destination tag.\nA tracking tag means the destination tag\u2019s metadata will be imported duringthe import. To ensure the destination tag is updated whenever the source tagchanges, use the --alias=true flag:\nUse a tracking tag for creating permanent aliases (for example, latest orstable). The tag works correctly only within a single image stream. Tryingto create a cross-image-stream alias will produce an error.\nYou can also add the --scheduled=true flag to have the destination tag berefreshed (i.e., re-imported) periodically. The period is configured globally atsystem level. See Importing Tag and ImageMetadata for more details.\nIf you want to instruct Docker to always fetch the tagged image from theintegrated registry, use --reference-policy=local. The registry uses thepull-through featureto serve the image to the client. By default, the image blobs aremirrored locally by the registry. As a result, they are available for a  shortertime the next time they are needed. The flag also allows for pulling frominsecure registries without a need to supply --insecure-registry to the Dockerdaemon if the image stream has an insecure annotationor the tag has an insecure import policy.\nImages evolve over time and their tags reflect this. An image tag always pointsto the latest image built.\nIf there is too much information embedded in a tag name (for example,v2.0.1-may-2016), the tag will point to just one revision of an image and willnever be updated. Using default image pruning options, such an image will neverbe removed.\nInstead, if the tag is named v2.0, more image revisions are more likely. Thisresults in longertag history and, therefore, the image pruner will more likely remove old and unused images.\nAlthough tag naming convention is up to you, here are a few examples in theformat <image_name>:<image_tag>:\nIf you require dates in tag names, periodically inspect old and unsupportedimages and istags and remove them. Otherwise, you might experience increasingresource usage caused by old images.\nTo remove a tag completely from an image stream run:\nor:\nImages can be referenced in image streams using the following reference types:\nThe <id> is an immutable identifier for a specific image, also called adigest.\nWhen no tag is specified, it is assumed the latest tag will be used.\nYou can also reference a third-party registry:\nOr an image with a digest:\nWhen viewing example image stream definitions, such as theexampleCentOS image streams, you may notice they contain definitions ofImageStreamTag and references to DockerImage, but nothing related toImageStreamImage.\nThis is because the ImageStreamImage objects are automatically created inAtomic Registry whenever you import or tag an image into the image stream. Youshould never have to explicitly define an ImageStreamImage object in anyimage stream definition that you use to create image streams.\nYou can view an image\u2019s object definition by retrieving an ImageStreamImagedefinition using the image stream name and ID:\nYou can find valid <id> values for a given image stream by running:\nFor example, from the ruby image stream asking for the ImageStreamImagewith the name and ID of ruby@3a335d7:\nYou can access Atomic Registry\u2019s internal registry directly to push or pullimages. For example, this could be helpful if you wanted tocreate an imagestream by manually pushing an image, or just to docker pull an imagedirectly.\nThe internal registry authenticates using the sametokensas the Atomic Registry API. To perform a docker login against the internalregistry, you can choose any user name and email, but the password must be avalid Atomic Registry token.\nTo log into the internal registry:\nContact your cluster administrator if you do not know the registry IP or hostname and port to use.\nIn order to pull an image, the authenticated user must have get rights on therequested imagestreams/layers. In order to push an image, the authenticateduser must have update rights on the requested imagestreams/layers.\nBy default, all service accounts in a project have rights to pull any image inthe same project, and the builder service account has rights to push any imagein the same project.\nAn image stream can be configured to import tag and image metadata from an imagerepository in an external Docker image registry. You can do this using a fewdifferent methods.\nFor example:\nYou can also add the --all flag to import all tags for the image instead ofjust latest.\nThen create the object:\nWhen you create an image stream that references an image in an external Dockerregistry, Atomic Registry communicates with the external registry within a shortamount of time to get up to date information about the image.\nAfter the tag and image metadata is synchronized, the image stream object wouldlook similar to the following:\nYou can set a tag to query external registries at a scheduled interval tosynchronize tag and image metadata by setting the --scheduled=true flag withthe oc tag command as mentioned in Adding Tags to ImageStreams.\nAlternatively, you can set importPolicy.scheduled to true in the tag\u2019sdefinition:\nAn image stream can be configured to import tag and image metadata from insecureimage registries, such as those signed with a self-signed certificate or usingplain HTTP instead of HTTPS.\nTo configure this, add the openshift.io/image.insecureRepository annotationand set it to true. This setting bypasses certificate validation whenconnecting to the registry:\nThis option instructs integrated registry to fall back to an insecure transportfor any external image tagged in the image stream when serving it, which isdangerous. If possible, avoid this risk bymarking just an istag as insecure.\nThe above annotation applies to all images and tags of a particularImageStream. For a finer-grained control, policies may be set onistags.Set importPolicy.insecure in the tag\u2019s definition to true to allow afall-back to insecure transport just for images under this tag.\nThe fall-back to insecure transport for an image under particular istag willbe enabled either when the image stream is annotated as insecure or the istaghas insecure import policy. The importPolicy.insecure` set to false can notoverride the image stream annotation.\nThe Reference Policy allows you to specify where the image consumers will pullfrom. It is only applicable to remote images (those imported from externalregistries). There are two options to choose from, Local and Source.\nThe Source policy instructs clients to pull directly from the source registryof the image. The integrated registry is not involved unless the image ismanaged by the cluster. (It is not an external image.) This is the defaultpolicy.\nThe Local policy instructs clients to always pull from the integratedregistry. This is useful if you want to pull from external insecure registrieswithout modifying Docker daemon settings.\nThepull-through featureof the registry serves the remote image to the client. Additionally, all theblobs are mirrored for faster access later.\nYou can set the policy in a specification of image stream tag asreferencePolicy.type.\nAn image stream can also be automatically created by manually pushing an imageto the internal registry. This is only possible when using an Atomic Registryinternal registry.\nBefore performing this procedure, the following must be satisfied:\nTo create an image stream by manually pushing an image:\nImage streams for S2I builders that are displayed in the managementconsole\u2019s catalog page require additional metadata to provide the bestexperience for end users.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/dev_guide/service_accounts.html", "title": "Atomic Registry Latest | User Guide | Service Accounts", "content": "When a person uses the Atomic Registry CLI or web console, their API tokenauthenticates them to the OpenShift API. However, when a regular user\u2019scredentials are not available, it is common for components to make API callsindependently.\nService accounts provide a flexible way to control API access without sharing aregular user\u2019s credentials.\nEvery service account has an associated user name that can be granted roles,just like a regular user. The user name is derived from its project and name:\nFor example, to add the view role to the robot service account in thetop-secret project:\nEvery service account is also a member of two groups:\nFor example, to allow all service accounts in all projects to view resources inthe top-secret project:\nTo allow all service accounts in the managers project to edit resources in thetop-secret project:\nThree service accounts are automatically created in every project:\nAll service accounts in a project are given the system:image-puller role,which allows pulling images from any image stream in the project using theinternal Docker registry.\nAs soon as a service account is created, two secrets are automatically added toit:\nThese can be seen by describing the service account:\nThe system ensures that service accounts always have an API token and internalDocker registry credentials.\nThe generated API token and Docker registry credentials do not expire, but theycan be revoked by deleting the secret. When the secret is deleted, a new one isautomatically generated to take its place.\nThe same token can be distributed to external applications that need toauthenticate to the API.\nUse the following syntax to to view a service account\u2019s API token:\nFor example:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/adding_hosts_to_existing_cluster.html", "title": "Atomic Registry Latest | Installation and Configuration | Adding Hosts to an Existing Cluster", "content": "Depending on how your Atomic Registry cluster was installed, you can add newhosts (either nodes or masters) to your installation by using the install toolfor quick installations, or by using the scaleup.yml playbook for advancedinstallations.\nIf you used the quick install tool to install your Atomic Registry cluster, youcan use the quick install tool to add a new node host to your existing cluster.\nCurrently, you can not use the quick installer tool to add new master hosts. Youmust use theadvancedinstallation method to do so.\nIf you used the installer in eitherinteractive orunattended mode, you can re-run theinstallation as long as you have aninstallation configurationfile at ~/.config/openshift/installer.cfg.yml (or specify a differentlocation with the -c option).\nThe recommended maximum number of nodes is 300.\nTo add nodes to your installation:\nChoose (y) and follow the on-screen instructions to complete your desired task.\nIf you installed using the advanced install, you can add new hosts to yourcluster by running the scaleup.yml playbook. This playbook queries themaster, generates and distributes new certificates for the new hosts, then runsthe configuration playbooks on the new hosts only. Before running thescaleup.yml playbook, complete all prerequisitehostpreparation steps.\nYou must have an existing inventory file (for example, /etc/ansible/hosts)that is representative of your current cluster configuration in order to run thescaleup.yml playbook.\nThe recommended maximum number of nodes is 300.\nTo add a host to an existing cluster:\nFor example, to add a new node host, add new_nodes:\nTo add new master hosts, add new_masters.\nSeeConfiguringHost Variables for more options.\nWhen adding new masters, hosts added to the [new_masters] section must also beadded to the [new_nodes] section. This ensures the new master host is part ofthe OpenShift SDN.\nMasters are also automatically marked as unschedulable for pod placement by theinstaller.\nIf you label a master host with the region=infra label and have no otherdedicated infrastructure nodes, you must also explicitly mark the host asschedulable by adding openshift_schedulable=true to the entry. Otherwise, theregistry and router pods cannot be placed anywhere.\nFor additional nodes:\nFor additional masters:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/advanced_ldap_configuration/configuring_extended_ldap_attributes.html", "title": "Atomic Registry Latest | Installation and Configuration | Advanced LDAP Configuration | Configuring Extended LDAP Attributes", "content": "This topic builds uponSetting up SSSDfor LDAP Failover andConfiguringForm-Based Authentication and focuses on configuring extended LightweightDirectory Access Protocol (LDAP) attributes.\nYou need to ask System Security Services Daemon (SSSD) to look up attributes inLDAP that it normally does not care about for simple system-login use-cases. Inthe case of Atomic Registry, there is only one such attribute: email. So, you need to:\nNow that SSSD is set up and successfully serving extended attributes, configurethe web server to ask for them and to insert them in the correct places.\nTell Atomic Registry where to find these new attributes during login. To do so:\nYou should see their full name appear in the upper-right of thescreen. You can also verify with oc get identities -o yaml that both emailaddresses and full names are available.\nCurrently, Atomic Registry only saves these attributes to the user at the timeof the first login and does not update them again after that. So, while you aretesting (and only while testing), run oc delete users,identities --all toclear the identities out so you can log in again.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/advanced_ldap_configuration/configuring_form_based_authentication.html", "title": "Atomic Registry Latest | Installation and Configuration | Advanced LDAP Configuration | Configuring Form-Based Authentication", "content": "This topic builds uponSettingup SSSD for LDAP Failover and describes how to set up form-based authenticationfor signing into the Atomic Registry web console.\nThe Atomic Registry upstream repositories have a template for forms. Copy thatto your authenticating proxy on proxy.example.com:\nModify this .html file to change the logo icon and \"Welcome\" content for yourenvironment.\nTo intercept form-based authentication, install an Apache module:\nThis tells Apache to listen for POST requests on the/login-proxy/oauth/authorize and to pass the user name and password over tothe openshift PAM service.\nYou should be able to browse to https://openshift.example.com:8443 and useyour LDAP credentials to sign in via the login form.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/advanced_ldap_configuration/index.html", "title": "Atomic Registry Latest | Installation and Configuration | Advanced LDAP Configuration | Overview", "content": "Atomic Registry Advanced Lightweight Directory Access Protocol (LDAP)Configuration covers the following topics:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/advanced_ldap_configuration/sssd_for_ldap_failover.html", "title": "Atomic Registry Latest | Installation and Configuration | Advanced LDAP Configuration | Setting up SSSD for LDAP Failover", "content": "Atomic Registry provides anauthenticationprovider for use with Lightweight Directory Access Protocol (LDAP) setups, butit can only connect to a single LDAP server. This can be problematic if thatLDAP server becomes unavailable. System Security Services Daemon (SSSD) can beused to solve the issue.\nOriginally designed to manage local and remote authentication to the hostoperating system, SSSD can now be configured to provide identity,authentication, and authorization services to web services like Atomic Registry.SSSD provides advantages over the built-in LDAP provider, including the abilityto connect to any number of failover LDAP servers, as well as the ability tocache authentication attempts in case it can no longer reach any of thoseservers.\nThe setup for this configuration is advanced and requires a separateauthentication server (also called an authenticating proxy) forAtomic Registry to communicate with. This topic describes how to do this setupon a dedicated physical or virtual machine (VM), but the concepts are alsoapplicable to a setup in a container.\nThese VMs can be configured to run on the same system, but for the examples usedin this topic they are kept separate.\nThis phase generates certificate files that are valid for two years (or fiveyears for the certification authority (CA) certificate). This can be alteredwith the --expire-days and --signer-expire-days options, but for securityreasons, it is recommended to not make them greater than these values.\nAmong other things, this generates a /etc/origin/master/ca.{cert|key}. Usethis signing certificate to generate keys to use on the authenticating proxy.\nEnsure that any host names and interface IP addresses that need to access theproxy are listed. Otherwise, the HTTPS connection will fail.\nThis prevents malicious users from impersonating the proxy and sending fakeidentities.\nThis section guides you through the steps to authenticate the proxy setup.\nFrom openshift.example.com, securely copy the necessary certificates to theproxy machine:\nThis gives you the needed SSSD and the web server components.\nFor more advanced case, see theSystem-Level Authentication Guide\nIf you want to use SSSD to manage failover situations for LDAP, this can beconfigured by adding additional entries in /etc/sssd/sssd.conf on theldap_uri line. Systems enrolled with FreeIPA can automatically handlefailover using DNS SRV records.\nIf you do not want LDAP users to be able to log into this machine, it isrecommended to modify /etc/pam.d/system-auth and/etc/pam.d/password-auth to remove the lines containing pam_sss.so.\nYou need to set up Apache to communicate with SSSD. Create a PAM stack file foruse with Apache. To do so:\nThis configuration enables PAM (the pluggable authentication module) to usepam_sss.so to determine authentication and access control when anauthentication request is issued for the openshift stack.\nConfiguringForm-Based Authentication explains how to set up a graphical login using SSSDas well, but it requires the rest of this setup as a prerequisite.\nConfiguringForm-Based Authentication explains how to add the login-proxy block tosupport form authentication.\nThis section describes how to set up an Atomic Registry server from scratch inan \"all in one\" configuration.Master and NodeConfiguration provides more information on alternate configurations.\nModify the default configuration to use the new identity provider justcreated. To do so:\nConfiguringForm-Based Authentication explains how to add the login URL to support weblogins.\nConfiguringExtended LDAP Attributes explains how to add the email and full-nameattributes. Note that the full-name attributes are only stored to the databaseon the first login.\nIt should now be possible to log in with only valid LDAP credentials.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/certificate_customization.html", "title": "Atomic Registry Latest | Installation and Configuration | Configuring Custom Certificates", "content": "Administrators can configure custom serving certificates for the public hostnames of the Atomic Registry API andweb console.This can be done during anadvanced installation or configured after installation.\nDuringadvanced installations,custom certificates can be configured using theopenshift_master_named_certificates andopenshift_master_overwrite_named_certificates parameters, which areconfigurable in the inventory file. More details are available aboutconfiguring custom certificates with Ansible.\nIn the masterconfiguration file you can list the namedCertificates section in the assetConfig.servingInfo section so the custom certificate serves up for the web console, and in the servingInfo section so the custom certificate serves up for the CLI and other API calls. Multiple certificates can be configured this way and each certificate may be associated with multiple host names or wildcards.\nA default certificate must be configured in the servingInfo.certFile andservingInfo.keyFile configuration sections in addition tonamedCertificates.\nThe namedCertificates section should only be configured for the host nameassociated with the masterPublicURL, assetConfig.publicURL, andoauthConfig.assetPublicURL settings. Using a custom serving certificate forthe host name associated with the masterURL will result in TLS errors asinfrastructure components will attempt to contact the master API using theinternal masterURL host.\nRelative paths are resolved relative to the master configuration file. Restartthe server to pick up the configuration changes.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/configuring_authentication.html", "title": "Atomic Registry Latest | Installation and Configuration | Configuring Authentication and User Agent", "content": "The Atomic Registrymasterincludes a built-inOAuthserver. Developers and administrators obtainOAuthaccess tokens to authenticate themselves to the API.\nAs an administrator, you can configure OAuth using themaster configuration file to specify anidentity provider.This can be done during anadvanced installation or configured after installation.\nIf you installed Atomic Registry usingtheAdvanced Installationmethod, the\nWhen running a master without a configuration file, theAllow All identity provider is used bydefault, which allows any non-empty user name and password to log in. This isuseful for testing purposes. To use other identity providers, or to modify anytoken, grant, orsession options, you must run the master from aconfiguration file.\nRoles needto be assigned to administer the setup with an external user.\nAfter making changes to an identity provider, you must restart the master service for the changes to take effect:\nFor initial advanced installations, theDeny All identity provider is configured by default,though it can beoverridden during installation using theopenshift_master_identity_providers parameter, which is configurable in the inventory file.Session options in the OAuth configuration are also configurable in the inventory file.\nYou can configure the master host for authentication using your desired identityprovider by modifying themaster configurationfile. The following sections detail the identity providers supported byAtomic Registry.\nThere are four parameters common to all identity providers:\nname\nThe provider name is prefixed to provider user names to form anidentity name.\nchallenge\nWhen true, unauthenticated token requests from non-webclients (like the CLI) are sent a WWW-Authenticate challenge header. Notsupported by all identity providers.\nTo prevent cross-site request forgery (CSRF) attacks against browser clientsBasic authentication challenges are only sent if a X-CSRF-Token header ispresent on the request. Clients that expect to receive Basic WWW-Authenticatechallenges should set this header to a non-empty value.\nlogin\nWhen true, unauthenticated token requests from web clients(like the web console) are redirected to a login page backed by this provider.Not supported by all identity providers.\nIf you want users to be sent to a branded page before being redirected tothe identity provider\u2019s login, then set oauthConfig \u2192 alwaysShowProviderSelection: truein the master configuration file. This provider selection page can becustomized.\nmappingMethod\nDefines how new identities are mapped to users when they login. See Mapping Identities to Users for more information.\nSetting the mappingMethod parameter in amaster configuration filedetermines how identities are mapped to users:\nWhen set to the default claim value, OAuth will fail if the identity ismapped to a previously-existing user name. The following table outlines the usecases for the available mappingMethod parameter values:\nSet AllowAllPasswordIdentityProvider in the identityProviders stanza toallow any non-empty user name and password to log in. This is the defaultidentity provider when running Atomic Registry without amaster configuration file.\nSet DenyAllPasswordIdentityProvider in the identityProviders stanza todeny access for all user names and passwords.\nSet HTPasswdPasswordIdentityProvider in the identityProviders stanza tovalidate user names and passwords against a flat file generated usinghtpasswd.\nThe htpasswd utility is in the httpd-tools package:\nAtomic Registry supports the Bcrypt, SHA-1, and MD5 cryptographic hashfunctions, and MD5 is the default for htpasswd. Plaintext, encrypted text, andother hash functions are not currently supported.\nThe flat file is reread if its modification time changes, without requiring aserver restart.\nTo use the htpasswd command:\nThen, enter and confirm a clear-text password for the user. The command generates a hashed version of the password.\nFor example:\nYou can include the -b option to supply the password on the command line:\nFor example:\nSet KeystonePasswordIdentityProvider in the identityProviders stanza tovalidate user names and passwords against an OpenStack Keystone v3 server.This enables shared authentication with an OpenStack server configured to storeusers in an internal Keystone database.\nSet LDAPPasswordIdentityProvider in the identityProviders stanza tovalidate user names and passwords against an LDAPv3 server, using simple bindauthentication.\nDuring authentication, the LDAP directory is searched for an entry that matchesthe provided user name. If a single unique match is found, a simple bind isattempted using the distinguished name (DN) of the entry plus the providedpassword.\nThese are the steps taken:\nThe configured url is an RFC 2255 URL, which specifies the LDAP host andsearch parameters to use. The syntax of the URL is:\nFor the above example:\nldap\nFor regular LDAP, use the string ldap. For secure LDAP(LDAPS), use ldaps instead.\nhost:port\nThe name and port of the LDAP server. Defaults tolocalhost:389 for ldap and localhost:636 for LDAPS.\nbasedn\nThe DN of the branch of the directory where all searches shouldstart from. At the very least, this must be the top of your directory tree, butit could also specify a subtree in the directory.\nattribute\nThe attribute to search for. Although RFC 2255 allows acomma-separated list of attributes, only the first attribute will be used, nomatter how many are provided. If no attributes are provided, the default is touse uid. It is recommended to choose an attribute that will be unique acrossall entries in the subtree you will be using.\nscope\nThe scope of the search. Can be either either one or sub.If the scope is not provided, the default is to use a scope of sub.\nfilter\nA valid LDAP search filter. If not provided, defaults to(objectClass=*)\nWhen doing searches, the attribute, filter, and provided user name are combinedto create a search filter that looks like:\nFor example, consider a URL of:\nWhen a client attempts to connect using a user name of bob, the resultingsearch filter will be (&(enabled=true)(cn=bob)).\nIf the LDAP directory requires authentication to search, specify a bindDN andbindPassword to use to perform the entry search.\nSet BasicAuthPasswordIdentityProvider in the identityProviders stanza tovalidate user names and passwords against a remote server using aserver-to-server Basic authentication request. User names and passwords arevalidated against a remote URL that is protected by Basic authentication andreturns JSON.\nA 401 response indicates failed authentication.\nA non-200 status, or the presence of a non-empty \"error\" key, indicates anerror:\nA 200 status with a sub (subject) key indicates success:\nA successful response may optionally provide additional data, such as:\nSet RequestHeaderIdentityProvider in the identityProviders stanza toidentify users from request header values, such as X-Remote-User. It istypically used in combination with an authenticating proxy, which sets therequest header value. This is similar to howthe remote user plug-in in OpenShift Enterprise 2 allowed administrators toprovide Kerberos, LDAP, and many other forms of enterprise authentication.\nFor users to authenticate using this identity provider, they must accesshttps://<master>/oauth/authorize (and subpaths) via an authenticating proxy.To accomplish this, configure the OAuth server to redirect unauthenticatedrequests for OAuth tokens to the proxy endpoint that proxies to https://<master>/oauth/authorize.\nTo redirect unauthenticated requests from clients expecting browser-based login flows:\nTo redirect unauthenticated requests from clients expecting WWW-Authenticate challenges:\nThe provider.challengeURL and provider.loginURL parameters can includethe following tokens in the query portion of the URL:\nFor example: https://www.example.com/sso-login?then=${url}\nFor example: https://www.example.com/auth-proxy/oauth/authorize?${query}\nIf you expect unauthenticated requests to reach the OAuth server, a clientCAparameter MUST be set for this identity provider, so that incoming requestsare checked for a valid client certificate before the request\u2019s headers arechecked for a user name. Otherwise, any direct request to the OAuth server canimpersonate any identity from this provider, merely by setting a request header.\nThis example configures an authentication proxy on the same host as the master.Having the proxy and master on the same host is merely a convenience and may notbe suitable for your environment. For example, if you were alreadyrunning a router on themaster, port 443 would not be available.\nIt is also important to note that while this reference configuration usesApache\u2019s mod_auth_form, it is by no means required and other proxies caneasily be used if the following requirements are met:\nInstalling the Prerequisites\nThe mod_auth_form module is shipped as part of the mod_session package thatis found in the Optional channel:\nGenerate a CA for validating requests that submit the trusted header. This CAshould be used as the file name for clientCA in themaster\u2019s identity provider configuration.\nGenerate a client certificate for the proxy. This can be done using any x509certificate tooling. For convenience, the oadm CLI can be used:\nConfiguring Apache\nUnlike OpenShift Enterprise 2, this proxy does not need to reside on the samehost as the master. It uses a client certificate to connect to the master, whichis configured to trust the X-Remote-User header.\nConfigure Apache per the following:\nAdditional mod_auth_form Requirements\nA sample login page is available from theopenshift_extrasrepository. This file should be placed in the DocumentRoot location(/var/www/html by default).\nCreating Users\nAt this point, you can create the users in the system Apache is using to storeaccounts information. In this example, file-backed authentication is used:\nConfiguring the Master\nThe identityProviders stanza in the/etc/origin/master/master-config.yaml file must be updated as well:\nRestarting Services\nFinally, restart the following services:\nVerifying the Configuration\nSet GitHubIdentityProvider in the identityProviders stanza to useGitHub as an identity provider, using theOAuth integration.\nUsing GitHub as an identity provider requires users to get a token using<master>/oauth/token/request to use with command-line tools.\nUsing GitHub as an identity provider allows any GitHub user to authenticate to your server.You can limit authentication to members of specific GitHub organizations with theorganizations configuration attribute, as shown below.\nSet GitLabIdentityProvider in the identityProviders stanza to useGitLab.com or any other GitLab instance as an identity provider, using theOAuth integration.The OAuth provider feature requires GitLab version 7.7.0 or higher.\nUsing GitLab as an identity provider requires users to get a token using<master>/oauth/token/request to use with command-line tools.\nSet GoogleIdentityProvider in the identityProviders stanza to use Googleas an identity provider, usingGoogle\u2019s OpenIDConnect integration.\nUsing Google as an identity provider requires users to get a token using<master>/oauth/token/request to use with command-line tools.\nUsing Google as an identity provider allows any Google user to authenticate to your server.You can limit authentication to members of a specific hosted domain with thehostedDomain configuration attribute, as shown below.\nSet OpenIDIdentityProvider in the identityProviders stanza to integratewith an OpenID Connect identity provider using anAuthorization Code Flow.\nID Token and UserInfo decryptions are not supported.\nBy default, the openid scope is requested. If required, extra scopes can bespecified in the extraScopes field.\nClaims are read from the JWT id_token returned from the OpenID identityprovider and, if specified, from the JSON returned by the UserInfo URL.\nAt least one claim must be configured to use as the user\u2019s identity. Thestandard identity claim is sub.\nYou can also indicate which claims to use as the user\u2019s preferred user name,display name, and email address. If multiple claims are specified, the first onewith a non-empty value is used. Thestandard claims are:\nUsing an OpenID Connect identity provider requires users to get a token using<master>/oauth/token/request to use with command-line tools.\nA custom certificate bundle, extra scopes, extra authorization requestparameters, and userInfo URL can also be specified:\nThe OAuth server generates two kinds of tokens:\nUse the tokenConfig stanza to set token options:\nWhen the OAuth server receives token requests for a client to which the userhas not previously granted permission, the action that the OAuth server takesis dependent on the OAuth client\u2019s grant strategy.\nWhen the OAuth client requesting token does not provide its own grant strategy,the server-wide default strategy is used. To configure the default strategy,set the method value in the grantConfig stanza. Valid values formethod are:\nThe OAuth server uses a signed and encrypted cookie-based session during loginand redirect flows.\nUse the sessionConfig stanza to set session options:\nIf no sessionSecretsFile is specified, a random signing and encryptionsecret is generated at each start of the master server. This means that anylogins in progress will have their sessions invalidated if the master isrestarted. It also means that if multiple masters are configured, they will notbe able to decode sessions generated by one of the other masters.\nTo specify the signing and encryption secret to use, specify asessionSecretsFile. This allows you separate secret values from theconfiguration file and keep the configuration file distributable, for examplefor debugging purposes.\nMultiple secrets can be specified in the sessionSecretsFile to enablerotation. New sessions are signed and encrypted using the first secret in thelist. Existing sessions are decrypted and authenticated by each secret until onesucceeds.\nAtomic Registry implements a user agent that can be used to prevent anapplication developer\u2019s CLI accessing the Atomic Registry API.\nUser agents for the Atomic Registry CLI are constructed from a set of valueswithin Atomic Registry:\nSo, for example, when:\nthe user agent will be:\nAs an Atomic Registry administrator, you can prevent clients from accessing theAPI with the userAgentMatching configuration setting of a masterconfiguration. So, if a client is using a particular library orbinary, they will be prevented from accessing the API.\nThe following user agent example denies the Kubernetes 1.2 client binary,OpenShift Origin 1.1.3 binary, and the POST and PUT httpVerbs:\nAdministrators can also deny clients that do not exactly match the expectedclients:\nWhen the client\u2019s user agent mismatches the configuration, errors occur. Toensure that mutating requests match, enforce a whitelist. Rules are mapped tospecific verbs, so you can ban mutating requests while allowing non-mutatingrequests.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/redeploying_certificates.html", "title": "Atomic Registry Latest | Installation and Configuration | Redeploying Certificates", "content": "Atomic Registry uses certificates to provide secure connections for thefollowing components:\nYou can use Ansible playbooks provided with the installer to automate checkingexpiration dates for cluster certificates. Playbooks are also provided toautomate backing up and redeploying these certificates, which can fix commoncertificate errors.\nPossible use cases for redeploying certificates include:\nYou can use the installer to warn you about any certificates expiring within aconfigurable window of days and notify you about any certificates that havealready expired. Certificate expiry playbooks use the Ansible roleopenshift_certificate_expiry.\nCertificates examined by the role include:\nThe openshift_certificate_expiry role uses the following variables:\nThe Atomic Registry installer provides a set of example certificate expirationplaybooks, using different sets of configuration for theopenshift_certificate_expiry role.\nThese playbooks must be used with aninventory file that is representative of the cluster. For best results, runansible-playbook with the -v option.\nUsing the easy-mode.yaml example playbook, you can try the role out beforetweaking it to your specifications as needed. This playbook:\nTo run the easy-mode.yaml  playbook:\nThe other example playbooks are also available to run directly out of the/usr/share/ansible/openshift-ansible/playbooks/certificate_expiry/directory.\nTo run any of these example playbooks:\nAs noted above, there are two ways to format your check report. In JSON formatfor machine parsing, or as a stylized HTML page for easy skimming.\nAn example of an HTML report is provided with the installer. You can open thefollowing file in your browser to view it:\n/usr/share/ansible/openshift-ansible/roles/openshift_certificate_expiry/examples/cert-expiry-report.html\nThere are two top-level keys in the saved JSON results: data and summary.\nThe data key is a hash where the keys are the names of each host examined andthe values are the check results for the certificates identified on eachrespective host.\nThe summary key is a hash that summarizes the total number of certificates:\nFor an example of the full JSON report, see /usr/share/ansible/openshift-ansible/roles/openshift_certificate_expiry/examples/cert-expiry-report.json.\nThe summary from the JSON data can be easily checked for warnings or expirationsusing a variety of command-line tools. For example, using grep you can lookfor the word summary and print out the two lines after the match (-A2):\nIf available, the jq tool can also be used to pick out specific values. Thefirst two examples below show how to select just one value, either warning orexpired. The third example shows how to select both values at once:\nUse the following playbooks to redeploy master, etcd, node, registry, and routercertificates on all relevant hosts. You can redeploy all of them at once usingthe current CA, redeploy certificates for specific components only, or redeploya newly generated or custom CA on its own.\nJust like the certificate expiry playbooks, these playbooks must be run with aninventory file that is representative of the cluster.\nIn particular, the inventory must specify or override all host names and IPaddresses set via the following variables such that they match the currentcluster configuration:\nThe validity (length in days until they expire) for any certificatesauto-generated while redeploying can be configured via Ansible as well. SeeConfiguring Certificate Validity.\nThe redeploy-certificates.yml playbook does not regenerate theAtomic Registry CA certificate. New master, etcd, node, registry, and routercertificates are created using the current CA certificate to sign newcertificates.\nThis also includes serial restarts of:\nTo redeploy master, etcd, and node certificates using the currentAtomic Registry CA, run this playbook, specifying your inventory file:\nThe redeploy-openshift-ca.yml playbook redeploys the Atomic Registry CAcertificate by generating a new CA certificate and distributing an updatedbundle to all components including client kubeconfig files and the node\u2019sdatabase of trusted CAs (the CA-trust).\nThis also includes serial restarts of:\nAdditionally, you can specify acustom CA certificate when redeploying certificates instead of relying on a CAgenerated by Atomic Registry.\nWhen the master services are restarted, the registry and routers can continue tocommunicate with the master without being redeployed because the master\u2019sserving certificate is the same, and the CA the registry and routers have arestill valid.\nTo redeploy a newly generated or custom CA:\nIf you do not set the above, then the current CA will be regenerated in the nextstep.\nWith the new CA in place, you can then use theredeploy-certificates.yml playbook at your discretion whenever you want to redeploy certificates signedby the new CA on all components.\nThe redeploy-master-certificates.yml playbook only redeploys mastercertificates. This also includes serial restarts of master services.\nTo redeploy master certificates, run this playbook, specifying your inventoryfile:\nThe redeploy-etcd-certificates.yml playbook only redeploys etcd certificatesincluding master client certificates.\nThis also include serial restarts of:\nTo redeploy etcd certificates, run this playbook, specifying your inventoryfile:\nThe redeploy-node-certificates.yml playbook only redeploys nodecertificates. This also include serial restarts of node services.\nTo redeploy node certificates, run this playbook, specifying your inventoryfile:\nThe redeploy-registry-certificates.yml andredeploy-router-certificates.yml playbooks replace installer-createdcertificates for the registry and router. If custom certificates are in use forthese components, seeRedeploying CustomRegistry or Router Certificates to replace them manually.\nTo redeploy registry certificates, run the following playbook, specifying yourinventory file:\nTo redeploy router certificates, run the following playbook, specifying yourinventory file:\nWhen nodes are evacuated due to a redeployed CA, registry and router pods arerestarted. If the registry and router certificates were not also redeployed withthe new CA, this can cause outages because they cannot reach the masters usingtheir old certificates.\nThe playbooks for redeploying certificates cannot redeploy custom registry orrouter certificates, so to address this issue, you can manually redeploy theregistry and router certificates.\nTo redeploy registry certificates manually, you must add new registrycertificates to a secret named registry-certificates, then redeploy theregistry:\nThen, run the following to remove the environment variables:\nWhen routers are initially deployed, an annotation is added to the router\u2019sservice that automatically creates aservice serving certificate secret.\nTo redeploy router certificates manually, that service serving certificate canbe triggered to be recreated by deleting the secret, removing and re-addingannotations to the router service, then redeploying the router:\nThen, run the following to remove the environment variables:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/syncing_groups_with_ldap.html", "title": "Atomic Registry Latest | Installation and Configuration | Syncing Groups With LDAP", "content": "As an Atomic Registry administrator, you can use groups to manage users, changetheir permissions, and enhance collaboration. Your organization may have alreadycreated user groups and stored them in an LDAP server. Atomic Registry can syncthose LDAP records with internal Atomic Registry records, enabling you to manageyour groups in one place. Atomic Registry currently supports group sync withLDAP servers using three common schemas for defining group membership: RFC 2307,Active Directory, and augmented Active Directory.\nYou must havecluster-adminprivileges to sync groups.\nBefore you can run LDAP sync, you need a syncconfiguration file. This file contains LDAP client configuration details:\nA sync configuration file can also contain an administrator-defined list of namemappings that maps Atomic Registry Group names to groups in your LDAP server.\nSync configurations consist of LDAP query definitions for the entries that arerequired for synchronization. The specific definition of an LDAP query dependson the schema used to store membership information in the LDAP server.\nbase\nOnly consider the object specified by the base DN given for the query.\none\nConsider all of the objects on the same level in the tree as the base DN forthe query.\nsub\nConsider the entire subtree rooted at the base DN given for the query.\nnever\nNever dereference any aliases found in the LDAP tree.\nsearch\nOnly dereference aliases found while searching.\nbase\nOnly dereference aliases while finding the base object.\nalways\nAlways dereference all aliases found in the LDAP tree.\nA user-defined name mapping explicitly maps the names of Atomic Registry Groups tounique identifiers that find groups on your LDAP server. The mapping uses normalYAML syntax. A user-defined mapping can contain an entry for every group in yourLDAP server or only a subset of those groups. If there are groups on the LDAPserver that do not have a user-defined name mapping, the default behavior duringsync is to use the attribute specified as the Group\u2019s name.\nOnce you have created a sync configuration file,then sync can begin. Atomic Registry allows administrators to perform a number ofdifferent sync types with the same server.\nBy default, all group synchronization or pruning operations are dry-run, so youmust set the --confirm flag on the oadm groups sync command in order to makechanges to Atomic Registry group records.\nTo sync all groups from the LDAP server with Atomic Registry:\nTo sync all Groups already in Atomic Registry that correspond to groups in theLDAP server specified in the configuration file:\nTo sync a subset of LDAP groups with Atomic Registry, you can use whitelist files,blacklist files, or both:\nAny combination of blacklist files, whitelist files, or whitelist literals willwork; whitelist literals can be included directly in the command itself. Thisapplies to groups found on LDAP servers, as well as Groups already present inAtomic Registry. Your files must contain one unique group identifier per line.\nAn administrator can also choose to remove groups from Atomic Registry recordsif the records on the LDAP server that created them are no longer present. Theprune job will accept the same sync configuration file and white- or black-listsas used for the sync job.\nFor example, if groups had previously been synchronized from LDAP using someconfig.yaml file, and some of those groups no longer existed on the LDAPserver, the following command would determine which Groups in Atomic Registrycorresponded to the deleted groups in LDAP and then remove them fromAtomic Registry:\nThis section contains examples for the RFC 2307,Active Directory, andaugmented Active Directory schemas.All of the following examples synchronize a group named admins that has twomembers: Jane and Jim. Each example explains:\nThese examples assume that all users are direct members of their respectivegroups. Specifically, no groups have other groups as members. SeeNested Membership Sync Example for information onhow to sync nested groups.\nIn the RFC 2307 schema, both users (Jane and Jim) and groups exist on the LDAPserver as first-class entries, and group membership is stored in attributes onthe group. The following snippet of ldif defines the users and group for thisschema:\nTo sync this group, you must first create the configuration file. TheRFC 2307 schema requires you to provide an LDAP query definition for both userand group entries, as well as the attributes with which to represent them in theinternal Atomic Registry records.\nFor clarity, the Group you create in Atomic Registry should use attributes otherthan the distinguished name whenever possible for user- or administrator-facingfields. For example, identify the users of a Group by their e-mail, and use thename of the group as the common name. The following configuration file createsthese relationships:\nIf using user-defined name mappings, yourconfiguration file will differ.\nTo run sync with the rfc2307_config.yaml file:\nAtomic Registry creates the following Group record as a result of the above syncoperation:\nWhen syncing groups with user-defined name mappings, the configuration filechanges to contain these mappings as shown below.\nTo run sync with the rfc2307_config_user_defined.yaml file:\nAtomic Registry creates the following Group record as a result of the above syncoperation:\nBy default, if the groups being synced contain members whose entries are outsideof the scope defined in the member query, the group sync fails with an error:\nThis often indicates a mis-configured baseDN in the usersQuery field.However, in cases where the baseDN intentionally does not contain some of themembers of the group, setting tolerateMemberOutOfScopeErrors: true allowsthe group sync to continue. Out of scope members will be ignored.\nSimilarly, when the group sync process fails to locate a member for a group, itfails outright with errors:\nThis often indicates a mis-configured usersQuery field. However, in caseswhere the group contains member entries that are known to be missing, settingtolerateMemberNotFoundErrors: true allows the group sync to continue.Problematic members will be ignored.\nEnabling error tolerances for the LDAP group sync causes the sync process toignore problematic member entries. If the LDAP group sync is not configuredcorrectly, this could result in synced Atomic Registry groups missing members.\nIn order to tolerate the errors in the above example, the following additions toyour sync configuration file must be made:\nTo run sync with the rfc2307_config_tolerating.yaml file:\nAtomic Registry creates the following group record as a result of the above syncoperation:\nIn the Active Directory schema, both users (Jane and Jim) exist in the LDAPserver as first-class entries, and group membership is stored in attributes onthe user. The following snippet of ldif defines the users and group for thisschema:\nTo sync this group, you must first create the configuration file. TheActive Directory schema requires you to provide an LDAP query definition foruser entries, as well as the attributes to represent them with in the internalAtomic Registry Group records.\nFor clarity, the Group you create in Atomic Registry should use attributes otherthan the distinguished name whenever possible for user- or administrator-facingfields. For example, identify the users of a Group by their e-mail, but definethe name of the Group by the name of the group on the LDAP server.The following configuration file creates these relationships:\nTo run sync with the active_directory_config.yaml file:\nAtomic Registry creates the following Group record as a result of the above syncoperation:\nIn the augmented Active Directory schema, both users (Jane and Jim) and groupsexist in the LDAP server as first-class entries, and group membership is storedin attributes on the user. The following snippet of ldif defines the users andgroup for this schema:\nTo sync this group, you must first create the configuration file. Theaugmented Active Directory schema requires you to provide an LDAP querydefinition for both user entries and group entries, as well as the attributeswith which to represent them in the internal Atomic Registry Group records.\nFor clarity, the Group you create in Atomic Registry should use attributes otherthan the distinguished name whenever possible for user- or administrator-facingfields. For example, identify the users of a Group by their e-mail,and use the name of the Group as the common name. The following configurationfile creates these relationships.\nTo run sync with the augmented_active_directory_config.yaml file:\nAtomic Registry creates the following Group record as a result of the above syncoperation:\nGroups in Atomic Registry do not nest. The LDAP server must flatten groupmembership before the data can be consumed. Microsoft\u2019s Active Directory Serversupports this feature via theLDAP_MATCHING_RULE_IN_CHAINrule, which has the OID 1.2.840.113556.1.4.1941. Furthermore, only explicitlywhitelistedgroups can be synced when using this matching rule.\nThis section has an example for the augmented Active Directory schema, whichsynchronizes a group named admins that has one user Jane and one groupotheradmins as members. The otheradmins group has one user member: Jim.This example explains:\nIn the augmented Active Directory schema, both users (Jane and Jim) andgroups exist in the LDAP server as first-class entries, and group membership isstored in attributes on the user or the group. The following snippet of ldifdefines the users and groups for this schema:\nTo sync nested groups with Active Directory, you must provide an LDAP querydefinition for both user entries and group entries, as well as the attributeswith which to represent them in the internal Atomic Registry Group records.Furthermore, certain changes are required in this configuration:\nFor clarity, the Group you create in Atomic Registry should use attributes otherthan the distinguished name whenever possible for user- or administrator-facingfields. For example, identify the users of a Group by their e-mail, and use thename of the Group as the common name. The following configuration file createsthese relationships:\nTo run sync with the augmented_active_directory_config_nested.yaml file:\nYou must explicitlywhitelistthe cn=admins,ou=groups,dc=example,dc=com group.\nAtomic Registry creates the following Group record as a result of the above syncoperation:\nThe object specification for the configuration file is below.  Note that the different schemaobjects have different fields.  For example, v1.ActiveDirectoryConfig has no groupsQueryfield whereas v1.RFC2307Config and v1.AugmentedActiveDirectoryConfig both do.\nLDAPSyncConfig holds the necessary configuration options to define an LDAPgroup sync.\nStringSource allows specifying a string inline, or externally via environmentvariable or file. When it contains only a string value, it marshals to a simpleJSON string.\nLDAPQuery holds the options necessary to build an LDAP query.\nRFC2307Config holds the necessary configuration options to define how an LDAPgroup sync interacts with an LDAP server using the RFC2307 schema.\nActiveDirectoryConfig holds the necessary configuration options to define howan LDAP group sync interacts with an LDAP server using the Active Directoryschema.\nAugmentedActiveDirectoryConfig holds the necessary configuration options todefine how an LDAP group sync interacts with an LDAP server using the augmentedActive Directory schema.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/install_config/web_console_customization.html", "title": "Atomic Registry Latest | Installation and Configuration | Customizing the Web Console", "content": "Catalog categories organize the display of builder images and templates on theAdd to Project page on the Atomic Registry web console. A builder image ortemplate is grouped in a category if it includes a tag with the same name of thecategory or category alias. Categories only display if one or more builderimages or templates with matching tags are present in the catalog.\nSignificant customizations to the catalog categories may affect the userexperience and should be done with careful consideration. You may need to updatethis customization in future upgrades if you modify existing category items.\nCreate from URLonly works with image streams or templates from namespaces that have beenexplicitly specified in OPENSHIFT_CONSTANTS.CREATE_FROM_URL_WHITELIST.  To addnamespaces to the whitelist, follow these steps:\nopenshift is included in the whitelist by default. Do not remove it.\nIf you enabled wildcard routes for a router, you can also enable wildcardroutes in the web console. This lets users enter hostnames starting with anasterisk like *.example.com when creating a route. To enable wildcard routes:\nLearnhow to configure HAProxy routers to allow wildcard routes.\nSometimes features are available in Technology Preview. By default, thesefeatures are disabled and hidden in the web console.\nCurrently, there are no web console features in Technology Preview.\nTo enable a Technology Preview feature:\nYou can serve other files from the Asset Server as well. For example, you mightwant to make the CLI executable available for download from the web console oradd images to use in a custom stylesheet.\nAdd the directory with the files you want using the following configurationoption:\nThe files under the /path/to/my_images directory will be available under theURL /<context>/extensions/images in the web console.\nTo reference these files from a stylesheet, you should generally use a relativepath. For example:\nThe web console has a special mode for supporting certain static webapplications that use the HTML5 history API:\nSetting html5Mode to true enables two behaviors:\nThis is needed for JavaScript frameworks such as AngularJS that require baseto be set in index.html.\nYou can also change the login page, and the login provider selection page forthe web console. Run the following commands to create templates you can modify:\nEdit the file to change the styles or add content, but be careful not to removeany required parameters inside the curly brackets.\nTo use your custom login page or provider selection page, set the followingoptions in the master configuration file:\nRelative paths are resolved relative to the master configuration file. You mustrestart the server after changing this configuration.\nWhen there are multiple login providers configured or when thealwaysShowProviderSelectionoption in the master-config.yaml file is set to true, each time a user\u2019stoken to Atomic Registry expires, the user is presented with this custom pagebefore they can proceed with other tasks.\nCustom login pages can be used to create Terms of Service information. They canalso be helpful if you use a third-party login provider, like GitHub or Google,to show users a branded page that they trust and expect before being redirectedto the authentication provider.\nWhen errors occur during authentication, you can change the page shown.\nYou can use the Error and ErrorCode variables in the template. To useyour custom error page, set the following option in the master configurationfile:\nRelative paths are resolved relative to the master configuration file.\nYou can change the location a console user is sent to when logging out ofthe console by modifying the logoutURL parameter in the/etc/origin/master/master-config.yaml file:\nThis can be useful when authenticating withRequestHeader and OAuth orOpenID identityproviders, which require visiting an external URL to destroy single sign-onsessions.\nDuringadvanced installations,many modifications to the web console can be configured usingthe following parameters, which are configurable in the inventory file:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/registry_quickstart/administrators/index.html", "title": "Atomic Registry Latest | Quickstart | Administrators | Overview", "content": "The Atomic Registry quickstart installation has been provided to get a systemrunning as quickly as possible. Several choices have been made toenable this installation experience.\nWhile Atomic Registry may run on a non-Red Hat-based distributions, most testinghas been performed on Red Hat Enterprise Linux, Centos and Fedora, including Atomic Host.\nThe installation will use the system hostname if not provided. If the hostnamedoes not resolve outside of the system via DNS or the external hostname isdifferent, use a hostname or external IP address that can be addressed fromoutside the system during the install and run steps below.\nThe very first time you start this service, it may take a few seconds to pulldown the atomic-registry-master, atomic-registry-console andatomic-registry container images from Docker Hub, and you will not see theimages in docker ps yet until that is complete. Use systemctl statusatomic-registry\\* to watch the progress.\nUntil the registry is secured with TLS certificates, configure any docker daemonclients by editing file /etc/sysconfig/docker to use insecure registry option andrestart the docker service.\nHere are some topics you may follow to explore Atomic Registry:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/registry_quickstart/administrators/system_configuration.html", "title": "Atomic Registry Latest | Quickstart | Administrators | System Configuration", "content": "Atomic Registry deploys container services managed by systemd.\nEach container service sends logs to the system journal because they are managedby systemd. For example, here we follow the log from the atomic-registry-masterservice unit.\nChanges to configuration files require a service restart. Under systemd, use systemctlto manage container lifecycle. Here we restart the atomic-registry-master service.\nThe three services may be restartedindependently.\nAuthentication configuration, project defaults, datastore, API certificates andother settings are defined globally in a configuration file mounted on the hostat /etc/atomic-registry/master/master-config.yaml. Whenever this file isupdated restart the atomic-registry-master service.\nGlobal configuration items managed by this configuration file include:\nMaster log level is determined using the service unit configuration file mountedon the host at /etc/sysconfig/atomic-registry-master.\nThe registry console is a stateless service that has limited configuration usingenvironment variables defined in a file mounted on the host at/etc/sysconfig/atomic-registry-console.\nConfiguration changes require a service restart.\nThe docker distribution registry configuration is managed by a file mounted onthe host at /etc/atomic-registry/registry/config.yml. Seeregistry configuration reference for complete settings.\nEnvironment variables for this service are managed by a file mounted on the hostat /etc/sysconfig/atomic-registry.\nConfiguration changes require a service restart.\nThere are many configuration options available in the upstreamdocker distribution library. Notall configuration optionsare supported or enabled. Use this section as a reference whenoverriding the registryconfiguration.\nUpstream configuration options in this file may also be overridden usingenvironment variables. However, themiddleware section maynot be overridden using environment variables.Learnhow to override specific configuration options.\nUpstream options are supported.\nExample:\nMail hooks are not supported.\nThis section lists the supported registry storage drivers.\nThe following list includes storage drivers that need to be configured in theregistry\u2019s configuration file:\nGeneral registry storage configuration options are supported.\nThe following storage options need to be configured through the filesystem driver:\nFor more information on supported persistent storage drivers, see Configuring Persistent Storage and Persistent Storage Examples.\nAuth options should not be altered. The openshift extension is the onlysupported option.\nThe repository middleware extension allows to configure Atomic Registrymiddleware responsible for interaction with Atomic Registry and image proxying.\nThe CloudFrontmiddleware extension can be added to support AWS, CloudFront CDN storageprovider. CloudFront middleware speeds up distribution of image contentinternationally. The blobs are distributed to several edge locations around theworld. The client is always directed to the edge with the lowest latency.\nThe CloudFrontmiddleware extension can be only used withS3 storage.It is utilized only during blob serving. Therefore, only blob downloads can bespeeded up, not uploads.\nThe following is an example of minimal configuration of S3 storage driver with aCloudFront middleware:\nThe middleware section cannot be overridden using environment variables.There are a few exceptions, however. For example:\nIf enabled, the registry will attempt to fetch requested blob from a remoteregistry unless the blob exists locally. The remote candidates are calculatedfrom DockerImage entries stored in status of theimagestream, a client pulls from. All the unique remote registry references insuch entries will be tried in turn until the blob is found. The blob, servedthis way, will not be stored in the registry.\nThis feature is on by default. However, it can be disabled using aconfiguration option.\nBy default, all the remote blobs served this way are stored locally forsubsequent faster access unless mirrorpullthrough is disabled. The downsideof this mirroring feature is an increased storage usage.\nThe mirroring starts when a client tries to fetch at least a single byte of theblob. To pre-fetch a particular image into integrated registry before it isactually needed, you can run the following command:\nEach image has a manifest describing its blobs, instructions for running itand additional metadata. The manifest is versioned which have differentstructure and fields as it evolves over time. The same image can be representedby multiple manifest versions. Each version will have different digest though.\nThe registry currently supportsmanifestv2 schema 1 (schema1) andmanifestv2 schema 2 (schema2). The former is being obsoleted but will be supportedfor an extended amount of time.\nYou should be wary of compatibility issues with various Docker clients:\nThe registry, storing an image with schema1 will always return it unchangedto the client. Schema2 will be transferred unchanged only to newer Dockerclient. For the older one, it will be converted on-the-fly to schema1.\nThis has significant consequences. For example an image pushed to the registryby a newer Docker client cannot be pulled by the older Docker by its digest.That\u2019s because the stored image\u2019s manifest is of schema2 and its digest canbe used to pull only this version of manifest.\nFor this reason, the registry is configured by default not to store schema2.This ensures that any docker client will be able to pull from the registry anyimage pushed there regardless of client\u2019s version.\nOnce you\u2019re confident that all the registry clients support schema2, you\u2019llbe safe to enable its support in the registry. See themiddlewareconfiguration reference above for particular option.\nThis section reviews the configuration of global settings related toAtomic Registry specific features. In the future, openshift-related settingsin the middlewaresection will be obsolete. Currently, this section allows you to configure onlyfor metrics collection.\nRefer to accessing metrics forusage information.\nReporting is unsupported.\nUpstream options aresupported. Learn how to alter these settings viaenvironment variables. Only the tls section should be altered. For example:\nUpstreamoptions are supported. The REST API Referenceprovides more comprehensive integration options.\nExample:\nRedis is not supported.\nUpstream optionsare supported. The registry deployment configuration provides an integratedhealth check at /healthz.\nProxy configuration should not be enabled. This functionality is provided bythe Atomic Registryrepository middleware extension, pullthrough: true.\nThe service endpoints are secured by certificates that are mounted on the host at/etc/atomic-registry/. There are separate directories for master and registry certificates.\nThe installer generates self-signed certificates during installation. Seecertificate customizationfor customizing the API master certificates.\nHere we create a self-signed certificate so docker clients can connect usingTLS. While other tools like openssl may be used to create certificates, themaster API provides a tool that may also be used.\nThe oadm ca create-server-cert command generates a certificate that is validfor two years. This can be altered with the --expire-days option, but forsecurity reasons, it is recommended to not make it greater than this value.\nIf you secure the registry using a self-signed certificate key pair you may wantto make the public CA certificate available to users so they don\u2019t have to putdocker into insecure mode. The registry master service is able to servearbitrary files.\nRegistry users may then be instructed to save this cert into their docker clientand restart their docker daemon.\nBy default the session token used for docker login is 24 hours. Thisrequires users to run docker login every day. This value may be extended inthe master configuration file /etc/atomic-registry/master/master-config.yaml.See below for usinglong-lived service account tokens.\nIn this example the session token expiration is extended to 30 days.\nRestart the atomic-registry-master service to update the running configuration.\nTypically long-lived, token-based authentication is desired. As an alternativeto using user session tokens that expire, users may useservice account tokens toauthenticate with docker. This is particularly useful when integrating automation.See thequickstart developer guidefor instructions.\nThe data that should be persisted is the configuration, image data and themaster database. The required directories are mounted from the container ontothe host. See Service Components table forspecific paths.\nAtomic Registry provides full control using a command line interface (CLI). Toaccess the CLI directly on the host you may enter the atomic-registry-mastercontainer.\nSee CLI reference for how to download theremote CLI client and using the CLI.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/registry_quickstart/administrators/uninstall.html", "title": "Atomic Registry Latest | Quickstart | Administrators | Uninstall", "content": "To uninstall the registry quickstart deployment execute the following command.This will retain the etcd datastore and image directories in /var/lib/atomic-registry.\nTo uninstall the registry quickstart deployment and remove all configuration andimage data in /var/lib/atomic-registry execute the following command.\nThis command will remove all configuration files for the registry.\nWhen complete you may need to manually remove stopped containers and unusedimages.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/registry_quickstart/developers.html", "title": "Atomic Registry Latest | Quickstart | Developers", "content": "There are several configuration options for Atomic Registry. Your administratormay have disabled project self-provisioning. Sharing images may also be disabled.In this configuration you may only be able to pull existing images.\nImages are pushed and pulled from the docker command line. The docker commandline reference is found on the bottom of the web interface overview page.\nYour username and password credentials are not valid when logging in with DockerCLI. Use the token for the password argument.\nAtomic Registry uses a session token for the Docker login command. With thedefault configuration this token will expire within 24 hours. Contact theAtomic Registry administrator for extending the session timeout.\nIf a long-lived Docker login is desired use a service account token. This may beused in place of the Docker login password value:\nThroughout this document sudo is prepended to example docker commands.Depending on your environment sudo may not be required. This impacts where theDocker credentials file is stored. With sudo, Docker login credentials arestored in /root/.docker/config.json. Without sudo, Docker login credentials are stored in~/.docker/config.json, which will be a different path if not logged in asroot user. This would cause docker commands to fail authentication if sudois used inconsistently.\nSee the Atomic Registry User Guide for these topics:\nFor long-lived, token-based authentication, users may createserviceaccount tokens to authenticate with Docker. This is particularly useful whenintegrating automation. Service accounts must be configured using the CLI. SeeGettingStarted with the CLI.\nService accounts may be deleted, which disables further authentication attempts.For example, as soon as the service account is deleted, docker push will no longersucceed if logged in with this service account token.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/registry_quickstart/index.html", "title": "Atomic Registry Latest | Quickstart | Overview", "content": "To get started with Atomic Registry, find the appropriate topic based on your role:\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/rest_api/index.html", "title": "Atomic Registry Latest | REST API Reference | Overview", "content": "\u00a0The Atomic Registry distribution of Kubernetes includes theKubernetes v1 RESTAPI and the OpenShiftv1 REST API. These are RESTful APIs accessible via HTTP(s) on theAtomic Registry master servers.\nThese REST APIs can be used to manage end-user applications, the cluster, andthe users of the cluster.\nAPI calls must be authenticated with an access token or X.509 certificate. SeeAuthenticationin the Architecture documentation for an overview.\nThis section highlights the token authentication method. With tokenauthentication, a bearer token must be passed in as anHTTPAuthorization header. There are two types of access tokens: session and serviceaccount.\nA session token is short-lived, expiring within 24 hours by default. Itrepresents auser.After logging in, the session token may be obtained with the oc whoamicommand:\nService account tokens are long-lived tokens. They areJSON Web Token (JWT) formatted tokensand are much longer strings than session tokens. SeeUsinga Service Account\u2019s Credentials Externally for steps on using these tokens toauthenticate using the CLI.\nA service account token may be obtained with these commands:\nThe token value may be used as an in an authorization header toauthenticate API calls, theCLIor in the docker login command. Service accounts maybe created and deleted as needed with the appropriate role(s) assigned. SeeAuthorizationin the Architecture documentation for a deeper discussion on roles.\nThese examples provide a quick reference for making successful REST API calls.They use insecure methods. In these examples, a simple GET call is made tolist available resources.\nThe Atomic Registry integrated Docker registry must be authenticated usingeither a user session orservice account token. The value of thetoken must be used as the value for the --password argument. The user andemail argument values are ignored:\nThe API is designed to work via thewebsocket protocol. API requests maytake the form of \"one-shot\" calls to list resources or by passing in queryparameter watch=true. When watching an endpoint, changes to the system may beobserved through an open endpoint. Using callbacks, dynamic systems may bedeveloped that integrate with the API.\nFor more information and examples, see the Mozilla Developer Network page onWritingWebSocket client applications.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/rest_api/kubernetes_v1.html", "title": "Atomic Registry Latest | REST API Reference | Kubernetes v1", "content": "The Kubernetes API allows you to run containerized applications, bind persistent storage, link those applications through service discovery, and manage the cluster infrastructure.\nVersion: v1\nHost: 127.0.0.1:8443BasePath: /Schemes: HTTPS\nAPIResource specifies the name of a resource and whether it is namespaced.\nAPIResourceList is a list of APIResource, it is used to expose the name of the resources supported in a specific group and version, and if the resource is namespaced.\nA label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.\nA label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.\nListMeta describes metadata that synthetic resources must have, including lists and various status objects. A resource may have only one of {ObjectMeta, ListMeta}.\nPatch is provided to give a concrete name and type to the Kubernetes PATCH request body.\nStatus is a return value for calls that don\u2019t return other objects.\nStatusCause provides more information about an api.Status failure, including cases when multiple errors are encountered.\nStatusDetails is a set of additional properties that MAY be set by the server to provide additional information about a response. The Reason field of a Status object defines what attributes will be set. Clients must ignore fields that do not match the defined type of each attribute, and should assume that any attribute may be empty, invalid, or under defined.\nRepresents a Persistent Disk resource in AWS.\nAn AWS EBS disk must exist before mounting to a container. The disk must also be in the same AWS zone as the kubelet. An AWS EBS disk can only be mounted as read/write once. AWS EBS volumes support ownership management and SELinux relabeling.\nAttachedVolume describes a volume attached to a node\nAzureDisk represents an Azure Data Disk mount on the host and bind mount to the pod.\nAzureFile represents an Azure File Service mount on the host and bind mount to the pod.\nBinding ties one object to another. For example, a pod is bound to a node by a scheduler.\nAdds and removes POSIX capabilities from running containers.\nRepresents a Ceph Filesystem mount that lasts the lifetime of a pod Cephfs volumes do not support ownership management or SELinux relabeling.\nRepresents a cinder volume resource in Openstack. A Cinder volume must exist before mounting to a container. The volume must also be in the same region as the kubelet. Cinder volumes support ownership management and SELinux relabeling.\nInformation about the condition of a component.\nComponentStatus (and ComponentStatusList) holds the cluster validation info.\nStatus of all the conditions for the component as a list of ComponentStatus objects.\nConfigMap holds configuration data for pods to consume.\nSelects a key from a ConfigMap.\nConfigMapList is a resource containing a list of ConfigMap objects.\nAdapts a ConfigMap into a volume.\nThe contents of the target ConfigMap\u2019s Data field will be presented in a volume as files using the keys in the Data field as the file names, unless the items element is populated with specific mappings of keys to paths. ConfigMap volumes support ownership management and SELinux relabeling.\nA single application container that you want to run within a pod.\nDescribe a container image\nContainerPort represents a network port in a single container.\nContainerState holds a possible state of container. Only one of its members may be specified. If none of them is specified, the default one is ContainerStateWaiting.\nContainerStateRunning is a running state of a container.\nContainerStateTerminated is a terminated state of a container.\nContainerStateWaiting is a waiting state of a container.\nContainerStatus contains details for the current status of this container.\nDaemonEndpoint contains information about a single Daemon endpoint.\nDeleteOptions may be provided when deleting an API object\nDeprecatedDownwardAPIVolumeFile represents information to create the file containing the pod field This type is deprecated and should be replaced by use of the downwardAPI volume source.\nDeprecatedDownwardAPIVolumeSource represents a volume containing downward API info. This type is deprecated and should be replaced by use of the downwardAPI volume source.\nDownwardAPIVolumeFile represents information to create the file containing the pod field\nDownwardAPIVolumeSource represents a volume containing downward API info. Downward API volumes support ownership management and SELinux relabeling.\nRepresents an empty directory for a pod. Empty directory volumes support ownership management and SELinux relabeling.\nEndpointAddress is a tuple that describes single IP address.\nEndpointPort is a tuple that describes a single port.\nEndpointSubset is a group of addresses with a common set of ports. The expanded set of endpoints is the Cartesian product of Addresses x Ports. For example, given:  {    Addresses: [{\"ip\": \"10.10.1.1\"}, {\"ip\": \"10.10.2.2\"}],    Ports:     [{\"name\": \"a\", \"port\": 8675}, {\"name\": \"b\", \"port\": 309}]  }The resulting set of endpoints can be viewed as:    a: [ 10.10.1.1:8675, 10.10.2.2:8675 ],    b: [ 10.10.1.1:309, 10.10.2.2:309 ]\nEndpoints is a collection of endpoints that implement the actual service. Example:  Name: \"mysvc\",  Subsets: [    {      Addresses: [{\"ip\": \"10.10.1.1\"}, {\"ip\": \"10.10.2.2\"}],      Ports: [{\"name\": \"a\", \"port\": 8675}, {\"name\": \"b\", \"port\": 309}]    },    {      Addresses: [{\"ip\": \"10.10.3.3\"}],      Ports: [{\"name\": \"a\", \"port\": 93}, {\"name\": \"b\", \"port\": 76}]    }, ]\nEndpointsList is a list of endpoints.\nEnvVar represents an environment variable present in a Container.\nEnvVarSource represents a source for the value of an EnvVar.\nEvent is a report of an event somewhere in the cluster.\nEventList is a list of events.\nEventSource contains information for an event.\nExecAction describes a \"run in container\" action.\nRepresents a Fibre Channel volume. Fibre Channel volumes can only be mounted as read/write once. Fibre Channel volumes support ownership management and SELinux relabeling.\nFSGroupStrategyOptions defines the strategy type and options used to create the strategy.\nFlexVolume represents a generic volume resource that is provisioned/attached using an exec based plugin. This is an alpha feature and may change in future.\nRepresents a Flocker volume mounted by the Flocker agent. One and only one of datasetName and datasetUUID should be set. Flocker volumes do not support ownership management or SELinux relabeling.\nRepresents a Persistent Disk resource in Google Compute Engine.\nA GCE PD must exist before mounting to a container. The disk must also be in the same GCE project and zone as the kubelet. A GCE PD can only be mounted as read/write once or read-only many times. GCE PDs support ownership management and SELinux relabeling.\nRepresents a volume that is populated with the contents of a git repository. Git repo volumes do not support ownership management. Git repo volumes support SELinux relabeling.\nRepresents a Glusterfs mount that lasts the lifetime of a pod. Glusterfs volumes do not support ownership management or SELinux relabeling.\nHTTPGetAction describes an action based on HTTP Get requests.\nHTTPHeader describes a custom header to be used in HTTP probes\nHandler defines a specific action that should be taken\nRepresents a host path mapped into a pod. Host path volumes do not support ownership management or SELinux relabeling.\nIDRange provides a min/max of an allowed range of IDs.\nRepresents an ISCSI disk. ISCSI volumes can only be mounted as read/write once. ISCSI volumes support ownership management and SELinux relabeling.\nMaps a string key to a path within a volume.\nLifecycle describes actions that the management system should take in response to container lifecycle events. For the PostStart and PreStop lifecycle handlers, management of the container blocks until the action is complete, unless the container process fails, in which case the handler is aborted.\nLimitRange sets resource usage limits for each kind of resource in a Namespace.\nLimitRangeItem defines a min/max usage limit for any resource that matches on kind.\nLimitRangeList is a list of LimitRange items.\nLimitRangeSpec defines a min/max usage limit for resources that match on kind.\nLoadBalancerIngress represents the status of a load-balancer ingress point: traffic intended for the service should be sent to an ingress point.\nLoadBalancerStatus represents the status of a load-balancer.\nLocalObjectReference contains enough information to let you locate the referenced object inside the same namespace.\nRepresents an NFS mount that lasts the lifetime of a pod. NFS volumes do not support ownership management or SELinux relabeling.\nNamespace provides a scope for Names. Use of multiple namespaces is optional.\nNamespaceList is a list of Namespaces.\nNamespaceSpec describes the attributes on a Namespace.\nNamespaceStatus is information about the current status of a Namespace.\nNode is a worker node in Kubernetes. Each node will have a unique identifier in the cache (i.e. in etcd).\nNodeAddress contains information for the node\u2019s address.\nNodeCondition contains condition information for a node.\nNodeDaemonEndpoints lists ports opened by daemons running on the Node.\nNodeList is the whole list of all Nodes which have been registered with master.\nNodeSpec describes the attributes that a node is created with.\nNodeStatus is information about the current status of a node.\nNodeSystemInfo is a set of ids/uuids to uniquely identify the node.\nObjectFieldSelector selects an APIVersioned field of an object.\nObjectMeta is metadata that all persisted resources must have, which includes all objects users must create.\nObjectReference contains enough information to let you inspect or modify the referred object.\nOwnerReference contains enough information to let you identify an owning object. Currently, an owning object must be in the same namespace, so there is no namespace field.\nPersistentVolume (PV) is a storage resource provisioned by an administrator. It is analogous to a node. More info: http://kubernetes.io/docs/user-guide/persistent-volumes\nPersistentVolumeClaim is a user\u2019s request for and claim to a persistent volume\nPersistentVolumeClaimList is a list of PersistentVolumeClaim items.\nPersistentVolumeClaimSpec describes the common attributes of storage devices and allows a Source for provider-specific attributes\nPersistentVolumeClaimStatus is the current status of a persistent volume claim.\nPersistentVolumeClaimVolumeSource references the user\u2019s PVC in the same namespace. This volume finds the bound PV and mounts that volume for the pod. A PersistentVolumeClaimVolumeSource is, essentially, a wrapper around another type of volume that is owned by someone else (the system).\nPersistentVolumeList is a list of PersistentVolume items.\nPersistentVolumeSpec is the specification of a persistent volume.\nPersistentVolumeStatus is the current status of a persistent volume.\nRepresents a Photon Controller persistent disk resource.\nPod is a collection of containers that can run on a host. This resource is created by clients and scheduled onto hosts.\nPodCondition contains details for the current condition of this pod.\nPodList is a list of Pods.\nPodSecurityContext holds pod-level security attributes and common container settings. Some fields are also present in container.securityContext.  Field values of container.securityContext take precedence over field values of PodSecurityContext.\nPodSpec is a description of a pod.\nPodStatus represents information about the status of a pod. Status may trail the actual state of a system.\nPodTemplate describes a template for creating copies of a predefined pod.\nPodTemplateList is a list of PodTemplates.\nPodTemplateSpec describes the data a pod should have when created from a template\nPreconditions must be fulfilled before an operation (update, delete, etc.) is carried out.\nProbe describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic.\nRepresents a Quobyte mount that lasts the lifetime of a pod. Quobyte volumes do not support ownership management or SELinux relabeling.\nRepresents a Rados Block Device mount that lasts the lifetime of a pod. RBD volumes support ownership management and SELinux relabeling.\nReplicationController represents the configuration of a replication controller.\nReplicationControllerCondition describes the state of a replication controller at a certain point.\nReplicationControllerList is a collection of replication controllers.\nReplicationControllerSpec is the specification of a replication controller.\nReplicationControllerStatus represents the current status of a replication controller.\nResourceFieldSelector represents container resources (cpu, memory) and their output format\nResourceQuota sets aggregate quota restrictions enforced per namespace\nResourceQuotaList is a list of ResourceQuota items.\nResourceQuotaSpec defines the desired hard limits to enforce for Quota.\nResourceQuotaStatus defines the enforced hard limits and observed use.\nResourceRequirements describes the compute resource requirements.\nRunAsUserStrategyOptions defines the strategy type and any options used to create the strategy.\nSELinuxContextStrategyOptions defines the strategy type and any options used to create the strategy.\nSELinuxOptions are the labels to be applied to the container\nScale represents a scaling request for a resource.\nScaleSpec describes the attributes of a scale subresource.\nScaleStatus represents the current status of a scale subresource.\nSecret holds secret data of a certain type. The total bytes of the values in the Data field must be less than MaxSecretSize bytes.\nSecretKeySelector selects a key of a Secret.\nSecretList is a list of Secret.\nAdapts a Secret into a volume.\nThe contents of the target Secret\u2019s Data field will be presented in a volume as files using the keys in the Data field as the file names. Secret volumes support ownership management and SELinux relabeling.\nSecurityContext holds security configuration that will be applied to a container. Some fields are present in both SecurityContext and PodSecurityContext.  When both are set, the values in SecurityContext take precedence.\nSecurityContextConstraints governs the ability to make requests that affect the SecurityContext that will be applied to a container.\nSecurityContextConstraintsList is a list of SecurityContextConstraints objects\nService is a named abstraction of software service (for example, mysql) consisting of local port (for example 3306) that the proxy listens on, and the selector that determines which pods will answer requests sent through the proxy.\nServiceAccount binds together: * a name, understood by users, and perhaps by peripheral systems, for an identity * a principal that can be authenticated and authorized * a set of secrets\nServiceAccountList is a list of ServiceAccount objects\nServiceList holds a list of services.\nServicePort contains information on service\u2019s port.\nServiceSpec describes the attributes that a user creates on a service.\nServiceStatus represents the current status of a service.\nSupplementalGroupsStrategyOptions defines the strategy type and options used to create the strategy.\nTCPSocketAction describes an action based on opening a socket\nVolume represents a named volume in a pod that may be accessed by any container in the pod.\nVolumeMount describes a mounting of a Volume within a container.\nRepresents a vSphere volume resource.\nEviction evicts a pod from its node subject to certain policies and safety constraints. This is a subresource of Pod.  A request to cause such an eviction is created by POSTing to \u2026\u200b/pods/<pod name>/evictions.\n", "site_name": "https://projectatomic.io/registry"}, {"topic_url": "/latest/rest_api/openshift_v1.html", "title": "Atomic Registry Latest | REST API Reference | OpenShift v1", "content": "The Atomic Registry API exposes operations for managing an enterprise Kubernetes cluster, including security and user management, application deployments, image and source builds, HTTP(s) routing, and project management.\nVersion: v1\nHost: 127.0.0.1:8443BasePath: /Schemes: HTTPS\nrepresents an object patch, which may be any of: JSON patch (RFC 6902), JSON merge patch (RFC 7396), or the Kubernetes strategic merge patch\nthis may be any JSON object with a 'kind' and 'apiVersion' field; and is preserved unmodified by processing\nAPIResource specifies the name of a resource and whether it is namespaced.\nAPIResourceList is a list of APIResource, it is used to expose the name of the resources supported in a specific group and version, and if the resource is namespaced.\nA label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.\nA label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.\nListMeta describes metadata that synthetic resources must have, including lists and various status objects. A resource may have only one of {ObjectMeta, ListMeta}.\nPatch is provided to give a concrete name and type to the Kubernetes PATCH request body.\nStatus is a return value for calls that don\u2019t return other objects.\nStatusCause provides more information about an api.Status failure, including cases when multiple errors are encountered.\nStatusDetails is a set of additional properties that MAY be set by the server to provide additional information about a response. The Reason field of a Status object defines what attributes will be set. Clients must ignore fields that do not match the defined type of each attribute, and should assume that any attribute may be empty, invalid, or under defined.\nRepresents a Persistent Disk resource in AWS.\nAn AWS EBS disk must exist before mounting to a container. The disk must also be in the same AWS zone as the kubelet. An AWS EBS disk can only be mounted as read/write once. AWS EBS volumes support ownership management and SELinux relabeling.\nAppliedClusterResourceQuota mirrors ClusterResourceQuota at a project scope, for projection into a project.  It allows a project-admin to know which ClusterResourceQuotas are applied to his project and their associated usage.\nAppliedClusterResourceQuotaList is a collection of AppliedClusterResourceQuotas\nAzureDisk represents an Azure Data Disk mount on the host and bind mount to the pod.\nAzureFile represents an Azure File Service mount on the host and bind mount to the pod.\nBinaryBuildSource describes a binary file to be used for the Docker and Source build strategies, where the file will be extracted and used as the build source.\nBuild encapsulates the inputs needed to produce a new deployable image, as well as the status of the execution and a reference to the Pod which executed the build.\nBuild configurations define a build process for new Docker images. There are three types of builds possible - a Docker build using a Dockerfile, a Source-to-Image build that uses a specially prepared base image that accepts source code that it can make runnable, and a custom build that can run // arbitrary Docker images as a base and accept the build parameters. Builds run on the cluster and on completion are pushed to the Docker registry specified in the \"output\" section. A build can be triggered via a webhook, when the base image changes, or when a user manually requests a new build be // created.\nEach build created by a build configuration is numbered and refers back to its parent configuration. Multiple builds can be triggered at once. Builds that do not have \"output\" set can be used to test code or run a verification build.\nBuildConfigList is a collection of BuildConfigs.\nBuildConfigSpec describes when and how builds are created\nBuildConfigStatus contains current state of the build config object.\nBuildList is a collection of Builds.\nBuildLog is the (unused) resource associated with the build log redirector\nBuildOutput is input to a build strategy and describes the Docker image that the strategy should produce.\nA BuildPostCommitSpec holds a build post commit hook specification. The hook executes a command in a temporary container running the build output image, immediately after the last layer of the image is committed and before the image is pushed to a registry. The command is executed with the current working directory ($PWD) set to the image\u2019s WORKDIR.\nThe build will be marked as failed if the hook execution fails. It will fail if the script or command return a non-zero exit code, or if there is any other error related to starting the temporary container.\nThere are five different ways to configure the hook. As an example, all forms below are equivalent and will execute rake test --verbose.\nIt is invalid to provide both Script and Command simultaneously. If none of the fields are specified, the hook is not executed.\nBuildRequest is the resource used to pass parameters to build generator\nBuildSource is the SCM used for the build.\nBuildSpec has the information to represent a build and also additional information about a build\nBuildStatus contains the status of a build\nBuildStatusOutput contains the status of the built image.\nBuildStatusOutputTo describes the status of the built image with regards to image registry to which it was supposed to be pushed.\nBuildStrategy contains the details of how to perform a build.\nBuildTriggerCause holds information about a triggered build. It is used for displaying build trigger data for each build and build configuration in oc describe. It is also used to describe which triggers led to the most recent update in the build configuration.\nBuildTriggerPolicy describes a policy for a single trigger that results in a new Build.\nAdds and removes POSIX capabilities from running containers.\nRepresents a Ceph Filesystem mount that lasts the lifetime of a pod Cephfs volumes do not support ownership management or SELinux relabeling.\nRepresents a cinder volume resource in Openstack. A Cinder volume must exist before mounting to a container. The volume must also be in the same region as the kubelet. Cinder volumes support ownership management and SELinux relabeling.\nClusterNetwork describes the cluster network. There is normally only one object of this type, named \"default\", which is created by the SDN network plugin based on the master configuration when the cluster is brought up for the first time.\nClusterNetworkList is a collection of ClusterNetworks\nClusterPolicy is a object that holds all the ClusterRoles for a particular namespace.  There is at most one ClusterPolicy document per namespace.\nClusterPolicyBinding is a object that holds all the ClusterRoleBindings for a particular namespace.  There is one ClusterPolicyBinding document per referenced ClusterPolicy namespace\nClusterPolicyBindingList is a collection of ClusterPolicyBindings\nClusterPolicyList is a collection of ClusterPolicies\nClusterResourceQuota mirrors ResourceQuota at a cluster scope.  This object is easily convertible to synthetic ResourceQuota object to allow quota evaluation re-use.\nClusterResourceQuotaList is a collection of ClusterResourceQuotas\nClusterResourceQuotaSelector is used to select projects.  At least one of LabelSelector or AnnotationSelector must present.  If only one is present, it is the only selection criteria.  If both are specified, the project must match both restrictions.\nClusterResourceQuotaSpec defines the desired quota restrictions\nClusterResourceQuotaStatus defines the actual enforced quota and its current usage\nClusterRole is a logical grouping of PolicyRules that can be referenced as a unit by ClusterRoleBindings.\nClusterRoleBinding references a ClusterRole, but not contain it.  It can reference any ClusterRole in the same namespace or in the global namespace. It adds who information via (Users and Groups) OR Subjects and namespace information by which namespace it exists in. ClusterRoleBindings in a given namespace only have effect in that namespace (excepting the master namespace which has power in all namespaces).\nClusterRoleBindingList is a collection of ClusterRoleBindings\nClusterRoleList is a collection of ClusterRoles\nClusterRoleScopeRestriction describes restrictions on cluster role scopes\nSelects a key from a ConfigMap.\nAdapts a ConfigMap into a volume.\nThe contents of the target ConfigMap\u2019s Data field will be presented in a volume as files using the keys in the Data field as the file names, unless the items element is populated with specific mappings of keys to paths. ConfigMap volumes support ownership management and SELinux relabeling.\nA single application container that you want to run within a pod.\nContainerPort represents a network port in a single container.\nCustomBuildStrategy defines input parameters specific to Custom build.\nCustomDeploymentStrategyParams are the input to the Custom deployment strategy.\nDeleteOptions may be provided when deleting an API object\nDeploymentCause captures information about a particular cause of a deployment.\nDeploymentCauseImageTrigger represents details about the cause of a deployment originating from an image change trigger\nDeploymentCondition describes the state of a deployment config at a certain point.\nDeployment Configs define the template for a pod and manages deploying new images or configuration changes. A single deployment configuration is usually analogous to a single micro-service. Can support many different deployment patterns, including full restart, customizable rolling updates, and  fully custom behaviors, as well as pre- and post- deployment hooks. Each individual deployment is represented as a replication controller.\nA deployment is \"triggered\" when its configuration is changed or a tag in an Image Stream is changed. Triggers can be disabled to allow manual control over a deployment. The \"strategy\" determines how the deployment is carried out and may be changed at any time. The latestVersion field is updated when a new deployment is triggered by any means.\nDeploymentConfigList is a collection of deployment configs.\nDeploymentConfigRollback provides the input to rollback generation.\nDeploymentConfigRollbackSpec represents the options for rollback generation.\nDeploymentConfigSpec represents the desired state of the deployment.\nDeploymentConfigStatus represents the current deployment state.\nDeploymentDetails captures information about the causes of a deployment.\nDeploymentLog represents the logs for a deployment\nDeploymentRequest is a request to a deployment config for a new deployment.\nDeploymentStrategy describes how to perform a deployment.\nDeploymentTriggerImageChangeParams represents the parameters to the ImageChange trigger.\nDeploymentTriggerPolicy describes a policy for a single trigger that results in a new deployment.\nDeprecatedDownwardAPIVolumeFile represents information to create the file containing the pod field This type is deprecated and should be replaced by use of the downwardAPI volume source.\nDeprecatedDownwardAPIVolumeSource represents a volume containing downward API info. This type is deprecated and should be replaced by use of the downwardAPI volume source.\nDockerBuildStrategy defines input parameters specific to Docker build.\nDownwardAPIVolumeFile represents information to create the file containing the pod field\nDownwardAPIVolumeSource represents a volume containing downward API info. Downward API volumes support ownership management and SELinux relabeling.\nEgressNetworkPolicy describes the current egress network policy for a Namespace. When using the 'redhat/openshift-ovs-multitenant' network plugin, traffic from a pod to an IP address outside the cluster will be checked against each EgressNetworkPolicyRule in the pod\u2019s namespace\u2019s EgressNetworkPolicy, in order. If no rule matches (or no EgressNetworkPolicy is present) then the traffic will be allowed by default.\nEgressNetworkPolicyList is a collection of EgressNetworkPolicy\nEgressNetworkPolicyPeer specifies a target to apply egress network policy to\nEgressNetworkPolicyRule contains a single egress network policy rule\nEgressNetworkPolicySpec provides a list of policies on outgoing network traffic\nRepresents an empty directory for a pod. Empty directory volumes support ownership management and SELinux relabeling.\nEnvVar represents an environment variable present in a Container.\nEnvVarSource represents a source for the value of an EnvVar.\nExecAction describes a \"run in container\" action.\nExecNewPodHook is a hook implementation which runs a command in a new pod based on the specified container which is assumed to be part of the deployment template.\nRepresents a Fibre Channel volume. Fibre Channel volumes can only be mounted as read/write once. Fibre Channel volumes support ownership management and SELinux relabeling.\nFlexVolume represents a generic volume resource that is provisioned/attached using an exec based plugin. This is an alpha feature and may change in future.\nRepresents a Flocker volume mounted by the Flocker agent. One and only one of datasetName and datasetUUID should be set. Flocker volumes do not support ownership management or SELinux relabeling.\nRepresents a Persistent Disk resource in Google Compute Engine.\nA GCE PD must exist before mounting to a container. The disk must also be in the same GCE project and zone as the kubelet. A GCE PD can only be mounted as read/write once or read-only many times. GCE PDs support ownership management and SELinux relabeling.\nGenericWebHookCause holds information about a generic WebHook that triggered a build.\nGitBuildSource defines the parameters of a Git SCM\nGitHubWebHookCause has information about a GitHub webhook that triggered a build.\nRepresents a volume that is populated with the contents of a git repository. Git repo volumes do not support ownership management. Git repo volumes support SELinux relabeling.\nGitSourceRevision is the commit information from a git source for a build\nRepresents a Glusterfs mount that lasts the lifetime of a pod. Glusterfs volumes do not support ownership management or SELinux relabeling.\nGroup represents a referenceable set of Users\nGroupList is a collection of Groups\nGroupRestriction matches a group either by a string match on the group name or a label selector applied to group labels.\nHTTPGetAction describes an action based on HTTP Get requests.\nHTTPHeader describes a custom header to be used in HTTP probes\nHandler defines a specific action that should be taken\nRepresents a host path mapped into a pod. Host path volumes do not support ownership management or SELinux relabeling.\nHostSubnet describes the container subnet network on a node. The HostSubnet object must have the same name as the Node object it corresponds to.\nHostSubnetList is a collection of HostSubnets\nRepresents an ISCSI disk. ISCSI volumes can only be mounted as read/write once. ISCSI volumes support ownership management and SELinux relabeling.\nIdentity records a successful authentication of a user with an identity provider. The information about the source of authentication is stored on the identity, and the identity is then associated with a single user object. Multiple identities can reference a single user. Information retrieved from the authentication provider is stored in the extra field using a schema determined by the provider.\nIdentityList is a collection of Identities\nImage is an immutable representation of a Docker image and metadata at a point in time.\nImageChangeCause contains information about the image that triggered a build\nImageChangeTrigger allows builds to be triggered when an ImageStream changes\nImageImportSpec describes a request to import a specific image.\nImageImportStatus describes the result of an image import.\nImageLabel represents a label applied to the resulting image.\nImageLayer represents a single layer of the image. Some images may have multiple layers. Some may have none.\nImageList is a list of Image objects.\nImageSignature holds a signature of an image. It allows to verify image identity and possibly other claims as long as the signature is trusted. Based on this information it is possible to restrict runnable images to those matching cluster-wide policy. Mandatory fields should be parsed by clients doing image verification. The others are parsed from signature\u2019s content by the server. They serve just an informative purpose.\nImageSource is used to describe build source that will be extracted from an image. A reference of type ImageStreamTag, ImageStreamImage or DockerImage may be used. A pull secret can be specified to pull the image from an external registry or override the default service account secret if pulling from the internal registry. A list of paths to copy from the image and their respective destination within the build directory must be specified in the paths array.\nImageSourcePath describes a path to be copied from a source image and its destination within the build directory.\nImageStream stores a mapping of tags to images, metadata overrides that are applied when images are tagged in a stream, and an optional reference to a Docker image repository on a registry.\nImageStreamImage represents an Image that is retrieved by image name from an ImageStream.\nThe image stream import resource provides an easy way for a user to find and import Docker images from other Docker registries into the server. Individual images or an entire image repository may be imported, and users may choose to see the results of the import prior to tagging the resulting images into the specified image stream.\nThis API is intended for end-user tools that need to see the metadata of the image prior to import (for instance, to generate an application from it). Clients that know the desired image can continue to create spec.tags directly into their image streams.\nImageStreamImportSpec defines what images should be imported.\nImageStreamImportStatus contains information about the status of an image stream import.\nImageStreamList is a list of ImageStream objects.\nImageStreamMapping represents a mapping from a single tag to a Docker image as well as the reference to the Docker image stream the image came from.\nImageStreamSpec represents options for ImageStreams.\nImageStreamStatus contains information about the state of this image stream.\nImageStreamTag represents an Image that is retrieved by tag name from an ImageStream.\nImageStreamTagList is a list of ImageStreamTag objects.\nJenkinsPipelineBuildStrategy holds parameters specific to a Jenkins Pipeline build. This strategy is in tech preview.\nMaps a string key to a path within a volume.\nLifecycle describes actions that the management system should take in response to container lifecycle events. For the PostStart and PreStop lifecycle handlers, management of the container blocks until the action is complete, unless the container process fails, in which case the handler is aborted.\nLifecycleHook defines a specific deployment lifecycle action. Only one type of action may be specified at any time.\nLocalObjectReference contains enough information to let you locate the referenced object inside the same namespace.\nLocalResourceAccessReview is a means to request a list of which users and groups are authorized to perform the action specified by spec in a particular namespace\nLocalSubjectAccessReview is an object for requesting information about whether a user or group can perform an action in a particular namespace\nRepresents an NFS mount that lasts the lifetime of a pod. NFS volumes do not support ownership management or SELinux relabeling.\nNamedClusterRole relates a name with a cluster role\nNamedClusterRoleBinding relates a name with a cluster role binding\nNamedRole relates a Role with a name\nNamedRoleBinding relates a role binding with a name\nNamedTagEventList relates a tag to its image history.\nNetNamespace describes a single isolated network. When using the redhat/openshift-ovs-multitenant plugin, every Namespace will have a corresponding NetNamespace object with the same name. (When using redhat/openshift-ovs-subnet, NetNamespaces are not used.)\nNetNamespaceList is a collection of NetNamespaces\nOAuthAccessToken describes an OAuth access token\nOAuthAccessTokenList is a collection of OAuth access tokens\nOAuthAuthorizeToken describes an OAuth authorization token\nOAuthAuthorizeTokenList is a collection of OAuth authorization tokens\nOAuthClient describes an OAuth client\nOAuthClientAuthorization describes an authorization created by an OAuth client\nOAuthClientAuthorizationList is a collection of OAuth client authorizations\nOAuthClientList is a collection of OAuth clients\nObjectFieldSelector selects an APIVersioned field of an object.\nObjectMeta is metadata that all persisted resources must have, which includes all objects users must create.\nObjectReference contains enough information to let you inspect or modify the referred object.\nOwnerReference contains enough information to let you identify an owning object. Currently, an owning object must be in the same namespace, so there is no namespace field.\nParameter defines a name/value variable that is to be processed during the Template to Config transformation.\nPersistentVolumeClaimVolumeSource references the user\u2019s PVC in the same namespace. This volume finds the bound PV and mounts that volume for the pod. A PersistentVolumeClaimVolumeSource is, essentially, a wrapper around another type of volume that is owned by someone else (the system).\nRepresents a Photon Controller persistent disk resource.\nPodSecurityContext holds pod-level security attributes and common container settings. Some fields are also present in container.securityContext.  Field values of container.securityContext take precedence over field values of PodSecurityContext.\nPodSecurityPolicyReview checks which service accounts (not users, since that would be cluster-wide) can create the PodTemplateSpec in question.\nPodSecurityPolicyReviewSpec defines specification for PodSecurityPolicyReview\nPodSecurityPolicyReviewStatus represents the status of PodSecurityPolicyReview.\nPodSecurityPolicySelfSubjectReview checks whether this user/SA tuple can create the PodTemplateSpec\nPodSecurityPolicySelfSubjectReviewSpec contains specification for PodSecurityPolicySelfSubjectReview.\nPodSecurityPolicySubjectReview checks whether a particular user/SA tuple can create the PodTemplateSpec.\nPodSecurityPolicySubjectReviewSpec defines specification for PodSecurityPolicySubjectReview\nPodSecurityPolicySubjectReviewStatus contains information/status for PodSecurityPolicySubjectReview.\nPodSpec is a description of a pod.\nPodTemplateSpec describes the data a pod should have when created from a template\nPolicy is a object that holds all the Roles for a particular namespace.  There is at most one Policy document per namespace.\nPolicyBinding is a object that holds all the RoleBindings for a particular namespace.  There is one PolicyBinding document per referenced Policy namespace\nPolicyBindingList is a collection of PolicyBindings\nPolicyList is a collection of Policies\nPolicyRule holds information that describes a policy rule, but does not contain information about who the rule applies to or which namespace the rule applies to.\nPreconditions must be fulfilled before an operation (update, delete, etc.) is carried out.\nProbe describes a health check to be performed against a container to determine whether it is alive or ready to receive traffic.\nProjects are the unit of isolation and collaboration in OpenShift. A project has one or more members, a quota on the resources that the project may consume, and the security controls on the resources in the project. Within a project, members may have different roles - project administrators can set membership, editors can create and manage the resources, and viewers can see but not access running containers. In a normal cluster project administrators are not able to alter their quotas - that is restricted to cluster administrators.\nListing or watching projects will return only projects the user has the reader role on.\nAn OpenShift project is an alternative representation of a Kubernetes namespace. Projects are exposed as editable to end users while namespaces are not. Direct creation of a project is typically restricted to administrators, while end users should use the requestproject resource.\nProjectList is a list of Project objects.\nProjectRequest is the set of options necessary to fully qualify a project request\nProjectSpec describes the attributes on a Project\nProjectStatus is information about the current status of a Project\nRepresents a Quobyte mount that lasts the lifetime of a pod. Quobyte volumes do not support ownership management or SELinux relabeling.\nRepresents a Rados Block Device mount that lasts the lifetime of a pod. RBD volumes support ownership management and SELinux relabeling.\nRecreateDeploymentStrategyParams are the input to the Recreate deployment strategy.\nRepositoryImportSpec describes a request to import images from a Docker image repository.\nRepositoryImportStatus describes the result of an image repository import\nResourceAccessReview is a means to request a list of which users and groups are authorized to perform the action specified by spec\nResourceFieldSelector represents container resources (cpu, memory) and their output format\nResourceQuotaSpec defines the desired hard limits to enforce for Quota.\nResourceQuotaStatus defines the enforced hard limits and observed use.\nResourceQuotaStatusByNamespace gives status for a particular project\nResourceRequirements describes the compute resource requirements.\nRole is a logical grouping of PolicyRules that can be referenced as a unit by RoleBindings.\nRoleBinding references a Role, but not contain it.  It can reference any Role in the same namespace or in the global namespace. It adds who information via (Users and Groups) OR Subjects and namespace information by which namespace it exists in. RoleBindings in a given namespace only have effect in that namespace (excepting the master namespace which has power in all namespaces).\nRoleBindingList is a collection of RoleBindings\nRoleBindingRestriction is an object that can be matched against a subject (user, group, or service account) to determine whether rolebindings on that subject are allowed in the namespace to which the RoleBindingRestriction belongs.  If any one of those RoleBindingRestriction objects matches a subject, rolebindings on that subject in the namespace are allowed.\nRoleBindingRestrictionList is a collection of RoleBindingRestriction objects.\nRoleBindingRestrictionSpec defines a rolebinding restriction.  Exactly one field must be non-nil.\nRoleList is a collection of Roles\nRollingDeploymentStrategyParams are the input to the Rolling deployment strategy.\nA route allows developers to expose services through an HTTP(S) aware load balancing and proxy layer via a public DNS entry. The route may further specify TLS options and a certificate, or specify a public CNAME that the router should also accept for HTTP and HTTPS traffic. An administrator typically configures their router to be visible outside the cluster firewall, and may also add additional security, caching, or traffic controls on the service content. Routers usually talk directly to the service endpoints.\nOnce a route is created, the host field may not be changed. Generally, routers use the oldest route with a given host when resolving conflicts.\nRouters are subject to additional customization and may support additional controls via the annotations field.\nBecause administrators may configure multiple routers, the route status field is used to return information to clients about the names and states of the route under each router. If a client chooses a duplicate name, for instance, the route status conditions are used to indicate the route cannot be chosen.\nRouteIngress holds information about the places where a route is exposed.\nRouteIngressCondition contains details for the current condition of this route on a particular router.\nRouteList is a collection of Routes.\nRoutePort defines a port mapping from a router to an endpoint in the service endpoints.\nRouteSpec describes the hostname or path the route exposes, any security information, and one or more backends the route points to. Weights on each backend can define the balance of traffic sent to each backend - if all weights are zero the route will be considered to have no backends and return a standard 503 response.\nThe tls field is optional and allows specific certificates or behavior for the route. Routers typically configure a default certificate on a wildcard domain to terminate routes without explicit certificates, but custom hostnames usually must choose passthrough (send traffic directly to the backend via the TLS Server-Name- Indication field) or provide a certificate.\nRouteStatus provides relevant info about the status of a route, including which routers acknowledge it.\nRouteTargetReference specifies the target that resolve into endpoints. Only the 'Service' kind is allowed. Use 'weight' field to emphasize one over others.\nSELinuxOptions are the labels to be applied to the container\nScopeRestriction describe one restriction on scopes.  Exactly one option must be non-nil.\nSecret holds secret data of a certain type. The total bytes of the values in the Data field must be less than MaxSecretSize bytes.\nSecretBuildSource describes a secret and its destination directory that will be used only at the build time. The content of the secret referenced here will be copied into the destination directory instead of mounting.\nSecretKeySelector selects a key of a Secret.\nSecretList is a list of Secret.\nSecretSpec specifies a secret to be included in a build pod and its corresponding mount point\nAdapts a Secret into a volume.\nThe contents of the target Secret\u2019s Data field will be presented in a volume as files using the keys in the Data field as the file names. Secret volumes support ownership management and SELinux relabeling.\nSecurityContext holds security configuration that will be applied to a container. Some fields are present in both SecurityContext and PodSecurityContext.  When both are set, the values in SecurityContext take precedence.\nSelfSubjectRulesReview is a resource you can create to determine which actions you can perform in a namespace\nSelfSubjectRulesReviewSpec adds information about how to conduct the check\nServiceAccountPodSecurityPolicyReviewStatus represents ServiceAccount name and related review status\nServiceAccountReference specifies a service account and namespace by their names.\nServiceAccountRestriction matches a service account by a string match on either the service-account name or the name of the service account\u2019s namespace.\nSignatureCondition describes an image signature condition of particular kind at particular probe time.\nSignatureIssuer holds information about an issuer of signing certificate or key.\nSignatureSubject holds information about a person or entity who created the signature.\nSourceBuildStrategy defines input parameters specific to an Source build.\nSourceControlUser defines the identity of a user of source control\nSourceRevision is the revision or commit information from the source for the build\nSubjectAccessReview is an object for requesting information about whether a user or group can perform an action\nSubjectRulesReview is a resource you can create to determine which actions another user can perform in a namespace\nSubjectRulesReviewSpec adds information about how to conduct the check\nSubjectRulesReviewStatus is contains the result of a rules check\nTCPSocketAction describes an action based on opening a socket\nTLSConfig defines config used to secure a route and provide termination\nTagEvent is used by ImageStreamStatus to keep a historical record of images associated with a tag.\nTagEventCondition contains condition information for a tag event.\nTagImageHook is a request to tag the image in a particular container onto an ImageStreamTag.\nTagImportPolicy controls how images related to this tag will be imported.\nTagReference specifies optional annotations for images using this tag and an optional reference to an ImageStreamTag, ImageStreamImage, or DockerImage this tag should track.\nTagReferencePolicy describes how pull-specs for images in this image stream tag are generated when image change triggers in deployment configs or builds are resolved. This allows the image stream author to control how images are accessed.\nTemplate contains the inputs needed to produce a Config.\nTemplateList is a list of Template objects.\nUpon log in, every user of the system receives a User and Identity resource. Administrators may directly manipulate the attributes of the users for their own tracking, or set groups via the API. The user name is unique and is chosen based on the value provided by the identity provider - if a user already exists with the incoming name, the user name may have a number appended to it depending on the configuration of the system.\nUserIdentityMapping maps a user to an identity\nUserList is a collection of Users\nUserRestriction matches a user either by a string match on the user name, a string match on the name of a group to which the user belongs, or a label selector applied to the user labels.\nVolume represents a named volume in a pod that may be accessed by any container in the pod.\nVolumeMount describes a mounting of a Volume within a container.\nRepresents a vSphere volume resource.\nWebHookTrigger is a trigger that gets invoked using a webhook type of post\nrepresents a scaling request for a resource.\ndescribes the attributes of a scale subresource\nrepresents the current status of a scale subresource.\n", "site_name": "https://projectatomic.io/registry"}]